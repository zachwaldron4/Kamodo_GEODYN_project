{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956d1a5c",
   "metadata": {},
   "source": [
    "# ICESat2 IT Model Validation for 3 months of PCE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acc2077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:28.994568Z",
     "start_time": "2022-04-02T00:23:28.289715Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f350ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:28.999568Z",
     "start_time": "2022-04-02T00:23:28.996806Z"
    }
   },
   "outputs": [],
   "source": [
    "run_list = [, 'msis2']\n",
    "arc_len_list = [  3 , 24]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf7b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T19:34:05.113577Z",
     "start_time": "2022-03-03T19:34:05.110177Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c57b65b9",
   "metadata": {},
   "source": [
    "## Automate the YAML file editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56d4e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.146768Z",
     "start_time": "2022-04-02T00:23:29.001977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 arc1_p1   2018.313 2018-11-09 00:00:00  to  2018-11-09 03:00:00\n",
      "2 arc2_p1   2018.313 2018-11-09 03:00:00  to  2018-11-09 06:00:00\n",
      "3 arc3_p1   2018.313 2018-11-09 06:00:00  to  2018-11-09 09:00:00\n",
      "4 arc4_p1   2018.313 2018-11-09 09:00:00  to  2018-11-09 12:00:00\n",
      "5 arc5_p1   2018.313 2018-11-09 12:00:00  to  2018-11-09 15:00:00\n",
      "6 arc6_p1   2018.313 2018-11-09 15:00:00  to  2018-11-09 18:00:00\n",
      "7 arc7_p1   2018.313 2018-11-09 18:00:00  to  2018-11-09 21:00:00\n",
      "8 arc8_p1   2018.313 2018-11-09 21:00:00  to  2018-11-10 00:00:00\n",
      "9 arc9_p1   2018.314 2018-11-10 00:00:00  to  2018-11-10 03:00:00\n",
      "10 arc10_p1   2018.314 2018-11-10 03:00:00  to  2018-11-10 06:00:00\n",
      "11 arc11_p1   2018.314 2018-11-10 06:00:00  to  2018-11-10 09:00:00\n",
      "12 arc12_p1   2018.314 2018-11-10 09:00:00  to  2018-11-10 12:00:00\n",
      "13 arc13_p1   2018.314 2018-11-10 12:00:00  to  2018-11-10 15:00:00\n",
      "14 arc14_p1   2018.314 2018-11-10 15:00:00  to  2018-11-10 18:00:00\n",
      "15 arc15_p1   2018.314 2018-11-10 18:00:00  to  2018-11-10 21:00:00\n",
      "16 arc16_p1   2018.314 2018-11-10 21:00:00  to  2018-11-11 00:00:00\n",
      "17 arc17_p1   2018.315 2018-11-11 00:00:00  to  2018-11-11 03:00:00\n",
      "18 arc18_p1   2018.315 2018-11-11 03:00:00  to  2018-11-11 06:00:00\n",
      "19 arc19_p1   2018.315 2018-11-11 06:00:00  to  2018-11-11 09:00:00\n",
      "20 arc20_p1   2018.315 2018-11-11 09:00:00  to  2018-11-11 12:00:00\n",
      "21 arc21_p1   2018.315 2018-11-11 12:00:00  to  2018-11-11 15:00:00\n",
      "22 arc22_p1   2018.315 2018-11-11 15:00:00  to  2018-11-11 18:00:00\n",
      "23 arc23_p1   2018.315 2018-11-11 18:00:00  to  2018-11-11 21:00:00\n",
      "24 arc24_p1   2018.315 2018-11-11 21:00:00  to  2018-11-12 00:00:00\n",
      "25 arc25_p1   2018.316 2018-11-12 00:00:00  to  2018-11-12 03:00:00\n",
      "--------- file 1\n",
      "1 arc26_p2   2018.316 2018-11-12 03:00:00  to  2018-11-12 06:00:00\n",
      "2 arc27_p2   2018.316 2018-11-12 06:00:00  to  2018-11-12 09:00:00\n",
      "3 arc28_p2   2018.316 2018-11-12 09:00:00  to  2018-11-12 12:00:00\n",
      "4 arc29_p2   2018.316 2018-11-12 12:00:00  to  2018-11-12 15:00:00\n",
      "5 arc30_p2   2018.316 2018-11-12 15:00:00  to  2018-11-12 18:00:00\n",
      "6 arc31_p2   2018.316 2018-11-12 18:00:00  to  2018-11-12 21:00:00\n",
      "7 arc32_p2   2018.316 2018-11-12 21:00:00  to  2018-11-13 00:00:00\n",
      "8 arc33_p2   2018.317 2018-11-13 00:00:00  to  2018-11-13 03:00:00\n",
      "9 arc34_p2   2018.317 2018-11-13 03:00:00  to  2018-11-13 06:00:00\n",
      "10 arc35_p2   2018.317 2018-11-13 06:00:00  to  2018-11-13 09:00:00\n",
      "11 arc36_p2   2018.317 2018-11-13 09:00:00  to  2018-11-13 12:00:00\n",
      "12 arc37_p2   2018.317 2018-11-13 12:00:00  to  2018-11-13 15:00:00\n",
      "13 arc38_p2   2018.317 2018-11-13 15:00:00  to  2018-11-13 18:00:00\n",
      "14 arc39_p2   2018.317 2018-11-13 18:00:00  to  2018-11-13 21:00:00\n",
      "15 arc40_p2   2018.317 2018-11-13 21:00:00  to  2018-11-14 00:00:00\n",
      "16 arc41_p2   2018.318 2018-11-14 00:00:00  to  2018-11-14 03:00:00\n",
      "17 arc42_p2   2018.318 2018-11-14 03:00:00  to  2018-11-14 06:00:00\n",
      "18 arc43_p2   2018.318 2018-11-14 06:00:00  to  2018-11-14 09:00:00\n",
      "19 arc44_p2   2018.318 2018-11-14 09:00:00  to  2018-11-14 12:00:00\n",
      "20 arc45_p2   2018.318 2018-11-14 12:00:00  to  2018-11-14 15:00:00\n",
      "21 arc46_p2   2018.318 2018-11-14 15:00:00  to  2018-11-14 18:00:00\n",
      "22 arc47_p2   2018.318 2018-11-14 18:00:00  to  2018-11-14 21:00:00\n",
      "23 arc48_p2   2018.318 2018-11-14 21:00:00  to  2018-11-15 00:00:00\n",
      "24 arc49_p2   2018.319 2018-11-15 00:00:00  to  2018-11-15 03:00:00\n",
      "25 arc50_p2   2018.319 2018-11-15 03:00:00  to  2018-11-15 06:00:00\n",
      "--------- file 2\n",
      "1 arc51_p3   2018.319 2018-11-15 06:00:00  to  2018-11-15 09:00:00\n",
      "2 arc52_p3   2018.319 2018-11-15 09:00:00  to  2018-11-15 12:00:00\n",
      "3 arc53_p3   2018.319 2018-11-15 12:00:00  to  2018-11-15 15:00:00\n",
      "4 arc54_p3   2018.319 2018-11-15 15:00:00  to  2018-11-15 18:00:00\n",
      "5 arc55_p3   2018.319 2018-11-15 18:00:00  to  2018-11-15 21:00:00\n",
      "6 arc56_p3   2018.319 2018-11-15 21:00:00  to  2018-11-16 00:00:00\n",
      "7 arc57_p3   2018.320 2018-11-16 00:00:00  to  2018-11-16 03:00:00\n",
      "8 arc58_p3   2018.320 2018-11-16 03:00:00  to  2018-11-16 06:00:00\n",
      "9 arc59_p3   2018.320 2018-11-16 06:00:00  to  2018-11-16 09:00:00\n",
      "10 arc60_p3   2018.320 2018-11-16 09:00:00  to  2018-11-16 12:00:00\n",
      "11 arc61_p3   2018.320 2018-11-16 12:00:00  to  2018-11-16 15:00:00\n",
      "12 arc62_p3   2018.320 2018-11-16 15:00:00  to  2018-11-16 18:00:00\n",
      "13 arc63_p3   2018.320 2018-11-16 18:00:00  to  2018-11-16 21:00:00\n",
      "14 arc64_p3   2018.320 2018-11-16 21:00:00  to  2018-11-17 00:00:00\n",
      "15 arc65_p3   2018.321 2018-11-17 00:00:00  to  2018-11-17 03:00:00\n",
      "16 arc66_p3   2018.321 2018-11-17 03:00:00  to  2018-11-17 06:00:00\n",
      "17 arc67_p3   2018.321 2018-11-17 06:00:00  to  2018-11-17 09:00:00\n",
      "18 arc68_p3   2018.321 2018-11-17 09:00:00  to  2018-11-17 12:00:00\n",
      "19 arc69_p3   2018.321 2018-11-17 12:00:00  to  2018-11-17 15:00:00\n",
      "20 arc70_p3   2018.321 2018-11-17 15:00:00  to  2018-11-17 18:00:00\n",
      "21 arc71_p3   2018.321 2018-11-17 18:00:00  to  2018-11-17 21:00:00\n",
      "22 arc72_p3   2018.321 2018-11-17 21:00:00  to  2018-11-18 00:00:00\n",
      "23 arc73_p3   2018.322 2018-11-18 00:00:00  to  2018-11-18 03:00:00\n",
      "24 arc74_p3   2018.322 2018-11-18 03:00:00  to  2018-11-18 06:00:00\n",
      "25 arc75_p3   2018.322 2018-11-18 06:00:00  to  2018-11-18 09:00:00\n",
      "--------- file 3\n",
      "1 arc76_p4   2018.322 2018-11-18 09:00:00  to  2018-11-18 12:00:00\n",
      "2 arc77_p4   2018.322 2018-11-18 12:00:00  to  2018-11-18 15:00:00\n",
      "3 arc78_p4   2018.322 2018-11-18 15:00:00  to  2018-11-18 18:00:00\n",
      "4 arc79_p4   2018.322 2018-11-18 18:00:00  to  2018-11-18 21:00:00\n",
      "5 arc80_p4   2018.322 2018-11-18 21:00:00  to  2018-11-19 00:00:00\n",
      "6 arc81_p4   2018.323 2018-11-19 00:00:00  to  2018-11-19 03:00:00\n",
      "7 arc82_p4   2018.323 2018-11-19 03:00:00  to  2018-11-19 06:00:00\n",
      "8 arc83_p4   2018.323 2018-11-19 06:00:00  to  2018-11-19 09:00:00\n",
      "9 arc84_p4   2018.323 2018-11-19 09:00:00  to  2018-11-19 12:00:00\n",
      "10 arc85_p4   2018.323 2018-11-19 12:00:00  to  2018-11-19 15:00:00\n",
      "11 arc86_p4   2018.323 2018-11-19 15:00:00  to  2018-11-19 18:00:00\n",
      "12 arc87_p4   2018.323 2018-11-19 18:00:00  to  2018-11-19 21:00:00\n",
      "13 arc88_p4   2018.323 2018-11-19 21:00:00  to  2018-11-20 00:00:00\n",
      "14 arc89_p4   2018.324 2018-11-20 00:00:00  to  2018-11-20 03:00:00\n",
      "15 arc90_p4   2018.324 2018-11-20 03:00:00  to  2018-11-20 06:00:00\n",
      "16 arc91_p4   2018.324 2018-11-20 06:00:00  to  2018-11-20 09:00:00\n",
      "17 arc92_p4   2018.324 2018-11-20 09:00:00  to  2018-11-20 12:00:00\n",
      "18 arc93_p4   2018.324 2018-11-20 12:00:00  to  2018-11-20 15:00:00\n",
      "19 arc94_p4   2018.324 2018-11-20 15:00:00  to  2018-11-20 18:00:00\n",
      "20 arc95_p4   2018.324 2018-11-20 18:00:00  to  2018-11-20 21:00:00\n",
      "21 arc96_p4   2018.324 2018-11-20 21:00:00  to  2018-11-21 00:00:00\n",
      "22 arc97_p4   2018.325 2018-11-21 00:00:00  to  2018-11-21 03:00:00\n",
      "23 arc98_p4   2018.325 2018-11-21 03:00:00  to  2018-11-21 06:00:00\n",
      "24 arc99_p4   2018.325 2018-11-21 06:00:00  to  2018-11-21 09:00:00\n",
      "25 arc100_p4   2018.325 2018-11-21 09:00:00  to  2018-11-21 12:00:00\n",
      "--------- file 4\n",
      "1 arc101_p5   2018.325 2018-11-21 12:00:00  to  2018-11-21 15:00:00\n",
      "2 arc102_p5   2018.325 2018-11-21 15:00:00  to  2018-11-21 18:00:00\n",
      "3 arc103_p5   2018.325 2018-11-21 18:00:00  to  2018-11-21 21:00:00\n",
      "4 arc104_p5   2018.325 2018-11-21 21:00:00  to  2018-11-22 00:00:00\n",
      "5 arc105_p5   2018.326 2018-11-22 00:00:00  to  2018-11-22 03:00:00\n",
      "6 arc106_p5   2018.326 2018-11-22 03:00:00  to  2018-11-22 06:00:00\n",
      "7 arc107_p5   2018.326 2018-11-22 06:00:00  to  2018-11-22 09:00:00\n",
      "8 arc108_p5   2018.326 2018-11-22 09:00:00  to  2018-11-22 12:00:00\n",
      "9 arc109_p5   2018.326 2018-11-22 12:00:00  to  2018-11-22 15:00:00\n",
      "10 arc110_p5   2018.326 2018-11-22 15:00:00  to  2018-11-22 18:00:00\n",
      "11 arc111_p5   2018.326 2018-11-22 18:00:00  to  2018-11-22 21:00:00\n",
      "12 arc112_p5   2018.326 2018-11-22 21:00:00  to  2018-11-23 00:00:00\n",
      "13 arc113_p5   2018.327 2018-11-23 00:00:00  to  2018-11-23 03:00:00\n",
      "14 arc114_p5   2018.327 2018-11-23 03:00:00  to  2018-11-23 06:00:00\n",
      "15 arc115_p5   2018.327 2018-11-23 06:00:00  to  2018-11-23 09:00:00\n",
      "16 arc116_p5   2018.327 2018-11-23 09:00:00  to  2018-11-23 12:00:00\n",
      "17 arc117_p5   2018.327 2018-11-23 12:00:00  to  2018-11-23 15:00:00\n",
      "18 arc118_p5   2018.327 2018-11-23 15:00:00  to  2018-11-23 18:00:00\n",
      "19 arc119_p5   2018.327 2018-11-23 18:00:00  to  2018-11-23 21:00:00\n",
      "20 arc120_p5   2018.327 2018-11-23 21:00:00  to  2018-11-24 00:00:00\n",
      "      \n",
      "+++++++++++++++++++++\n",
      "1 arc1_p1   2018.313 2018-11-09 00:00:00  to  2018-11-10 00:00:00\n",
      "2 arc2_p1   2018.314 2018-11-10 00:00:00  to  2018-11-11 00:00:00\n",
      "3 arc3_p1   2018.315 2018-11-11 00:00:00  to  2018-11-12 00:00:00\n",
      "4 arc4_p1   2018.316 2018-11-12 00:00:00  to  2018-11-13 00:00:00\n",
      "5 arc5_p1   2018.317 2018-11-13 00:00:00  to  2018-11-14 00:00:00\n",
      "6 arc6_p1   2018.318 2018-11-14 00:00:00  to  2018-11-15 00:00:00\n",
      "7 arc7_p1   2018.319 2018-11-15 00:00:00  to  2018-11-16 00:00:00\n",
      "8 arc8_p1   2018.320 2018-11-16 00:00:00  to  2018-11-17 00:00:00\n",
      "9 arc9_p1   2018.321 2018-11-17 00:00:00  to  2018-11-18 00:00:00\n",
      "10 arc10_p1   2018.322 2018-11-18 00:00:00  to  2018-11-19 00:00:00\n",
      "11 arc11_p1   2018.323 2018-11-19 00:00:00  to  2018-11-20 00:00:00\n",
      "12 arc12_p1   2018.324 2018-11-20 00:00:00  to  2018-11-21 00:00:00\n",
      "13 arc13_p1   2018.325 2018-11-21 00:00:00  to  2018-11-22 00:00:00\n",
      "14 arc14_p1   2018.326 2018-11-22 00:00:00  to  2018-11-23 00:00:00\n",
      "15 arc15_p1   2018.327 2018-11-23 00:00:00  to  2018-11-24 00:00:00\n",
      "      \n",
      "+++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# # if 24:\n",
    "# arcs = {}\n",
    "# arcs['2018.304'] = ['181031 000000.0000000' , '181101 000000.0000000']\n",
    "# arcs['2018.305'] = ['181101 000000.0000000' , '181102 000000.0000000']\n",
    "# arcs['2018.306'] = ['181102 000000.0000000' , '181103 000000.0000000']\n",
    "# arcs['2018.307'] = ['181103 000000.0000000' , '181104 000000.0000000']\n",
    "# arcs['2018.308'] = ['181104 000000.0000000' , '181105 000000.0000000']\n",
    "\n",
    "# arc                 : [ '2018.313', '2018.314', '2018.315', '2018.316',\n",
    "#                         '2018.317', '2018.318', '2018.319', '2018.320',\n",
    "#                         '2018.321', '2018.322', '2018.323', '2018.324', \n",
    "#                         '2018.325', '2018.326', '2018.327' ]\n",
    "# epoch_start         : [ '181109 000000.0000000', '181110 000000.0000000', \n",
    "#                         '181111 000000.0000000', '181112 000000.0000000', \n",
    "#                         '181113 000000.0000000', '181114 000000.0000000',\n",
    "#                         '181115 000000.0000000', '181116 000000.0000000',\n",
    "#                         '181117 000000.0000000', '181118 000000.0000000',\n",
    "#                         '181119 000000.0000000', '181120 000000.0000000',\n",
    "#                         '181121 000000.0000000', '181122 000000.0000000',\n",
    "#                         '181123 000000.0000000' ]\n",
    "# epoch_end           : [ '181110 000000.0000000', '181111 000000.0000000',\n",
    "#                         '181112 000000.0000000', '181113 000000.0000000',\n",
    "#                         '181114 000000.0000000', '181115 000000.0000000',\n",
    "#                         '181116 000000.0000000', '181117 000000.0000000',\n",
    "#                         '181118 000000.0000000', '181119 000000.0000000',\n",
    "#                         '181120 000000.0000000', '181121 000000.0000000',\n",
    "#                         '181122 000000.0000000', '181123 000000.0000000',\n",
    "#                         '181124 000000.0000000'  ]\n",
    "\n",
    "\n",
    "timestart = pd.to_datetime('181109 000000.0000000', format='%y%m%d %H%M%S.%f')\n",
    "# timeend   = pd.to_datetime('181115 000000.0000000', format='%y%m%d %H%M%S.%f')\n",
    "\n",
    "\n",
    "# timeend   = pd.to_datetime('181112 000000.0000000', format='%y%m%d %H%M%S.%f')\n",
    "\n",
    "timeend   = pd.to_datetime('181124 000000.0000000', format='%y%m%d %H%M%S.%f')\n",
    "\n",
    "arcs = {}\n",
    "mod_count = 1\n",
    "\n",
    "for ihr,arclen in enumerate(arc_len_list): \n",
    "    arcs[arclen] = {}\n",
    "    counter = 0\n",
    "    itime = timestart\n",
    "    mod_count = 1\n",
    "    mod_counter = 0\n",
    "    arcs[arclen][mod_count]={}\n",
    "\n",
    "    while itime < timeend:\n",
    "        if mod_counter == 25:\n",
    "            print('--------- file',mod_count )\n",
    "            mod_count += 1 \n",
    "            arcs[arclen][mod_count]={}\n",
    "        mod_counter = np.mod(counter,25)+1\n",
    "\n",
    "        stringkey = 'arc'+str(counter+1)+'_p'+str(mod_count)\n",
    "\n",
    "        \n",
    "        itime_0 = itime\n",
    "        itime = itime + pd.to_timedelta(arclen,'h')\n",
    "        \n",
    "        arcs[arclen][mod_count][stringkey] = [itime_0.strftime('%y%m%d %H%M%S.%f')+'0', \n",
    "                                            itime.strftime(  '%y%m%d %H%M%S.%f')+'0', \n",
    "                                            itime_0.strftime('%Y.%j')]\n",
    "\n",
    "        counter += 1 \n",
    "        \n",
    "#         print(mod_count)\n",
    "        print(mod_counter, stringkey,' ',itime_0.strftime('%Y.%j'),itime_0, ' to ', itime)        \n",
    "    print('      ')    \n",
    "    print('+++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4c34c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.152838Z",
     "start_time": "2022-04-02T00:23:29.148448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in arcs[3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9bc068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.158455Z",
     "start_time": "2022-04-02T00:23:29.154241Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_arcs = len(arcs[3].keys())\n",
    "# num_fileparts =int(np.ceil(num_arcs/25))\n",
    "# print('Splitting up into',num_fileparts , 'files')\n",
    "\n",
    "# # a_list = arcs[3]\n",
    "\n",
    "# # for i in arcs[arclen].keys():\n",
    "# #     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50457ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.165529Z",
     "start_time": "2022-04-02T00:23:29.160331Z"
    }
   },
   "outputs": [],
   "source": [
    "# a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db47416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.193830Z",
     "start_time": "2022-04-02T00:23:29.169198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hour FILE # 1\n",
      "3 hour FILE # 2\n",
      "3 hour FILE # 3\n",
      "3 hour FILE # 4\n",
      "3 hour FILE # 5\n",
      "24 hour FILE # 1\n",
      "3 hour FILE # 1\n",
      "3 hour FILE # 2\n",
      "3 hour FILE # 3\n",
      "3 hour FILE # 4\n",
      "3 hour FILE # 5\n",
      "24 hour FILE # 1\n"
     ]
    }
   ],
   "source": [
    "global_path = '/data/zach_work/validation/investigate_arclength/'\n",
    "template = global_path+'template'\n",
    "counter = 0\n",
    "\n",
    "\n",
    "for imod,model in enumerate(run_list):\n",
    "    \n",
    "    for ihr,arclen in enumerate(arc_len_list): \n",
    "\n",
    "        for ifile in arcs[arclen].keys():\n",
    "            print(arclen, 'hour FILE #',ifile)\n",
    "\n",
    "            run_settings = global_path+'ArclengthTests_periodC_'+ model+'_'+str(arclen)+'hour_p'+str(ifile)+'.yaml'\n",
    "\n",
    "\n",
    "            #### Make path to new settings file\n",
    "            ##### MODIFY THE SETTINGS FILE\n",
    "\n",
    "            #### Read the file.\n",
    "            with open(template, \"r\") as f:\n",
    "                lines_all = f.readlines()\n",
    "\n",
    "            ### Re-write the file line-by-line with your changes\n",
    "            with open(run_settings, \"w\") as f:\n",
    "                for line_num, line in enumerate(lines_all):\n",
    "\n",
    "                    if 'arc_length                   :' in line:\n",
    "                        f.write(\"arc_length                   : '\"+str(arclen)+\"hr'\\n\")\n",
    "\n",
    "\n",
    "                    elif 'epoch_start  :' in line:\n",
    "                        f.write(\"epoch_start         : [ \\n\")\n",
    "                        for i in arcs[arclen][ifile]:\n",
    "                            f.write(\"                    '\"+arcs[arclen][ifile][i][0]+\"', \\n\")\n",
    "                        f.write(\"                      ] \\n\")\n",
    "\n",
    "                    elif 'epoch_end    :' in line:\n",
    "                        f.write(\"epoch_end         : [ \\n\")\n",
    "                        for i in arcs[arclen][ifile]:\n",
    "                            f.write(\"                    '\"+arcs[arclen][ifile][i][1]+\"', \\n\")\n",
    "                        f.write(\"                      ] \\n\")\n",
    "\n",
    "                    elif 'arc          :' in line:\n",
    "                        f.write(\"arc         : [ \\n\")\n",
    "                        for i in arcs[arclen][ifile]:\n",
    "                            f.write(\"                    '\"+arcs[arclen][ifile][i][2]+\"', \\n\")\n",
    "                        f.write(\"                      ] \\n\")\n",
    "\n",
    "\n",
    "                    elif 'den_model                    :' in line:\n",
    "                        f.write(\"den_model                    : '\"+model+\"' \\n\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        f.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f15789c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.234543Z",
     "start_time": "2022-04-02T00:23:29.196500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY\n",
      "------\n",
      "Total      :    3.7G\n",
      "Available  :    1.8G\n",
      "Percent    :    50.1\n",
      "Used       :    1.5G\n",
      "Free       :    1.1G\n",
      "Active     :    1.9G\n",
      "Inactive   :  558.4M\n",
      "Buffers    :    0.0B\n",
      "Cached     :    1.1G\n",
      "Shared     :  182.3M\n",
      "Slab       :   69.5M\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print system memory information.\n",
    "$ python3 scripts/meminfo.py\n",
    "MEMORY\n",
    "------\n",
    "\"\"\"\n",
    "\n",
    "import psutil\n",
    "from psutil._common import bytes2human\n",
    "\n",
    "def pprint_ntuple(nt):\n",
    "    for name in nt._fields:\n",
    "        value = getattr(nt, name)\n",
    "        if name != 'percent':\n",
    "            value = bytes2human(value)\n",
    "        print('%-10s : %7s' % (name.capitalize(), value))\n",
    "\n",
    "\n",
    "def main_memory():\n",
    "    print('MEMORY\\n------')\n",
    "    pprint_ntuple(psutil.virtual_memory())\n",
    "#     print('\\nSWAP\\n----')\n",
    "#     pprint_ntuple(psutil.swap_memory())\n",
    "if __name__ == '__main__':\n",
    "    main_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce3063",
   "metadata": {},
   "source": [
    "## Clear memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24838339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T02:09:24.649532Z",
     "start_time": "2022-04-01T02:09:24.599679Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85931cf6",
   "metadata": {},
   "source": [
    "## GEODYN Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea42742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T00:23:29.243693Z",
     "start_time": "2022-04-02T00:23:29.236562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY\n",
      "------\n",
      "Total      :    3.7G\n",
      "Available  :    1.8G\n",
      "Percent    :    50.1\n",
      "Used       :    1.5G\n",
      "Free       :    1.1G\n",
      "Active     :    1.9G\n",
      "Inactive   :  558.4M\n",
      "Buffers    :    0.0B\n",
      "Cached     :    1.1G\n",
      "Shared     :  182.3M\n",
      "Slab       :   69.5M\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccfea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:21.949630Z",
     "start_time": "2022-04-01T01:23:20.851Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be6fd0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hour FILE # 1\n",
      "Run # 1     Current Time =      17:23:30  GMT-7\n",
      "Run # 1\n",
      "+ —————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "|\n",
      "| ---------------------- RUN PARAMETERS  ----------------------\n",
      "|\n",
      "|  Run # 1     IISSET Cleaned      tmp/.../cleaned_setup_2018313.01\n",
      "|  Run # 1     Density Model:      jb2008\n",
      "|  Run # 1     GEODYN Version:     CD_model_proj\n",
      "|  Run # 1     Output directory:   /data/zach_work/output_from_runs/jb2008/jb2008_BWDRAG_arclengthtests\n",
      "|  Run # 1     EXAT File:          /data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.313.gz\n",
      "|\n",
      "|  Run # 1     Epoch Start:  2018-11-09 00:00:00\n",
      "|  Run # 1     Epoch End:    2018-11-09 03:00:00\n",
      "|  Run # 1     Step Size:    10.0\n",
      "|\n",
      "|  Run # 1     ARC run:      icesat2_2018313.01_3hr.jb2008.\n",
      "+ —————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "\n",
      "Run # 1          Running IIS\n",
      "Run # 1          No errors in IIS\n",
      "Run # 1 ---------End of IIS\n",
      "\n",
      "Run # 1          Running IIE\n",
      "Run # 1          Current Time = 17:24:28 GMT-7\n",
      "Run # 1          No errors in IIE\n",
      "Run # 1 ---------End of IIE\n",
      "Run # 1          Time of IIE:  36.125688314437866 secs ( 0.6020948052406311  mins)\n",
      "Run # 1          Current Time = 00:25:04\n",
      "Run # 1                Finished renaming files\n",
      "Run # 1                Finished copying files to outputdir\n",
      "        Deleting tmp/:  jb2008_BWDRAG_arclengthtests\n",
      "MEMORY\n",
      "------\n",
      "Total      :    3.7G\n",
      "Available  :    1.8G\n",
      "Percent    :    51.0\n",
      "Used       :    1.5G\n",
      "Free       :  982.2M\n",
      "Active     :    1.9G\n",
      "Inactive   :  671.5M\n",
      "Buffers    :    0.0B\n",
      "Cached     :    1.2G\n",
      "Shared     :  182.3M\n",
      "Slab       :   65.6M\n",
      "Run # 2     Current Time =      17:25:06  GMT-7\n",
      "Run # 2\n",
      "+ —————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "|\n",
      "| ---------------------- RUN PARAMETERS  ----------------------\n",
      "|\n",
      "|  Run # 2     IISSET Cleaned      tmp/.../cleaned_setup_2018313.01\n",
      "|  Run # 2     Density Model:      jb2008\n",
      "|  Run # 2     GEODYN Version:     CD_model_proj\n",
      "|  Run # 2     Output directory:   /data/zach_work/output_from_runs/jb2008/jb2008_BWDRAG_arclengthtests\n",
      "|  Run # 2     EXAT File:          /data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.313.gz\n",
      "|\n",
      "|  Run # 2     Epoch Start:  2018-11-09 03:00:00\n",
      "|  Run # 2     Epoch End:    2018-11-09 06:00:00\n",
      "|  Run # 2     Step Size:    10.0\n",
      "|\n",
      "|  Run # 2     ARC run:      icesat2_2018313.01_3hr.jb2008.\n",
      "+ —————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "\n",
      "Run # 2          Running IIS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys  \n",
    "import pickle \n",
    "sys.path.insert(0, '/data/geodyn_proj/pygeodyn/pygeodyn_develop/')\n",
    "from PYGEODYN import Pygeodyn\n",
    "\n",
    "dont_delete = ['run_list',\n",
    "               'arc_len_list',\n",
    "               'arcs',\n",
    "               'imod',\n",
    "               'model',\n",
    "               'ihr',\n",
    "               'arclen',\n",
    "               'ifile',\n",
    "               'global_path',\n",
    "               'pprint_ntuple',\n",
    "               'main_memory', \n",
    "               'psutil',\n",
    "               'bytes2human',\n",
    "               'Pygeodyn',\n",
    "               'RunObject',\n",
    "               'gc',\n",
    "              ]\n",
    "\n",
    "\n",
    "for imod,model in enumerate(run_list):\n",
    "    for ihr,arclen in enumerate(arc_len_list): #arc_len_list\n",
    "    \n",
    "#         run_settings = global_path+'ArclengthTests_periodC_'+ model+'_'+str(arclen)+'hour.yaml'\n",
    "        for ifile in arcs[arclen].keys():\n",
    "\n",
    "            run_settings = global_path+'ArclengthTests_periodC_'+ model+'_'+str(arclen)+'hour_p'+str(ifile)+'.yaml'\n",
    "            print(arclen, 'hour FILE #',ifile)\n",
    "\n",
    "            done_list = ['jb2008_1hr_p1']\n",
    "            check_done = model+'_'+str(arclen)+'hr_p'+str(ifile)\n",
    "            if check_done  in done_list:\n",
    "                print(check_done, 'has already been run... skipping.')\n",
    "            \n",
    "\n",
    "            else:\n",
    "                ### Load the data into an object\n",
    "                RunObject = Pygeodyn(run_settings)\n",
    "                RunObject.RUN_GEODYN()\n",
    "\n",
    "\n",
    "                #### REMOVE MORE SELECTIVELY\n",
    "                import sys\n",
    "                _this = sys.modules[__name__]\n",
    "                for n in dir():\n",
    "                    if n[0]!='_' and n != 'dont_delete' and n not in dont_delete : \n",
    "                        delattr(_this, n)\n",
    "\n",
    "                del RunObject \n",
    "                gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f219d9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.441Z"
    }
   },
   "outputs": [],
   "source": [
    "RunObject.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c45cef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.443Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ec64c",
   "metadata": {},
   "source": [
    "##  GEODYN read and dump to pickle, OR load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76940776",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.446Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys  \n",
    "import os\n",
    "import pickle \n",
    "import gc\n",
    "# gc.collect()\n",
    "\n",
    "sys.path.insert(0, '/data/geodyn_proj/pygeodyn/pygeodyn_develop/')\n",
    "from PYGEODYN import Pygeodyn\n",
    "\n",
    "Obj_Geodyn={}\n",
    "global_path = '/data/zach_work/validation/investigate_arclength/'\n",
    "dir_save = '/data/zach_work/output_from_runs/icesat2_shortarc_pickles/'\n",
    "\n",
    "\n",
    "for imod,model in enumerate(run_list):\n",
    "    for ihr,arclen in enumerate(arc_len_list): #arc_len_list\n",
    "    \n",
    "        pickle_file = dir_save+'PeriodC_313_318_'+model+'.'+str(arclen)+'hr.pkl'\n",
    "\n",
    "        if not os.path.exists(pickle_file):\n",
    "\n",
    "            run_settings = global_path+'ArclengthTests_periodC_'+ model+'_'+str(arclen)+'hour.yaml'        \n",
    "\n",
    "            ### Load the data into an object\n",
    "            Obj_Geodyn[model+'.'+str(arclen)] = Pygeodyn(run_settings)\n",
    "            Obj_Geodyn[model+'.'+str(arclen)].getData_BigData_lowmemory()\n",
    "\n",
    "#             pickle_file = dir_save+'PeriodC_313_318_'+i+'hr.pkl'\n",
    "\n",
    "            #### Pickle the object to save it\n",
    "            print('   ', 'Saving pickle')\n",
    "            filehandler = open(pickle_file, 'wb') \n",
    "            pickle.dump(Obj_Geodyn[model+'.'+str(arclen)], filehandler)\n",
    "            filehandler.close()\n",
    "            print('   ', 'Saved pickle',pickle_file)\n",
    "            \n",
    "        else:\n",
    "            print(model,'.',arclen,'  Pickle created -- will load in next step', sep='')\n",
    "            \n",
    "\n",
    "### Load the data if the pickles exist\n",
    "print()\n",
    "print()\n",
    "gc.collect()\n",
    "\n",
    "Obj_Geodyn = {}\n",
    "\n",
    "for imod,model in enumerate(['jb2008']):\n",
    "    for ihr,arclen in enumerate([3,6,12,24]):    \n",
    "#         pickle_file = dir_save+'PeriodC_313_318_'+str(arclen)+'hr.pkl'\n",
    "        pickle_file = dir_save+'PeriodC_313_318_'+model+'.'+str(arclen)+'hr.pkl'\n",
    "\n",
    "\n",
    "        if os.path.exists(pickle_file):\n",
    "            filehandler = open(pickle_file, 'rb') \n",
    "            Obj_Geodyn[model+'.'+str(arclen)] = pickle.load(filehandler)\n",
    "            filehandler.close()\n",
    "            print('Loaded data... \\n     ',pickle_file)\n",
    "\n",
    "            \n",
    "# for imod,model in enumerate(['msis2']):\n",
    "#     for ihr,arclen in enumerate([3,6]):    \n",
    "# #         pickle_file = dir_save+'PeriodC_313_318_'+str(arclen)+'hr.pkl'\n",
    "#         pickle_file = dir_save+'PeriodC_313_318_'+model+'.'+str(arclen)+'hr.pkl'\n",
    "\n",
    "\n",
    "#         if os.path.exists(pickle_file):\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             Obj_Geodyn[model+'.'+str(arclen)] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "#             print('Loaded data... \\n     ',pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b09bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T20:22:16.273752Z",
     "start_time": "2022-03-24T20:22:16.250585Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf26b7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.450Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ### Load the data if the pickles exist\n",
    "# print()\n",
    "# print()\n",
    "# gc.collect()\n",
    "\n",
    "# Obj_Geodyn = {}\n",
    "# for i,val in enumerate(run_list): #,'jb2008','msis2'\n",
    "#     for ii in ['_p1', '_p2']:\n",
    "        \n",
    "#         dir_save = '/data/zach_work/output_from_runs/icesat2_3monthrun_pickles/'\n",
    "#         pickle_file = dir_save+'Fulltime_valid_'+val+ii+'.pkl'\n",
    "#         filehandler = open(pickle_file, 'rb') \n",
    "#         Obj_Geodyn[val+ii] = pickle.load(filehandler)\n",
    "#         filehandler.close()\n",
    "#         print('Loaded data... ', val+ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41746c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obj_Geodyn['jb2008.6'].__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54354cff",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45df3b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.457Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "config = dict({\n",
    "                'displayModeBar': True,\n",
    "                'responsive': False,\n",
    "                'staticPlot': False,\n",
    "                'displaylogo': False,\n",
    "                'showTips': False,\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# px.colors.colorscale_to_colors()\n",
    "# plotly.colors.PLOTLY_SCALES[\"Viridis\"]\n",
    "\n",
    "def get_color(colorscale_name, loc):\n",
    "    from _plotly_utils.basevalidators import ColorscaleValidator\n",
    "    # first parameter: Name of the property being validated\n",
    "    # second parameter: a string, doesn't really matter in our use case\n",
    "    cv = ColorscaleValidator(\"colorscale\", \"\")\n",
    "    # colorscale will be a list of lists: [[loc1, \"rgb1\"], [loc2, \"rgb2\"], ...] \n",
    "    colorscale = cv.validate_coerce(colorscale_name)\n",
    "    \n",
    "    if hasattr(loc, \"__iter__\"):\n",
    "        return [get_continuous_color(colorscale, x) for x in loc]\n",
    "    return get_continuous_color(colorscale, loc)\n",
    "        \n",
    "\n",
    "# Identical to Adam's answer\n",
    "import plotly.colors\n",
    "from PIL import ImageColor\n",
    "\n",
    "def get_continuous_color(colorscale, intermed):\n",
    "    \"\"\"\n",
    "    Plotly continuous colorscales assign colors to the range [0, 1]. This function computes the intermediate\n",
    "    color for any value in that range.\n",
    "\n",
    "    Plotly doesn't make the colorscales directly accessible in a common format.\n",
    "    Some are ready to use:\n",
    "    \n",
    "        colorscale = plotly.colors.PLOTLY_SCALES[\"Greens\"]\n",
    "\n",
    "    Others are just swatches that need to be constructed into a colorscale:\n",
    "\n",
    "        viridis_colors, scale = plotly.colors.convert_colors_to_same_type(plotly.colors.sequential.Viridis)\n",
    "        colorscale = plotly.colors.make_colorscale(viridis_colors, scale=scale)\n",
    "\n",
    "    :param colorscale: A plotly continuous colorscale defined with RGB string colors.\n",
    "    :param intermed: value in the range [0, 1]\n",
    "    :return: color in rgb string format\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if len(colorscale) < 1:\n",
    "        raise ValueError(\"colorscale must have at least one color\")\n",
    "\n",
    "    hex_to_rgb = lambda c: \"rgb\" + str(ImageColor.getcolor(c, \"RGB\"))\n",
    "\n",
    "    if intermed <= 0 or len(colorscale) == 1:\n",
    "        c = colorscale[0][1]\n",
    "        return c if c[0] != \"#\" else hex_to_rgb(c)\n",
    "    if intermed >= 1:\n",
    "        c = colorscale[-1][1]\n",
    "        return c if c[0] != \"#\" else hex_to_rgb(c)\n",
    "\n",
    "    for cutoff, color in colorscale:\n",
    "        if intermed > cutoff:\n",
    "            low_cutoff, low_color = cutoff, color\n",
    "        else:\n",
    "            high_cutoff, high_color = cutoff, color\n",
    "            break\n",
    "\n",
    "    if (low_color[0] == \"#\") or (high_color[0] == \"#\"):\n",
    "        # some color scale names (such as cividis) returns:\n",
    "        # [[loc1, \"hex1\"], [loc2, \"hex2\"], ...]\n",
    "        low_color = hex_to_rgb(low_color)\n",
    "        high_color = hex_to_rgb(high_color)\n",
    "\n",
    "    return plotly.colors.find_intermediate_color(\n",
    "        lowcolor=low_color,\n",
    "        highcolor=high_color,\n",
    "        intermed=((intermed - low_cutoff) / (high_cutoff - low_cutoff)),\n",
    "        colortype=\"rgb\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols = get_color(\"Viridis\", np.linspace(0, 1, 4))\n",
    "map_cols = np.linspace(0, 1, 4)\n",
    "colorscale=[]\n",
    "for i,val in enumerate(map_cols):\n",
    "    colorscale.append([val, cols[i]])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Simplify Plotting Schemes:\n",
    "col1 =  px.colors.qualitative.Plotly[2]\n",
    "col2 =  px.colors.qualitative.Plotly[4]\n",
    "col3 =  px.colors.qualitative.Plotly[1]\n",
    "col4 =  px.colors.qualitative.Plotly[3]\n",
    "col5 =  px.colors.qualitative.Plotly[4]\n",
    "col6 =  px.colors.qualitative.Plotly[5]\n",
    "\n",
    "\n",
    "\n",
    "# Simplify Plotting Schemes:\n",
    "col_msis2 =  px.colors.qualitative.Plotly[2]\n",
    "col_jb2008 =  px.colors.qualitative.Plotly[4]\n",
    "col_dtm2020 =  px.colors.qualitative.Plotly[1]\n",
    "col_tiegcm_oc =  px.colors.qualitative.Plotly[5]\n",
    "\n",
    "x_annot_val = 1.1\n",
    "\n",
    "\n",
    "\n",
    "def get_plot_params(plot_num, model_name_string):\n",
    "    '''\n",
    "    INPUT:   \n",
    "        Plot number, model_name string, x_annot_val\n",
    "    \n",
    "    RETURN:\n",
    "        col\n",
    "        x_annot\n",
    "        y_annot1\n",
    "        y_annot2\n",
    "        m_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if plot_num == 0:\n",
    "        col = col1\n",
    "        x_annot = x_annot_val\n",
    "        y_annot1 = 1\n",
    "        y_annot2 = .9\n",
    "        m_size = 3.5\n",
    "    elif plot_num == 1:\n",
    "        x_annot = x_annot_val\n",
    "        y_annot1 = .8\n",
    "        y_annot2 = .8\n",
    "        col = col2\n",
    "        m_size = 3.5\n",
    "    elif plot_num == 2:\n",
    "        x_annot = x_annot_val\n",
    "        y_annot1 = .5\n",
    "        y_annot2 = .7\n",
    "        col = col3\n",
    "        m_size = 3.5\n",
    "    elif plot_num == 3:\n",
    "        x_annot = x_annot_val\n",
    "        y_annot1 = .2\n",
    "        y_annot2 = .55\n",
    "        col = col4\n",
    "        m_size = 3.5\n",
    "    elif plot_num == 4:\n",
    "        x_annot = x_annot_val\n",
    "        y_annot1 =  0 \n",
    "        y_annot2 = .45\n",
    "        col = col5\n",
    "        m_size = 3.5\n",
    "        \n",
    "    elif plot_num == 5:\n",
    "        x_annot = x_annot_val\n",
    "        y_annot1 =  0 \n",
    "        y_annot2 = .35\n",
    "        col = col6\n",
    "        m_size = 3.5\n",
    "        \n",
    "    if model_name_string == 'msis2':\n",
    "        col=col_msis2\n",
    "    elif model_name_string == 'dtm2020':\n",
    "        col=col_dtm2020\n",
    "    elif model_name_string == 'jb2008':\n",
    "        col=col_jb2008\n",
    "    elif model_name_string == 'tiegcm_oc':\n",
    "        col=col_tiegcm_oc\n",
    "        \n",
    "    ### Old Models\n",
    "    elif model_name_string == 'dtm87':\n",
    "        col='grey'\n",
    "    elif model_name_string == 'jaachia71':\n",
    "        col='grey'\n",
    "    elif model_name_string == 'msis86':\n",
    "        col='grey'\n",
    "    elif model_name_string == 'msis00':\n",
    "        col='tan'\n",
    "\n",
    "        \n",
    "        \n",
    "    return(col,x_annot,y_annot1,y_annot2,m_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def orb_avg(den_df, arc):\n",
    "    \n",
    "    \n",
    "    #### Find the index for the correct date\n",
    "    vals  = np.arange(den_df[arc].index[0],den_df[arc].index[-1]+1)\n",
    "    df = den_df[arc].set_index('Date',drop=False ) \n",
    "    df['i_vals'] = vals\n",
    "    index_date = df.loc[df.index.max()]['i_vals'].min()\n",
    "    \n",
    "#     print('index_date', index_date)\n",
    "    lat = np.asarray(den_df[arc]['Lat'][:index_date])\n",
    "    time_pd = pd.to_datetime(den_df[arc]['Date'][:index_date])\n",
    "    i = np.nonzero( lat[1:]*lat[0:-1]  <  np.logical_and(0 , lat[1:] > lat[0:-1] )  )\n",
    "    i = i[0]\n",
    "\n",
    "    d_avg = np.zeros(np.size(i))\n",
    "    height_avg = np.zeros(np.size(i))\n",
    "    \n",
    "#     print('time_pd',time_pd)\n",
    "\n",
    "    time_avg = []\n",
    "    d_avg_rolling = []\n",
    "    \n",
    "    roll_avg_count = 0\n",
    "    for j in range(np.size(i)-1):\n",
    "        d_avg[j]      = np.mean(den_df[arc]['rho (kg/m**3)'  ][i[j] : i[j+1]-1  ]  )\n",
    "        height_avg[j] = np.mean(den_df[arc]['Height (meters)'][i[j] : i[j+1]-1  ]  )\n",
    "#         mean_time      = np.mean(time_pd[   i[j] : i[j+1]-1  ])\n",
    "        t1 = pd.to_datetime(time_pd[ i[j]    ])\n",
    "        t2 = pd.to_datetime(time_pd[ i[j+1]-1])\n",
    "        datemiddle = pd.Timestamp(t1) + (pd.Timestamp(t2) - pd.Timestamp(t1)) / 2\n",
    "\n",
    "        time_avg.append(datemiddle)\n",
    "\n",
    "        if roll_avg_count ==1:\n",
    "            d_avg_rolling.append(np.mean([ d_avg[j],  d_avg[j-1]]))\n",
    "            roll_avg_count =0\n",
    "            \n",
    "        roll_avg_count+=1 \n",
    "    d_avg_rolling.append(np.mean([ d_avg[j],  d_avg[j-1]]))\n",
    "        \n",
    "    return(time_avg, d_avg, d_avg_rolling )\n",
    "    \n",
    "\n",
    "def plot_density_orbit_avg(fig, obj_m1, plot_num ):\n",
    "    \n",
    "\n",
    "    ####  Get plot Parameters for this model\n",
    "    model_m1 = obj_m1.__dict__['global_params']['den_model']\n",
    "    col,x_annot,y_annot1,y_annot2,m_size = get_plot_params(plot_num, model_m1)\n",
    "    \n",
    "    for ii,arc in enumerate(obj_m1.__dict__['global_params']['arc_input']):\n",
    "        \n",
    "        vals  = np.arange(obj_m1.__dict__['Density'][arc].index[0],obj_m1.__dict__['Density'][arc].index[-1]+1)\n",
    "        df = obj_m1.__dict__['Density'][arc].set_index('Date',drop=False ) \n",
    "        df['i_vals'] = vals\n",
    "        index_date = df.loc[df.index.max()]['i_vals'].min()\n",
    "\n",
    "        \n",
    "        time_avg,d_avg, d_avg_rolling = orb_avg(obj_m1.Density, arc)\n",
    "        \n",
    "        \n",
    "        fig.add_trace(go.Scattergl(x=time_avg,\n",
    "                                 y=d_avg_rolling,\n",
    "#                                  y=d_avg,\n",
    "                                name= model_m1 ,\n",
    "                                mode='markers',\n",
    "                                marker=dict(\n",
    "                                color=col,\n",
    "                                size=7,),\n",
    "                                showlegend=False,\n",
    "                                   ),\n",
    "                                   row=1, col=1,\n",
    "                                   )\n",
    "        \n",
    "\n",
    "        fig.update_yaxes(type=\"log\", exponentformat= 'power',row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"kg/m^3\", row=1, col=1)\n",
    "#     fig.update_yaxes(title_text=\"nT\", row=2, col=1)\n",
    "#     fig.update_yaxes(title_text=\"sfu\", row=3, col=1)\n",
    "    fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "#     fig.update_layout(\n",
    "#         font=dict(          size=18, ),\n",
    "#         autosize=False,\n",
    "#         width=900,\n",
    "#         height=1000,)\n",
    "    \n",
    "    return(fig)\n",
    "\n",
    "\n",
    "\n",
    "def legend_as_annotation(fig, den_model_string, color_it, x_annot, y_annot):\n",
    "    fig.add_annotation(\n",
    "            x=x_annot,\n",
    "            y=y_annot,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            showarrow=False,\n",
    "            text=den_model_string,\n",
    "            font=dict(\n",
    "                size=16,\n",
    "                color=\"#ffffff\"\n",
    "                ),\n",
    "            align=\"center\",\n",
    "            bordercolor=\"#c7c7c7\",\n",
    "            borderwidth=2,\n",
    "            borderpad=4,\n",
    "            bgcolor=color_it,\n",
    "            opacity=0.9\n",
    "            )\n",
    "\n",
    "    return(fig)\n",
    "\n",
    "def add_stats_annotation(fig, text_in, col, x_annot, y_annot):\n",
    "    fig.add_annotation(\n",
    "            x=x_annot,\n",
    "            y=y_annot,\n",
    "            xref=\"x domain\",\n",
    "            yref=\"y domain\",\n",
    "            showarrow=False,\n",
    "            text=text_in,\n",
    "            font=dict(\n",
    "                size=13,\n",
    "                color=\"#ffffff\"\n",
    "                ),\n",
    "            align=\"center\",\n",
    "            bordercolor=\"#c7c7c7\",\n",
    "            borderwidth=2,\n",
    "            borderpad=4,\n",
    "            bgcolor=col,\n",
    "            opacity=0.9\n",
    "            )\n",
    "    return(fig)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def STATS_residuals(residuals,measurement_type):\n",
    "    import numpy as np\n",
    "    n = np.size(residuals)\n",
    "    mean = (1/n)*(np.sum(residuals))\n",
    "    variance = (1/n)*(np.sum(np.square(residuals)))\n",
    "    rms = np.sqrt(variance)\n",
    "    rms_about_zero = np.sqrt((n/(n-1))*variance)\n",
    "    \n",
    "                \n",
    "#     print('mean            ',measurement_type ,':',mean)\n",
    "#     print('rms             ',measurement_type ,':',rms)\n",
    "#     print('rms about zero  ',measurement_type ,':',rms_about_zero)\n",
    "#     print()\n",
    "    \n",
    "    return(mean,rms,rms_about_zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4ec09",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.459Z"
    }
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "def read_nc_file( filename, variables):\n",
    "    ''' This function reads the TIEGCM .nc files and saves the given input variables to a dictionary.\n",
    "        The breakloop feature is here so that if the file doesn't exist the code can still continue.  '''\n",
    "    status = os.path.exists(filename)\n",
    "    \n",
    "    if status == True:\n",
    "        data = {}\n",
    "        for i, var_names in enumerate(variables):\n",
    "            ncid =  Dataset(filename,\"r+\", format=\"NETCDF4\")# filename must be a string\n",
    "            varData = ncid.variables\n",
    "            data[var_names] = np.array(varData[var_names])  \n",
    "    elif status == False:\n",
    "        print('No File Found', filename )\n",
    "        breakloop = True\n",
    "        data = 0\n",
    "        return( data , breakloop)\n",
    "    breakloop = False\n",
    "    return(data,breakloop )\n",
    "\n",
    "\n",
    "arc_list = []\n",
    "\n",
    "arc_list_18 = np.arange(292,366)\n",
    "for i in arc_list_18:\n",
    "    val = '2018'+str(i)\n",
    "    arc_list.append(int(val))\n",
    "    \n",
    "    #     print(val)\n",
    "    \n",
    "arc_list_19 = np.arange(1,10)\n",
    "for i in arc_list_19:\n",
    "    val = '201900'+str(i)\n",
    "    arc_list.append(int(val))\n",
    "\n",
    "\n",
    "path_to_f107 = '/data/data_geodyn/gpi_1960001-2021243_f107aDaily.nc'\n",
    "variables = ['year_day', 'f107d', 'f107a', 'kp']\n",
    "f107_data = read_nc_file(path_to_f107, variables)\n",
    "\n",
    "date = []\n",
    "kp_list = []\n",
    "f107d_list = []\n",
    "f107a_list  = []\n",
    "date_3hr = []\n",
    "\n",
    "\n",
    "for i,val in enumerate(arc_list):\n",
    "    \n",
    "    index = f107_data[0]['year_day']==val\n",
    "    kp_list.append(f107_data[0]['kp'][index][0])\n",
    "    f107d_list.append(f107_data[0]['f107d'][index][0])\n",
    "    f107a_list.append(f107_data[0]['f107a'][index][0])\n",
    "    \n",
    "    date.append(pd.to_datetime( str(val), format='%Y%j'))\n",
    "\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=0))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=3))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=6))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=9))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=12))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=15))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=18))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=21))\n",
    "#     date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=24))\n",
    "    \n",
    "kp_expand = []\n",
    "for i in kp_list:\n",
    "    for ii in i:\n",
    "        kp_expand.append(ii)\n",
    "        \n",
    "        \n",
    "        \n",
    "solar_fluxes = {}\n",
    "\n",
    "solar_fluxes['f107d_list'] = f107d_list\n",
    "solar_fluxes['f107a_list'] = f107a_list\n",
    "solar_fluxes['date']       = date\n",
    "solar_fluxes['date_3hr']   = date_3hr\n",
    "solar_fluxes['kp_expand']  = kp_expand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d5cbc",
   "metadata": {},
   "source": [
    "### plotall func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2f397",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.462Z"
    }
   },
   "outputs": [],
   "source": [
    "# cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f6de5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.464Z"
    }
   },
   "outputs": [],
   "source": [
    "# obj_m1 = Obj_Geodyn['jb2008.6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb08b48",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.467Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# obj_m1.__dict__['global_params']['arc_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b68d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T19:52:05.192646Z",
     "start_time": "2022-03-24T19:52:05.157191Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8fb5c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.473Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = get_color(\"Viridis\", np.linspace(0, 1, 4))\n",
    "map_cols = np.linspace(0, 1, 4)\n",
    "colorscale=[]\n",
    "for i,val in enumerate(map_cols):\n",
    "    colorscale.append([val, cols[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7806c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.476Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_all_shortarcs(fig, obj_m1, plot_num, solar_fluxes, TWOWEEK_FLAG=True ):\n",
    "    \n",
    "    cols = get_color(\"Viridis\", np.linspace(0, 1, 4))\n",
    "    map_cols = np.linspace(0, 1, 4)\n",
    "    colorscale=[]\n",
    "    for i,val in enumerate(map_cols):\n",
    "        colorscale.append([val, cols[i]])\n",
    "\n",
    "    ####  Get plot Parameters for this model\n",
    "    model_m1 = obj_m1.__dict__['global_params']['den_model']\n",
    "    col,x_annot,y_annot1,y_annot2,m_size = get_plot_params(plot_num, model_m1)\n",
    "    \n",
    "    #### -----------------------------------------------------------------------------------------------------\n",
    "    #### Solar Flux Plot\n",
    "    \n",
    "    if TWOWEEK_FLAG==True:\n",
    "        index1 = 21\n",
    "        index2 = 36\n",
    "        index3h_1 = 168\n",
    "        index3h_2 = 281\n",
    "        \n",
    "    else:\n",
    "        index1 = 0\n",
    "        index2 =-1\n",
    "        index3h_1 = 0\n",
    "        index3h_2 =-1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    fig.add_trace(go.Scattergl(x=solar_fluxes['date_3hr'][index3h_1:index3h_2],\n",
    "                               y=solar_fluxes['kp_expand'][index3h_1:index3h_2],\n",
    "                               name= 'Kp',\n",
    "                               mode='markers+lines',\n",
    "                               opacity=1,\n",
    "                               marker=dict(color='cornflowerblue',size=4,),\n",
    "                               showlegend=False),\n",
    "                               secondary_y=True,row=1, col=1) \n",
    "\n",
    "    fig.add_trace(go.Scattergl(  x=date[index1:index2],\n",
    "                                 y=f107d_list[index1:index2],\n",
    "                                 name= 'F107d',\n",
    "                                 mode='markers+lines',\n",
    "                                 opacity=.8,\n",
    "                                 marker=dict( color='black', size=4, ),\n",
    "                                 showlegend=False,\n",
    "                              ),\n",
    "                                  secondary_y=False,\n",
    "                                   row=1, col=1,\n",
    "                              )\n",
    "\n",
    "    \n",
    "#     #### Arc Background \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Prepare RMS total Plot arrays\n",
    "\n",
    "    arc_listlist=   ['2018.313', '2018.314', '2018.315', '2018.316', '2018.317',\n",
    "                     '2018.318', '2018.319', '2018.320', '2018.321', '2018.322',\n",
    "                     '2018.323', '2018.324', '2018.325', '2018.326', '2018.327' ]  \n",
    "                    \n",
    "#     for i1,arc1 in enumerate(arc_listlist):#obj_m1.__dict__['global_params']['arc_input']):\n",
    "    uniquelist = []\n",
    "    dateplot = []\n",
    "    rms_totals = []\n",
    "    count=0\n",
    "\n",
    "    for ii,arcval in enumerate(obj_m1.__dict__['global_params']['arc_input']):#obj_m1.__dict__['global_params']['arc_input']):\n",
    "\n",
    "        if arcval not in uniquelist:\n",
    "            uniquelist.append(arcval)\n",
    "            count=0\n",
    "        else:\n",
    "            count+=1\n",
    "\n",
    "        txt = obj_m1.__dict__['global_params']['arc_length']\n",
    "        chars = [s for s in [char for char in txt] if s.isdigit()]\n",
    "        arc_length = int(''.join(chars))\n",
    "\n",
    "        \n",
    "        if arc_length==24:\n",
    "            arc = arcval+ '.01'\n",
    "        else:\n",
    "            arc = arcval+ '.%02d'  %  (ii+1)\n",
    "\n",
    "        \n",
    "        dateplot.append(pd.to_datetime(datetime.datetime(int(arc.split('.')[0]), 1, 1) + datetime.timedelta(int(arc.split('.')[1])-1) + datetime.timedelta(hours=arc_length*count)+ datetime.timedelta(hours=arc_length/2) ))\n",
    "        rms_totals.append(obj_m1.__dict__['Statistics'][arc]['RMS_total_NTW'].values[0])\n",
    "        ### -----------------------------------------------------------------------------------------------------\n",
    "        ###     DENSITY\n",
    "        ###\n",
    "        ## Remove the denisty file duplication\n",
    "#         vals  = np.arange(obj_m1.__dict__['Density'][arc].index[0],obj_m1.__dict__['Density'][arc].index[-1]+1)\n",
    "#         df = obj_m1.__dict__['Density'][arc].set_index('Date',drop=False ) \n",
    "#         df['i_vals'] = vals\n",
    "#         index_date = df.loc[df.index.max()]['i_vals'].min()\n",
    "#         time_avg,d_avg, d_avg_rolling = orb_avg(obj_m1.Density, arc)\n",
    "#         #\n",
    "#         fig.add_trace(go.Scattergl(x=time_avg,\n",
    "#                                    y=d_avg_rolling,\n",
    "#                                    name= model_m1,\n",
    "#                                    mode='markers',\n",
    "#                                    marker=dict(color=cols[plot_num],size=4),\n",
    "#                                    showlegend=False),\n",
    "#                                    row=2, col=1)\n",
    "        ### -----------------------------------------------------------------------------------------------------\n",
    "        ###     TOTAL RMS\n",
    "        ###\n",
    "        \n",
    "        fig.add_trace(go.Scattergl(x=dateplot,\n",
    "                                   y=rms_totals,\n",
    "                                   name= 'NTW '+model_m1,\n",
    "                                   mode='markers+lines',\n",
    "                                   marker=dict(color=cols[plot_num],size=6),\n",
    "                                   showlegend=False),\n",
    "                                   row=2, col=1)\n",
    "\n",
    "        data_resids = obj_m1.__dict__['OrbitResids'][arc]['resids']\n",
    "        fig.add_trace(go.Scattergl(x=data_resids['Date'][::10],\n",
    "                                   y=data_resids['T'][::10],\n",
    "                                     name= model_m1,\n",
    "                                     mode='markers+lines',opacity=0.9,\n",
    "                                     marker=dict(color=cols[plot_num],size=3),\n",
    "                                     showlegend=False),\n",
    "                                         secondary_y=False, row=3, col=1)\n",
    "\n",
    "    ### -----------------------------------------------------------------------------------------------------\n",
    "    ###     TOTAL RMS\n",
    "    ###\n",
    "#     fig.add_trace(go.Scattergl(x=doy_dates,\n",
    "#                                y=rms_totals,\n",
    "#                                name= 'NTW '+model_m1,\n",
    "#                                mode='markers+lines',\n",
    "#                                marker=dict(color=col,size=8),\n",
    "#                                showlegend=False),\n",
    "#                                row=3, col=1)\n",
    "    \n",
    "\n",
    "    ### DENSITY AXIS\n",
    "#     fig.update_yaxes(title_text=r\"$\\frac{kg}{m^3}$\", type=\"log\", exponentformat= 'power',row=2, col=1)\n",
    "    \n",
    "    ### RMS AXIS\n",
    "    fig.update_yaxes( title=\"Total RMS (m)\" ,type=\"linear\" , exponentformat= 'power', row=2, col=1)\n",
    "\n",
    "    ### InTrack Residual Axis\n",
    "    fig.update_yaxes( title=\"Residuals (m)\",exponentformat= 'power',row=3, col=1)\n",
    "    \n",
    "    \n",
    "    ###  DATE on Final X-Axis\n",
    "    fig.update_xaxes( title=\"Date\", row=3, col=1)\n",
    "\n",
    "    return(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1decd",
   "metadata": {},
   "source": [
    "### the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f09d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.478Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1,\n",
    "                    subplot_titles=(['Solar Flux Indices (F10.7 (black), Kp (Blue))',\n",
    "#                                      'Orbit Averaged Density',\n",
    "                                     'Total RMS of NTW Coordinates',\n",
    "                                     'In-Track Residuals',]),\n",
    "\n",
    "                    specs=[[ {\"secondary_y\": True} ],\n",
    "#                            [ {\"secondary_y\": False} ],\n",
    "                           [ {\"secondary_y\": False} ],\n",
    "                           [ {\"secondary_y\": False} ]],\n",
    "                    vertical_spacing = 0.12,\n",
    "                    shared_xaxes=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.3'], 0, solar_fluxes, True)\n",
    "fig = plot_all_shortarcs(fig, Obj_Geodyn['msis2.3'], 1, solar_fluxes, True)\n",
    "# fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.6'], 1, solar_fluxes, True)\n",
    "# fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.12'], 2, solar_fluxes, True)\n",
    "# fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.24'], 3, solar_fluxes, True)\n",
    "\n",
    "\n",
    "font_dict=dict(family='sans-serif',size=14,color='black')\n",
    "fig.update_xaxes(showline=True,\n",
    "                 showticklabels=True,\n",
    "                 linecolor='black',\n",
    "                 linewidth=1,\n",
    "                 ticks='inside',\n",
    "                 tickfont=font_dict,\n",
    "                 mirror='allticks',\n",
    "                 tickwidth=1.4,\n",
    "                 tickcolor='black',\n",
    "                 gridcolor='lightgray',\n",
    "                 layer='above traces')\n",
    "\n",
    "\n",
    "fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                 linecolor='black',  # line color\n",
    "                 linewidth=1,        # line size\n",
    "                 ticks='inside',     # ticks outside axis\n",
    "                 tickfont=font_dict, # tick label font\n",
    "                 mirror='allticks',  # add ticks to top/right axes\n",
    "                 tickwidth=1.4,      # tick width\n",
    "                 tickcolor='black',  # tick color\n",
    "                 gridcolor='lightgray',\n",
    "                 layer='above traces')\n",
    "\n",
    "fig.update_layout(\n",
    "#                   title = '',\n",
    "                  autosize=False,    width=1000,    height=800,\n",
    "                  legend= {'itemsizing': 'constant'},\n",
    "                  font=font_dict,\n",
    "                  plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set Secondary Y-axis titles\n",
    "fig.update_yaxes(title_text=\"Kp\", color='cornflowerblue', secondary_y=True, row=1, col=1,\n",
    "                 showline=True,      # add line at x=0\n",
    "                 linecolor='cornflowerblue',  # line color\n",
    "                 linewidth=0.1,        # line size\n",
    "                 ticks='inside',     # ticks outside axis\n",
    "                 tickfont=dict(family='sans-serif',size=14,color='cornflowerblue'), # tick label font\n",
    "                 mirror=False,  # add ticks to top/right axes\n",
    "                 tickwidth=1,      # tick width\n",
    "                 tickcolor='cornflowerblue',   # tick color\n",
    "                 gridcolor='white',\n",
    "                 anchor=\"x\", overlaying=\"y\", side=\"right\",\n",
    "                layer='above traces')\n",
    "\n",
    "fig.update_yaxes(title_text=\"Daily F10.7\", secondary_y=False, row=1, col=1,\n",
    "                 showline=True,      # add line at x=0\n",
    "                 linecolor='black',  # line color\n",
    "                 linewidth=0.1,        # line size\n",
    "                 ticks='inside',     # ticks outside axis\n",
    "                 tickfont=dict(family='sans-serif',size=14,color='black'), # tick label font\n",
    "                 mirror=False,  # add ticks to top/right axes\n",
    "                 tickwidth=1,      # tick width\n",
    "                 tickcolor='black',   # tick color\n",
    "                 gridcolor='white',\n",
    "                 layer='above traces')\n",
    "\n",
    "\n",
    "x_annot = 1.09\n",
    "y_shift = 0.05\n",
    "fig.add_annotation(x= x_annot  , y=.8-y_shift,\n",
    "                   xref=\"paper\", yref=\"paper\",\n",
    "                   showarrow=False, text='3hr'.center(8,' '),\n",
    "                   font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "                   align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "                   borderwidth=2,borderpad=4, bgcolor=cols[0],  opacity=0.9)\n",
    "\n",
    "fig.add_annotation(x= x_annot  , y=.7-y_shift,\n",
    "                   xref=\"paper\", yref=\"paper\",\n",
    "                   showarrow=False, text='6hr'.center(8,' '),\n",
    "                   font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "                   align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "                   borderwidth=2,borderpad=4, bgcolor=cols[1],  opacity=0.9)\n",
    "\n",
    "fig.add_annotation(x= x_annot  , y=.6-y_shift,\n",
    "                   xref=\"paper\", yref=\"paper\",\n",
    "                   showarrow=False, text='12hr'.center(8,' '),\n",
    "                   font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "                   align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "                   borderwidth=2,borderpad=4, bgcolor=cols[2],  opacity=0.9)\n",
    "\n",
    "fig.add_annotation(x= x_annot  , y=.5-y_shift,\n",
    "                   xref=\"paper\", yref=\"paper\",\n",
    "                   showarrow=False, text='24hr'.center(8,' '),\n",
    "                   font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "                   align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "                   borderwidth=2,borderpad=4, bgcolor=cols[3],  opacity=0.9)\n",
    "\n",
    "\n",
    "fig.update_annotations(font_size=18)  # Increase size of subplot title\n",
    "\n",
    "\n",
    "\n",
    "fig.update_xaxes(range=[solar_fluxes['date'][21], solar_fluxes['date'][27]])\n",
    "\n",
    "fig.show(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f4e93",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.481Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1,\n",
    "                    subplot_titles=(['Solar Flux Indices (F10.7 (black), Kp (Blue))',\n",
    "#                                      'Orbit Averaged Density',\n",
    "                                     'Total RMS of NTW Coordinates',\n",
    "                                     'In-Track Residuals',]),\n",
    "\n",
    "                    specs=[[ {\"secondary_y\": True} ],\n",
    "#                            [ {\"secondary_y\": False} ],\n",
    "                           [ {\"secondary_y\": False} ],\n",
    "                           [ {\"secondary_y\": False} ]],\n",
    "                    vertical_spacing = 0.12,\n",
    "                    shared_xaxes=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.3'], 0, solar_fluxes, True)\n",
    "fig = plot_all_shortarcs(fig, Obj_Geodyn['msis2.3'], 1, solar_fluxes, True)\n",
    "# fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.6'], 1, solar_fluxes, True)\n",
    "# fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.12'], 2, solar_fluxes, True)\n",
    "# fig = plot_all_shortarcs(fig, Obj_Geodyn['jb2008.24'], 3, solar_fluxes, True)\n",
    "\n",
    "\n",
    "font_dict=dict(family='sans-serif',size=14,color='black')\n",
    "fig.update_xaxes(showline=True,\n",
    "                 showticklabels=True,\n",
    "                 linecolor='black',\n",
    "                 linewidth=1,\n",
    "                 ticks='inside',\n",
    "                 tickfont=font_dict,\n",
    "                 mirror='allticks',\n",
    "                 tickwidth=1.4,\n",
    "                 tickcolor='black',\n",
    "                 gridcolor='lightgray',\n",
    "                 layer='above traces')\n",
    "\n",
    "\n",
    "fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                 linecolor='black',  # line color\n",
    "                 linewidth=1,        # line size\n",
    "                 ticks='inside',     # ticks outside axis\n",
    "                 tickfont=font_dict, # tick label font\n",
    "                 mirror='allticks',  # add ticks to top/right axes\n",
    "                 tickwidth=1.4,      # tick width\n",
    "                 tickcolor='black',  # tick color\n",
    "                 gridcolor='lightgray',\n",
    "                 layer='above traces')\n",
    "\n",
    "fig.update_layout(\n",
    "#                   title = '',\n",
    "                  autosize=False,    width=1000,    height=800,\n",
    "                  legend= {'itemsizing': 'constant'},\n",
    "                  font=font_dict,\n",
    "                  plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set Secondary Y-axis titles\n",
    "fig.update_yaxes(title_text=\"Kp\", color='cornflowerblue', secondary_y=True, row=1, col=1,\n",
    "                 showline=True,      # add line at x=0\n",
    "                 linecolor='cornflowerblue',  # line color\n",
    "                 linewidth=0.1,        # line size\n",
    "                 ticks='inside',     # ticks outside axis\n",
    "                 tickfont=dict(family='sans-serif',size=14,color='cornflowerblue'), # tick label font\n",
    "                 mirror=False,  # add ticks to top/right axes\n",
    "                 tickwidth=1,      # tick width\n",
    "                 tickcolor='cornflowerblue',   # tick color\n",
    "                 gridcolor='white',\n",
    "                 anchor=\"x\", overlaying=\"y\", side=\"right\",\n",
    "                layer='above traces')\n",
    "\n",
    "fig.update_yaxes(title_text=\"Daily F10.7\", secondary_y=False, row=1, col=1,\n",
    "                 showline=True,      # add line at x=0\n",
    "                 linecolor='black',  # line color\n",
    "                 linewidth=0.1,        # line size\n",
    "                 ticks='inside',     # ticks outside axis\n",
    "                 tickfont=dict(family='sans-serif',size=14,color='black'), # tick label font\n",
    "                 mirror=False,  # add ticks to top/right axes\n",
    "                 tickwidth=1,      # tick width\n",
    "                 tickcolor='black',   # tick color\n",
    "                 gridcolor='white',\n",
    "                 layer='above traces')\n",
    "\n",
    "\n",
    "x_annot = 1.09\n",
    "y_shift = 0.05\n",
    "fig.add_annotation(x= x_annot  , y=.8-y_shift,\n",
    "                   xref=\"paper\", yref=\"paper\",\n",
    "                   showarrow=False, text='jb2008'.center(8,' '),\n",
    "                   font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "                   align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "                   borderwidth=2,borderpad=4, bgcolor=cols[0],  opacity=0.9)\n",
    "\n",
    "fig.add_annotation(x= x_annot  , y=.7-y_shift,\n",
    "                   xref=\"paper\", yref=\"paper\",\n",
    "                   showarrow=False, text='msis2'.center(8,' '),\n",
    "                   font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "                   align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "                   borderwidth=2,borderpad=4, bgcolor=cols[1],  opacity=0.9)\n",
    "\n",
    "# fig.add_annotation(x= x_annot  , y=.6-y_shift,\n",
    "#                    xref=\"paper\", yref=\"paper\",\n",
    "#                    showarrow=False, text='12hr'.center(8,' '),\n",
    "#                    font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "#                    align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "#                    borderwidth=2,borderpad=4, bgcolor=cols[2],  opacity=0.9)\n",
    "\n",
    "# fig.add_annotation(x= x_annot  , y=.5-y_shift,\n",
    "#                    xref=\"paper\", yref=\"paper\",\n",
    "#                    showarrow=False, text='24hr'.center(8,' '),\n",
    "#                    font=dict( family='Courier New', size=16, color=\"#ffffff\" ),\n",
    "#                    align=\"center\", bordercolor=\"#c7c7c7\",\n",
    "#                    borderwidth=2,borderpad=4, bgcolor=cols[3],  opacity=0.9)\n",
    "\n",
    "\n",
    "fig.update_annotations(font_size=18)  # Increase size of subplot title\n",
    "\n",
    "\n",
    "\n",
    "fig.update_xaxes(range=[solar_fluxes['date'][21], solar_fluxes['date'][27]])\n",
    "\n",
    "fig.show(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd705d82",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.485Z"
    }
   },
   "outputs": [],
   "source": [
    "# shorter arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028a2f7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-02T00:23:27.488Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import sys  \n",
    "# import os\n",
    "# import pickle \n",
    "# import gc\n",
    "# # gc.collect()\n",
    "\n",
    "# sys.path.insert(0, '/data/geodyn_proj/pygeodyn/pygeodyn_develop/')\n",
    "# from PYGEODYN import Pygeodyn\n",
    "\n",
    "# Obj_Geodyn={}\n",
    "# global_path = '/data/zach_work/validation/investigate_arclength/'\n",
    "# dir_save = '/data/zach_work/output_from_runs/icesat2_shortarc_pickles/'\n",
    "\n",
    "\n",
    "# for imod,model in enumerate(['msis2']):\n",
    "# #     print(model)\n",
    "#     for ihr,arclen in enumerate(arc_len_list): #arc_len_list\n",
    "#         print(model, arclen)\n",
    "    \n",
    "#         pickle_file = dir_save+'PeriodC_313_318_'+model+'.'+str(arclen)+'hr.pkl'\n",
    "\n",
    "#         if not os.path.exists(pickle_file):\n",
    "\n",
    "#             run_settings = global_path+'ArclengthTests_periodC_'+ model+'_'+str(arclen)+'hour.yaml'        \n",
    "\n",
    "#             ### Load the data into an object\n",
    "#             Obj_Geodyn[model+'.'+str(arclen)] = Pygeodyn(run_settings)\n",
    "#             Obj_Geodyn[model+'.'+str(arclen)].getData_BigData_lowmemory()\n",
    "\n",
    "# #             pickle_file = dir_save+'PeriodC_313_318_'+i+'hr.pkl'\n",
    "\n",
    "#             #### Pickle the object to save it\n",
    "#             print('   ', 'Saving pickle')\n",
    "#             filehandler = open(pickle_file, 'wb') \n",
    "#             pickle.dump(Obj_Geodyn[model+'.'+str(arclen)], filehandler)\n",
    "#             filehandler.close()\n",
    "#             print('   ', 'Saved pickle',pickle_file)\n",
    "            \n",
    "#         else:\n",
    "#             print(model,'.',arclen,'  Pickle created -- will load in next step', sep='')\n",
    "            \n",
    "\n",
    "# ### Load the data if the pickles exist\n",
    "# print()\n",
    "# print()\n",
    "# gc.collect()\n",
    "\n",
    "# Obj_Geodyn = {}\n",
    "\n",
    "# for imod,model in enumerate(['jb2008']):\n",
    "#     for ihr,arclen in enumerate([3,6,12,24]):    \n",
    "# #         pickle_file = dir_save+'PeriodC_313_318_'+str(arclen)+'hr.pkl'\n",
    "#         pickle_file = dir_save+'PeriodC_313_318_'+model+'.'+str(arclen)+'hr.pkl'\n",
    "\n",
    "\n",
    "#         if os.path.exists(pickle_file):\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             Obj_Geodyn[model+'.'+str(arclen)] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "#             print('Loaded data... \\n     ',pickle_file)\n",
    "\n",
    "            \n",
    "# for imod,model in enumerate(['msis2']):\n",
    "#     for ihr,arclen in enumerate([3,6]):    \n",
    "# #         pickle_file = dir_save+'PeriodC_313_318_'+str(arclen)+'hr.pkl'\n",
    "#         pickle_file = dir_save+'PeriodC_313_318_'+model+'.'+str(arclen)+'hr.pkl'\n",
    "\n",
    "\n",
    "#         if os.path.exists(pickle_file):\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             Obj_Geodyn[model+'.'+str(arclen)] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "#             print('Loaded data... \\n     ',pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45251544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089145ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "793.75px",
    "left": "594px",
    "top": "377.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 635.26078,
   "position": {
    "height": "657.483px",
    "left": "961.448px",
    "right": "20px",
    "top": "350.899px",
    "width": "545.035px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
