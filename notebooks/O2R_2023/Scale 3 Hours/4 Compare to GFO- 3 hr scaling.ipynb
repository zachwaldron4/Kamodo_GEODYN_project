{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aad1109",
   "metadata": {},
   "source": [
    "# Normalize to 500 KM using MSIS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2be918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:09.233222Z",
     "start_time": "2024-04-01T17:27:08.504910Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf1162",
   "metadata": {},
   "source": [
    "### Load GFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1971e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:09.238934Z",
     "start_time": "2024-04-01T17:27:09.235450Z"
    }
   },
   "outputs": [],
   "source": [
    "month_list = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr']\n",
    "# month_list = ['nov']\n",
    "\n",
    "# gfo_file = 'gfo_nov.csv'\n",
    "# ice_file = 'icesat2_nov.csv'\n",
    "gfo_file = 'gfo_6month.csv'\n",
    "ice_file = 'icesat2_6month.csv'\n",
    "scale_cadence=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9025982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:09.253188Z",
     "start_time": "2024-04-01T17:27:09.240309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfo_6month.csv  exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(gfo_file) :\n",
    "    print(gfo_file, ' exists' )\n",
    "\n",
    "else:\n",
    "    gfo_bigdf = {}\n",
    "\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        if month=='oct':\n",
    "            m_num = 10\n",
    "            y_num = 2018\n",
    "        if month=='nov':\n",
    "            m_num = 11\n",
    "            y_num = 2018\n",
    "        if month=='dec':\n",
    "            m_num = 12\n",
    "            y_num = 2018\n",
    "        if month=='jan':\n",
    "            m_num = 1\n",
    "            y_num = 2019\n",
    "        if month=='feb':\n",
    "            m_num = 2\n",
    "            y_num = 2019\n",
    "        if month=='mar':\n",
    "            m_num = 3\n",
    "            y_num = 2019\n",
    "        if month=='apr':\n",
    "            m_num = 4\n",
    "            y_num = 2019\n",
    "\n",
    "        path_gfo     = \"/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/\"\n",
    "        filename_gfo = f'GC_DNS_ACC_{y_num}_{m_num:02d}_v02.txt'\n",
    "\n",
    "        datapath_gfo     = path_gfo + filename_gfo\n",
    "\n",
    "        headers = [\n",
    "            'date',        #         Date (yyyy-mm-dd)\n",
    "            'time',        #         Time (hh:mm:ss.sss)\n",
    "            'time_system', #         Time system: UTC or GPS (differs per mission)\n",
    "            'alt',         #  f10.3  Altitude (m), GRS80\n",
    "            'lon',         #   f8.3  Geodetic longitude (deg), GRS80\n",
    "            'lat',         #   f7.3  Geodetic latitude (deg), GRS80\n",
    "            'lst',         #   f6.3  Local solar time (h)\n",
    "            'arglat',      #   f7.3  Argument of latitude (deg)\n",
    "            'dens_x',      #  e15.8  Density derived from accelerometer measurements (kg/m3)\n",
    "            'dens_mean',   #  e15.8  Running orbit average of density (kg/m3)\n",
    "            'flag_den',    #   f4.1  Flag for density: 0 = nominal data, 1 = anomalous data (-)\n",
    "            'flag_orbitavg',#   f4.1  Flag for running orbit average density: 0 = nominal data, 1 = anomalous data (-)\n",
    "                    ]\n",
    "\n",
    "\n",
    "        gfo_bigdf[month] = pd.read_csv(datapath_gfo, \n",
    "                skiprows = 38, \n",
    "                sep = '\\s+',\n",
    "                names = headers,\n",
    "                           )\n",
    "\n",
    "        #Convert date from GPS to UTC\n",
    "        date = pd.to_datetime(\\\n",
    "                            + gfo_bigdf[month]['date']  \\\n",
    "                            + gfo_bigdf[month]['time'], \\\n",
    "                                    format='%Y-%m-%d%H:%M:%S.000') - pd.to_timedelta(18,'s')\n",
    "\n",
    "        gfo_bigdf[month].insert(0, 'Date', date)\n",
    "\n",
    "        del gfo_bigdf[month]['date'], gfo_bigdf[month]['time'], date\n",
    "        del gfo_bigdf[month]['time_system']\n",
    "        del gfo_bigdf[month]['dens_mean']\n",
    "        del gfo_bigdf[month]['flag_den']\n",
    "        del gfo_bigdf[month]['flag_orbitavg']\n",
    "\n",
    "\n",
    "    #     resid_meas_summry = pd.concat([ gfo_bigdf, resid_meas_summry_iter])\n",
    "\n",
    "    gfo_df = pd.concat([ gfo_bigdf[month] for month in month_list]  )\n",
    "    gfo_df = gfo_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649f2ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T19:39:25.633107Z",
     "start_time": "2024-02-14T19:39:25.602208Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40d20b1",
   "metadata": {},
   "source": [
    "### Load Icesat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcf2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc5fc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:09.285984Z",
     "start_time": "2024-04-01T17:27:09.255503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icesat2_6month.csv  exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(ice_file):\n",
    "    print(ice_file, ' exists' )\n",
    "    \n",
    "else:\n",
    "\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from gc import collect as gc_collect\n",
    "    import pickle \n",
    "    from datetime import datetime,timedelta\n",
    "\n",
    "    run_list = ['msis2',\n",
    "                'jb2008',\n",
    "                'dtm2020_o']\n",
    "\n",
    "#     month_list = ['nov']\n",
    "\n",
    "\n",
    "    scale_cadence = 3\n",
    "\n",
    "    dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "    run_dict={}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i in run_list:\n",
    "            if i =='msis2':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 5\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='dtm2020_o':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 3\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='jb2008':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 1\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "\n",
    "    print(run_dict)\n",
    "\n",
    "\n",
    "\n",
    "    dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "                 + 'icesat2/O2R2023_longimeperiod/1_DRIAruns/'\n",
    "    obj = {}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i,model in enumerate(run_list):\n",
    "            pickleName = f'_{month}_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "            ### Load the data if the pickles exist\n",
    "            print()\n",
    "            print()\n",
    "            gc_collect()\n",
    "\n",
    "            pickle_file = dir_save+model+pickleName\n",
    "\n",
    "            filehandler = open(pickle_file, 'rb') \n",
    "            obj[month+model] = pickle.load(filehandler)\n",
    "            filehandler.close()\n",
    "            print('Loaded data from pickle... ',  month+model)\n",
    "\n",
    "\n",
    "    ### Save space if doing density retrieval\n",
    "    for model in run_dict.keys():\n",
    "        del obj[model]['OrbitResids']\n",
    "        del obj[model]['Trajectory_orbfil']\n",
    "\n",
    "    gc_collect()\n",
    "    \n",
    "    \n",
    "    ## MAKE SCALING FACTORS\n",
    "    \n",
    "    satid = 1807001\n",
    "    wgts = {}\n",
    "\n",
    "    for model in run_dict.keys():\n",
    "        wgts[model] = {}\n",
    "        ScalingFactors  = []\n",
    "        ScalingFactor_times = []\n",
    "\n",
    "        for ii,arc in enumerate(obj[model]['global_params']['arc_input']):\n",
    "            epochstart = obj[model]['global_params']['prms']['epoch_start'][ii]\n",
    "            hrs = pd.to_datetime(epochstart, format='%Y-%m-%d %H:%M:%S').hour\n",
    "            frachours =(hrs/24)\n",
    "            #\n",
    "            if len(arc) == 9:\n",
    "                maneuv_indicator = arc[8]\n",
    "            else:\n",
    "                maneuv_indicator = ''\n",
    "            arc_type = obj[model]['global_params']['prms']['arc_type']\n",
    "            if arc_type == \"Nominal30hr_and_AB\":\n",
    "                arc_name =arc[:8]+ maneuv_indicator\n",
    "            else:\n",
    "                arc_name =arc[:8]+('%.3f'%frachours).lstrip('0')+ maneuv_indicator\n",
    "            ### Collect the weights for the ensemble average\n",
    "            inv_rms          = 1/obj[model]['Statistics'][arc_name]['T_RMS'].values[0]\n",
    "            wgts[model][arc_name] = inv_rms#/sum_wgts\n",
    "\n",
    "            iters = int(obj[model]['run_parameters'+arc_name]['total_iterations']) \n",
    "            for iit, itime in enumerate(obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'].keys()):\n",
    "                if iit == 0 or iit==9:\n",
    "                    pass\n",
    "                else:\n",
    "                    CURRENT_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['CURRENT_VALUE']\n",
    "                    APRIORI_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['APRIORI_VALUE']\n",
    "                    ScalingFactors.append(CURRENT_VALUE/APRIORI_VALUE)\n",
    "                    ScalingFactor_times.append(itime)\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "\n",
    "\n",
    "\n",
    "    ###### Scale the densities\n",
    "    models_dens = {}\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(f\"---Making continuous scaled rho for {monthmodel}\")\n",
    "        models_dens[monthmodel] = get_continuous_scaled_densities(obj, run_dict, monthmodel, scale_cadence)\n",
    "\n",
    "\n",
    "    ## Retrieve scaled ensemble weighted average\n",
    "    ##     'Rho_x' denotes the ensemble weighted avg\n",
    "    print(f\"---Making ensemble avg\")\n",
    "#     for monthmodel in run_dict.keys():\n",
    "#         month = monthmodel[:3]\n",
    "    models_dens =  calc_rho_ScaledEnsembleWgtAvg(models_dens, run_dict, month_list, run_list)\n",
    "\n",
    "\n",
    "    ### Clear up space \n",
    "    del obj\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(monthmodel)\n",
    "        if 'Rho_x' not in monthmodel:\n",
    "            del models_dens[monthmodel]\n",
    "\n",
    "    gc_collect()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c14a50",
   "metadata": {},
   "source": [
    "## Process normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085f9069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:09.310946Z",
     "start_time": "2024-04-01T17:27:09.287723Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c160626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e7f474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:11.832008Z",
     "start_time": "2024-04-01T17:27:09.312715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfo_6month.csv  exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists(gfo_file) :\n",
    "    print(gfo_file, ' exists' )\n",
    "    \n",
    "    \n",
    "    gfo_df = pd.read_csv(gfo_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(f\"---Calculating Grace-FO normalization\")\n",
    "    D500_gfo = normalize_density_msis2( gfo_df , 'GRACE-FO', 500)\n",
    "    gfo_df['D500_gfo'] = D500_gfo\n",
    "    #### save to a csv\n",
    "    gfo_df.to_csv(gfo_file, index=False)  \n",
    "\n",
    "    \n",
    "\n",
    "# gfo_df2 =     gfo_df.query(\"Date >= '2018-10-14' and Date < '2018-12-30'\")\n",
    "# del gfo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a288f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:13.844310Z",
     "start_time": "2024-04-01T17:27:11.835596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icesat2_6month.csv  exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(ice_file) :\n",
    "    print(ice_file, ' exists' )\n",
    "    \n",
    "    ice_df = pd.read_csv(ice_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ice_bigdf = {}\n",
    "    print(f\"---Calculating ICESat-2 normalization\")\n",
    "    \n",
    "    for imonth,month in enumerate(month_list):\n",
    "\n",
    "        D500_ice = normalize_density_msis2( models_dens[month+'Rho_x'], 'ICESat-2', 500)\n",
    "        models_dens[month+'Rho_x']['D500_ice'] = D500_ice\n",
    "        \n",
    "        ice_bigdf[month] = pd.DataFrame.from_dict(models_dens[month+'Rho_x'])\n",
    "#     ice_df.to_csv('icecat2_3month.csv', index=False)  \n",
    "\n",
    "    \n",
    "    ice_df = pd.concat([ ice_bigdf[month] for month in month_list]  )\n",
    "    ice_df = ice_df.reset_index(drop=True)\n",
    "    ice_df.to_csv(ice_file, index=False)  \n",
    "\n",
    "    ### Need to set this up to only write to file once. and have that write include the full dataset. Maybe concat the dataframes by month first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77062482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:13.862221Z",
     "start_time": "2024-04-01T17:27:13.846898Z"
    }
   },
   "outputs": [],
   "source": [
    "# gfo_df['Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7cefa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:13.879302Z",
     "start_time": "2024-04-01T17:27:13.864233Z"
    }
   },
   "outputs": [],
   "source": [
    "# ice_df['datescaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf54264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T21:27:10.850332Z",
     "start_time": "2024-02-16T21:27:10.816806Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538ab1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T21:27:52.618312Z",
     "start_time": "2024-02-16T21:27:52.588267Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d000b1b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:16.972407Z",
     "start_time": "2024-04-01T17:27:13.881249Z"
    }
   },
   "outputs": [],
   "source": [
    "(timeavg_gfo,  denavg_gfo) = orbit_avg_generic(gfo_df['Date'], gfo_df['D500_gfo'], gfo_df['lat'])    \n",
    "(timeavg_ice,  denavg_ice) = orbit_avg_generic(ice_df['date'], ice_df['D500_ice'], ice_df['lat'])    \n",
    "\n",
    "\n",
    "# remove the datapoints that are definitely artifacts of the orbit average\n",
    "# 74, 334,  365  \n",
    "\n",
    "# timeavg_ice.pop(74)\n",
    "# timeavg_ice.pop(334)\n",
    "# timeavg_ice.pop(365)\n",
    "# list(denavg_ice).pop(74)\n",
    "# list(denavg_ice).pop(334)\n",
    "# list(denavg_ice).pop(365)\n",
    "\n",
    "\n",
    "# timeavg_ice[74]=np.nan\n",
    "# timeavg_ice[334]=np.nan\n",
    "# timeavg_ice[365]=np.nan\n",
    "# denavg_ice[74]=np.nan\n",
    "# denavg_ice[334]=np.nan\n",
    "# denavg_ice[365]=np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e049ba",
   "metadata": {},
   "source": [
    "For every time in the GRACEFO orbit average, I have found the closest time in the ICESat-2 orbit average and percent differenced those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938fb2d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:27:16.990023Z",
     "start_time": "2024-04-01T17:27:16.974254Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_percent_diff_logspace(model, obs):\n",
    "    from numpy import exp as np_exp\n",
    "    from numpy import log as np_log # this is the natural log\n",
    "    \n",
    "    return( ( np_exp(np_log(model/obs))-1 )*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051ec195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:25.119251Z",
     "start_time": "2024-04-01T17:27:16.992044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping  0 2018-10-14 00:51:54\n",
      "skipping  0 2018-10-14 00:51:54\n",
      "skipping  28 2018-10-17 00:22:04\n",
      "skipping  28 2018-10-17 00:22:04\n",
      "skipping  29 2018-10-18 02:17:54\n",
      "skipping  181 2018-10-29 00:32:49\n",
      "skipping  181 2018-10-29 00:32:49\n",
      "skipping  182 2018-10-30 01:52:54\n",
      "skipping  182 2018-10-30 01:52:54\n",
      "skipping  288 2018-11-07 00:02:14\n",
      "skipping  288 2018-11-07 00:02:14\n",
      "skipping  289 2018-11-08 01:10:49\n",
      "skipping  289 2018-11-08 01:10:49\n",
      "skipping  548 2018-11-25 11:58:44\n",
      "skipping  548 2018-11-25 11:58:44\n",
      "skipping  549 2018-11-26 01:20:14\n",
      "skipping  549 2018-11-26 01:20:14\n",
      "skipping  579 2018-11-29 00:03:14\n",
      "skipping  579 2018-11-29 00:03:14\n",
      "skipping  580 2018-11-30 01:11:54\n",
      "skipping  580 2018-11-30 01:11:54\n",
      "skipping  656 2018-12-06 00:12:14\n",
      "skipping  656 2018-12-06 00:12:14\n",
      "skipping  657 2018-12-07 01:20:54\n",
      "skipping  671 2018-12-09 11:29:39\n",
      "skipping  671 2018-12-09 11:29:39\n",
      "skipping  672 2018-12-11 01:12:34\n",
      "skipping  686 2018-12-13 00:21:14\n",
      "skipping  686 2018-12-13 00:21:14\n",
      "skipping  687 2018-12-14 01:29:54\n",
      "skipping  687 2018-12-14 01:29:54\n",
      "skipping  778 2018-12-20 12:17:29\n",
      "skipping  778 2018-12-20 12:17:29\n",
      "skipping  779 2018-12-21 01:38:54\n",
      "skipping  779 2018-12-21 01:38:54\n",
      "skipping  854 2018-12-27 23:26:34\n",
      "skipping  854 2018-12-27 23:26:34\n",
      "skipping  855 2018-12-30 00:56:39\n",
      "skipping  872 2018-12-31 12:18:14\n",
      "skipping  872 2018-12-31 12:18:14\n",
      "skipping  873 2019-01-01 01:39:39\n",
      "skipping  873 2019-01-01 01:39:39\n",
      "skipping  1008 2019-01-12 12:35:09\n",
      "skipping  1008 2019-01-12 12:35:09\n",
      "skipping  1009 2019-01-15 01:57:34\n",
      "skipping  1024 2019-01-17 00:43:49\n",
      "skipping  1024 2019-01-17 00:43:49\n",
      "skipping  1025 2019-01-18 02:14:54\n",
      "skipping  1085 2019-01-23 00:06:44\n",
      "skipping  1085 2019-01-23 00:06:44\n",
      "skipping  1086 2019-01-24 01:15:24\n",
      "skipping  1361 2019-02-12 00:40:24\n",
      "skipping  1361 2019-02-12 00:40:24\n",
      "skipping  1362 2019-02-13 02:08:04\n",
      "skipping  1453 2019-02-20 00:32:04\n",
      "skipping  1453 2019-02-20 00:32:04\n",
      "skipping  1454 2019-02-21 01:51:24\n",
      "skipping  1454 2019-02-21 01:51:24\n",
      "skipping  1728 2019-03-12 00:01:09\n",
      "skipping  1729 2019-03-13 01:09:44\n",
      "skipping  1729 2019-03-13 01:09:44\n",
      "skipping  1943 2019-03-28 00:41:44\n",
      "skipping  1943 2019-03-28 00:41:44\n",
      "skipping  1944 2019-03-29 02:10:44\n",
      "skipping  1991 2019-04-02 00:24:44\n",
      "skipping  1991 2019-04-02 00:24:44\n",
      "skipping  1992 2019-04-03 01:36:44\n",
      "skipping  1992 2019-04-03 01:36:44\n",
      "skipping  2188 2019-04-17 00:20:24\n",
      "skipping  2188 2019-04-17 00:20:24\n",
      "skipping  2189 2019-04-18 01:29:04\n"
     ]
    }
   ],
   "source": [
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "\n",
    "# def calc_percent_change(a, b):\n",
    "#     from numpy import absolute as np_abs\n",
    "#     return(  ( (b-a)/np_abs(a) )*100)\n",
    "\n",
    "# def calc_percent_diff(a, b):\n",
    "#     return(  ( (b-a)/((a+b)/2) )*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "perc_stat = {}\n",
    "perc_stat['time_midpoint'] = []\n",
    "perc_stat['gfo_denorbavg'] = []\n",
    "perc_stat['ice_denorbavg'] = []\n",
    "perc_stat['gfo_time']      = []\n",
    "perc_stat['ice_time']      = []\n",
    "# perc_stat['percchange']    = []\n",
    "perc_stat['percdiff_log']    = []\n",
    "# perc_stat['percdiff_log2']    = []\n",
    "perc_stat['ratio']    = []\n",
    "\n",
    "\n",
    "from numpy import exp as np_exp\n",
    "from numpy import log as np_log # this is the natural log\n",
    "\n",
    "\n",
    "for i,val in enumerate( timeavg_gfo):\n",
    "    date_near = nearest(timeavg_ice, val)\n",
    "    indx = np.where(pd.to_datetime(timeavg_ice) == pd.to_datetime(date_near))[0][0]\n",
    "    \n",
    "    if np.abs((val - timeavg_ice[indx]).total_seconds()) > 5700:  # only compute % change if values within 1 orbit (95mins)\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if np.abs((timeavg_ice[indx] - timeavg_ice[indx-1]).total_seconds()) > 40000:\n",
    "            print('skipping ', indx, timeavg_ice[indx])\n",
    "        else:\n",
    "      \n",
    "\n",
    "            perc_stat['time_midpoint'].append( pd.Timestamp(val) + (pd.Timestamp(timeavg_ice[indx]) - pd.Timestamp(val)) / 2)\n",
    "            perc_stat['gfo_denorbavg'].append( denavg_gfo[i])\n",
    "            perc_stat['ice_denorbavg'].append( denavg_ice[indx])\n",
    "            perc_stat['gfo_time'].append( val)\n",
    "            perc_stat['ice_time'].append( timeavg_ice[indx])\n",
    "#             perc_stat['percchange'].append( calc_percent_change(denavg_gfo[i], denavg_ice[indx])  )\n",
    "            perc_stat['percdiff_log'].append(calc_percent_diff_logspace(denavg_ice[indx], denavg_gfo[i]) )\n",
    "            perc_stat['ratio'].append(denavg_ice[indx]/denavg_gfo[i])\n",
    "    \n",
    "    \n",
    "\n",
    "#     print(val, date_near)\n",
    "    \n",
    "    \n",
    "#  2018-10-17 00:22:04\n",
    "#  2018-10-29 00:32:49\n",
    "#  2018-11-07 00:02:14\n",
    "#  2018-11-25 11:58:44\n",
    "#  2018-11-29 00:03:14\n",
    "#  2018-12-06 00:12:14\n",
    "#  2018-12-09 11:29:39\n",
    "#  2018-12-13 00:21:14\n",
    "#  2018-12-20 12:17:29\n",
    "#  2018-12-27 23:26:34\n",
    "#  2018-12-31 12:18:14\n",
    "#  2019-01-12 12:35:09\n",
    "#  2019-01-17 00:43:49\n",
    "#  2019-01-23 00:06:44\n",
    "#  2019-02-12 00:40:24\n",
    "#  2019-02-20 00:32:04\n",
    "#  2019-03-12 00:01:09\n",
    "#  2019-03-28 00:41:44\n",
    "#  2019-04-02 00:24:44\n",
    "#  2019-04-17 00:20:24  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d65b75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T20:56:34.920591Z",
     "start_time": "2024-02-22T20:56:34.887899Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052bb3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T20:53:05.299092Z",
     "start_time": "2024-02-22T20:53:05.270618Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db882fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:25.542410Z",
     "start_time": "2024-04-01T17:29:25.121117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *\n",
    "import os\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "def read_nc_file( filename, variables):\n",
    "    ''' This function reads the TIEGCM .nc files and saves the given input variables to a dictionary.\n",
    "        The breakloop feature is here so that if the file doesn't exist the code can still continue.  '''\n",
    "    status = os.path.exists(filename)\n",
    "    \n",
    "    if status == True:\n",
    "        data = {}\n",
    "        for i, var_names in enumerate(variables):\n",
    "            ncid =  Dataset(filename,\"r+\", format=\"NETCDF4\")# filename must be a string\n",
    "            varData = ncid.variables\n",
    "            data[var_names] = np.array(varData[var_names])  \n",
    "    elif status == False:\n",
    "        print('No File Found', filename )\n",
    "        breakloop = True\n",
    "        data = 0\n",
    "        return( data , breakloop)\n",
    "    breakloop = False\n",
    "    return(data,breakloop )\n",
    "\n",
    "\n",
    "arc_list = []\n",
    "\n",
    "arc_list_18 = np.arange(287,365)\n",
    "# arc_list_18 = np.arange(304,335)\n",
    "for i in arc_list_18:\n",
    "    val = '2018'+str(i)\n",
    "    arc_list.append(int(val))\n",
    "    \n",
    "    #     print(val)\n",
    "    \n",
    "arc_list_19 = np.arange(1,110)\n",
    "for i in arc_list_19:\n",
    "    val = f\"2019{i:03d}\"\n",
    "    arc_list.append(int(val))\n",
    "\n",
    "path_to_f107 = '/data/SatDragModelValidation/data/inputs/atmos_models/geo_phys_indicies/gpi_1960001-2021243_f107aDaily.nc'\n",
    "\n",
    "f107_data = read_nc_file(path_to_f107, ['year_day', 'f107d', 'f107a', 'kp'])\n",
    "\n",
    "\n",
    "date = []\n",
    "kp_list = []\n",
    "f107d_list = []\n",
    "f107a_list  = []\n",
    "date_3hr = []\n",
    "doy_list    = []\n",
    "\n",
    "\n",
    "\n",
    "for i,val in enumerate(arc_list):\n",
    "    \n",
    "    index = f107_data[0]['year_day']==val\n",
    "    kp_list.append(f107_data[0]['kp'][index][0])\n",
    "    f107d_list.append(f107_data[0]['f107d'][index][0])\n",
    "    f107a_list.append(f107_data[0]['f107a'][index][0])\n",
    "    doy_list.append(str(f107_data[0]['year_day'][index][0])[-3:])\n",
    "\n",
    "    date.append(pd.to_datetime( str(val), format='%Y%j'))\n",
    "\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=0))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=3))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=6))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=9))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=12))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=15))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=18))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=21))\n",
    "#     date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=24))\n",
    "    \n",
    "kp_expand = []\n",
    "for i in kp_list:\n",
    "    for ii in i:\n",
    "        kp_expand.append(ii)\n",
    "        \n",
    "        \n",
    "        \n",
    "solar_fluxes = {}\n",
    "solar_fluxes['f107d_list'] = f107d_list\n",
    "solar_fluxes['f107a_list'] = f107a_list\n",
    "solar_fluxes['date']       = date\n",
    "solar_fluxes['date_3hr']   = date_3hr\n",
    "solar_fluxes['kp_expand']  = kp_expand\n",
    "\n",
    "f107d_earth = []\n",
    "f107a_earth = []\n",
    "######################################################################### \n",
    "##### Account for the F10.7 at earth (instead of referenced at 1AU) #####\n",
    "######################################################################### \n",
    "\n",
    "for i_doy,val_doy in enumerate(doy_list):\n",
    "    iday = int(val_doy)\n",
    "    theta0 = 2 * np.pi * (iday)/365.\n",
    "    sfeps = 1.000110 + 0.034221*np.cos(theta0)+0.001280* np.sin(theta0) +0.000719*np.cos(2.*theta0)+0.000077*np.sin(2.*theta0)\n",
    "\n",
    "    f107d_earth.append(sfeps * solar_fluxes['f107d_list'][i_doy])\n",
    "    f107a_earth.append(sfeps * solar_fluxes['f107a_list'][i_doy])\n",
    "\n",
    "solar_fluxes['f107d_earth'] = f107d_earth\n",
    "solar_fluxes['f107a_earth'] = f107a_earth\n",
    "\n",
    " \n",
    "del f107d_earth\n",
    "del f107a_earth\n",
    "del kp_expand\n",
    "del f107d_list\n",
    "del f107a_list\n",
    "del date\n",
    "del date_3hr\n",
    "del f107_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be7e0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:29:33.252051Z",
     "start_time": "2024-04-01T18:29:33.152391Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_subplots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61426d014752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fig = make_subplots(rows=1, cols=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m fig = make_subplots(rows=3, cols=1, row_heights=[0.2, 0.4, 0.3],#,0.2],\n\u001b[0m\u001b[1;32m      3\u001b[0m                     specs=[[{\"secondary_y\": True}],\n\u001b[1;32m      4\u001b[0m                            \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"secondary_y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                            [{\"secondary_y\": False}],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_subplots' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "fig = make_subplots(rows=3, cols=1, row_heights=[0.2, 0.4, 0.3],#,0.2],\n",
    "                    specs=[[{\"secondary_y\": True}],\n",
    "                           [{\"secondary_y\": False}],\n",
    "#                            [{\"secondary_y\": False}],\n",
    "                           [{\"secondary_y\": False}]],\n",
    "                           shared_xaxes=True,\n",
    "                           vertical_spacing=0.02)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=solar_fluxes['date'],\n",
    "                           y=solar_fluxes['f107d_earth'],\n",
    "                           name= 'F107d_1AU',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh',dash='dash', color = 'blue', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=True,row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=solar_fluxes['date_3hr'],\n",
    "                           y=solar_fluxes['kp_expand'],\n",
    "                           name= 'Kp',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh', color = 'black', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=False,row=1, col=1) \n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x= perc_stat['gfo_time'],\n",
    "                            y=perc_stat['gfo_denorbavg'],\n",
    "                            name=f'GFO_500km',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=\"#d62728\" ),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['ice_time'],\n",
    "                            y=perc_stat['ice_denorbavg'],\n",
    "                            name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=\"#1f77b4\"),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint'],\n",
    "                            y=perc_stat['percdiff_log'],\n",
    "#                             name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color='black'),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "# window_size = 50\n",
    "# i = 0\n",
    "# # Initialize an empty list to store moving averages\n",
    "# moving_averages = []\n",
    "# # Loop through the array to consider\n",
    "# # every window of size 3\n",
    "# while i < len(perc_stat['percdiff_log']) - window_size + 1:\n",
    "#     # Store elements from i to i+window_size\n",
    "#     # in list to get the current window\n",
    "#     window = perc_stat['percdiff_log'][i : i + window_size]\n",
    "#     # Calculate the average of current window\n",
    "#     window_average = round(sum(window) / window_size, 2)\n",
    "#     # Store the average of current\n",
    "#     # window in moving average list\n",
    "#     moving_averages.append(window_average)\n",
    "#     # Shift window to right by one position\n",
    "#     i += 1\n",
    "fig.add_hline(y=0, line = dict(dash='solid', color = 'grey', width=1), row=3, col=1)\n",
    "# fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint'],\n",
    "#                             y=moving_averages,\n",
    "#                             mode='markers+lines',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=2, color=\"#ff7f0e\"),\n",
    "#                             showlegend=False),\n",
    "#                             row=3, col=1)\n",
    "N = len(perc_stat['ratio'])\n",
    "sum_log = 0\n",
    "sum_log2 = 0\n",
    "for i in range(N):\n",
    "    sum_log  += np_log(perc_stat['ratio'][i])\n",
    "    sum_log2 += np_log(perc_stat['ratio'][i])**2\n",
    "#         np_exp((1/N)*perc_stat['percdiff_log2'])-1 )*100\n",
    "per_diff_log     = 100* (np_exp( (1/N)*sum_log  )-1)\n",
    "per_rms_diff_log = 100* (np_exp( np.sqrt((1/N)*(sum_log2))  )-1)\n",
    "print(f\"% difference in log space:     % {per_diff_log:6.3f}\")\n",
    "print(f\"% RMS difference in log space: % {per_rms_diff_log:6.3f}\")\n",
    "\n",
    "# fig.add_annotation(\n",
    "#                 y= .28,\n",
    "#                 x= .8,\n",
    "#                 xref=\"paper\", yref=\"paper\",\n",
    "#                 showarrow=False,\n",
    "#                 # text=\"dZ Night = \"+str(np.round(deltaZ_night,2))+\" km\", \n",
    "#                 text=f\"% diff: {str(np.round(per_diff_log,2))}%\", \n",
    "#                 align = 'left',\n",
    "#                 valign = 'top',\n",
    "# #                 bgcolor='lightgrey',\n",
    "# #                 bordercolor=\"blue\",\n",
    "#                 borderwidth=1,\n",
    "#                 borderpad=2,\n",
    "#                 font=dict(family='Arial',size=18,color=\"black\"),\n",
    "#                 )\n",
    "fig.add_annotation(\n",
    "#                 y= .025,\n",
    "                y= .25,\n",
    "                x= .92,\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                showarrow=False,\n",
    "                text=f\"% diff: {per_diff_log:05.3f}% <br>% rms diff: {per_rms_diff_log:05.3f}%\", \n",
    "                align = 'left',\n",
    "                valign = 'bottom',\n",
    "                bgcolor='lightgrey',\n",
    "                bordercolor='black',\n",
    "                borderwidth=1,\n",
    "                borderpad=2,\n",
    "                font=dict(family='Arial',size=14,color=\"black\"),\n",
    "                )\n",
    "\n",
    "\n",
    "print(np.mean(perc_stat['percdiff_log']))\n",
    "\n",
    "\n",
    "\n",
    "#### FANCY LEGEND ################################################################\n",
    "modelnames=[]\n",
    "modelcolors = []\n",
    "modelnames.append(\"GraceFO\")\n",
    "modelcolors.append(\"#d62728\")\n",
    "modelnames.append(\"ICESat2\")\n",
    "modelcolors.append('#1f77b4')\n",
    "df_leg = pd.DataFrame({\"starts_colors\": modelcolors})\n",
    "fig.update_traces(showlegend=False).add_traces(\n",
    "    [   go.Scattergl(name=modelnames[i],\n",
    "               x=[pd.to_datetime( \"181107-000000\", format='%y%m%d-%H%M%S')],\n",
    "               mode='lines',\n",
    "               line = dict(shape = 'hv',  width=10),\n",
    "               marker_color=c,\n",
    "               showlegend=True)\n",
    "        for i,c in enumerate((df_leg.loc[:,[\"starts_colors\"]].values.ravel()))])\n",
    "## Legend Control\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=.75,\n",
    "    xanchor=\"center\",\n",
    "    x=.45,\n",
    "    orientation=\"h\",\n",
    "        font=dict(family='Arial',size=12,color='black'),\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"darkgrey\",\n",
    "        borderwidth=0.5,)  )\n",
    "################################################################################\n",
    "\n",
    "\n",
    "### UPDATE AXES \n",
    "fig.update_yaxes(title_text=\"Kp\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[0,7],\n",
    "                 secondary_y=False,\n",
    "                 row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F10.7\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[20,90],\n",
    "                 secondary_y=True,\n",
    "                 tickfont=dict(color=\"blue\"),\n",
    "                 titlefont=dict(color=\"blue\"),\n",
    "                 row=1, col=1)\n",
    "################################################################################\n",
    "\n",
    "### SYLIZE LEGEND \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = False\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = False\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "# fig.update_xaxes(range=[pd.to_datetime( \"181101-000000\", format='%y%m%d-%H%M%S'),\n",
    "#                         pd.to_datetime( \"181130-000000\", format='%y%m%d-%H%M%S')],row=1, col=1)\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "                 type=\"log\", \n",
    "                 exponentformat= 'power',row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Percent Difference\", \n",
    "                minor=dict(dtick=10, tickwidth=1,ticklen=4,tickcolor='grey',ticks='inside'),\n",
    "                 exponentformat= 'power',row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Ratio (GFO/ICE)\", \n",
    "#                 minor=dict(dtick=0.25, tickwidth=1,ticklen=4,tickcolor='grey',ticks='inside'),\n",
    "#                  exponentformat= 'power',row=4, col=1)\n",
    "\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),)\n",
    "\n",
    "fig.update_layout(#title=f\"ICESat2 {scale_cadence}-hr Scaled Model Rho vs GRACE-FO Rho, Norm 500km\",\n",
    "                  autosize=False,    width=1000,    height=800,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n",
    "pio.write_image(fig, 'ICEvGFO.jpg', scale=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c010f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T20:28:53.734280Z",
     "start_time": "2024-02-20T20:28:53.710559Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f286223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.516331Z",
     "start_time": "2024-04-01T17:29:26.494441Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/pygeodyn/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fc5fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.521106Z",
     "start_time": "2024-04-01T17:27:07.591Z"
    }
   },
   "outputs": [],
   "source": [
    "# (timeavg_gfo,  denavg_gfo) = orbit_avg_generic(gfo_df['Date'], gfo_df['D500_gfo'], gfo_df['lat'])    \n",
    "# (timeavg_ice,  denavg_ice) = orbit_avg_generic(ice_df['date'], ice_df['D500_ice'], ice_df['lat'])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311971ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.521903Z",
     "start_time": "2024-04-01T17:27:07.595Z"
    }
   },
   "outputs": [],
   "source": [
    "gfo_df = gfo_df.query(\"Date >= '2018-11-10' and Date < '2018-11-13'\")\n",
    "ice_df = ice_df.query(\"date >= '2018-11-10' and date < '2018-11-13'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b2c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.522651Z",
     "start_time": "2024-04-01T17:27:07.598Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ice_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305b93c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.523411Z",
     "start_time": "2024-04-01T17:27:07.601Z"
    }
   },
   "outputs": [],
   "source": [
    "gfo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e1707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.524275Z",
     "start_time": "2024-04-01T17:27:07.605Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "fig = make_subplots(rows=1, cols=1,\n",
    "                           shared_xaxes=True,\n",
    "                           vertical_spacing=0.02)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x= gfo_df['Date'],\n",
    "                            y=gfo_df['D500_gfo'],\n",
    "                            name=f'GFO_500km',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=\"#d62728\" ),\n",
    "                            showlegend=False),\n",
    "                            row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=ice_df['date'],\n",
    "                            y=ice_df['D500_ice'],\n",
    "                            name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=\"#1f77b4\"),\n",
    "                            showlegend=False),\n",
    "                            row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### FANCY LEGEND ################################################################\n",
    "modelnames=[]\n",
    "modelcolors = []\n",
    "modelnames.append(\"GraceFO\")\n",
    "modelcolors.append(\"#d62728\")\n",
    "modelnames.append(\"ICESat2\")\n",
    "modelcolors.append('#1f77b4')\n",
    "df_leg = pd.DataFrame({\"starts_colors\": modelcolors})\n",
    "fig.update_traces(showlegend=False).add_traces(\n",
    "    [   go.Scattergl(name=modelnames[i],\n",
    "               x=[pd.to_datetime( \"181111-000000\", format='%y%m%d-%H%M%S')],\n",
    "               mode='lines',\n",
    "               line = dict(shape = 'hv',  width=10),\n",
    "               marker_color=c,\n",
    "               showlegend=True)\n",
    "        for i,c in enumerate((df_leg.loc[:,[\"starts_colors\"]].values.ravel()))])\n",
    "## Legend Control\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=.95,\n",
    "    xanchor=\"center\",\n",
    "    x=.55,\n",
    "    orientation=\"h\",\n",
    "        font=dict(family='Arial',size=12,color='black'),\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"darkgrey\",\n",
    "        borderwidth=0.5,)  )\n",
    "\n",
    "################################################################################\n",
    "\n",
    "### SYLIZE LEGEND \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = True\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = False\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "                 type=\"log\", \n",
    "                  range=[-13.7-.05,  -12.6+.25],\n",
    "                 exponentformat= 'power',row=1, col=1)\n",
    "\n",
    "\n",
    "fig.update_layout(#title=f\"ICESat2 {scale_cadence}-hr Scaled Model Rho vs GRACE-FO Rho, Norm 500km\",\n",
    "                  autosize=False,    width=1000,    height=400,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbcca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c98987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a825e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f223676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20568b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.525142Z",
     "start_time": "2024-04-01T17:27:07.614Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # fig.add_trace(go.Scattergl(x=gfo_df['Date'][:10000],\n",
    "# #                             y=gfo_df['D500_gfo'][:10000],\n",
    "# #                             name=f'D500_gfo',\n",
    "# #                             mode='markers',\n",
    "# #                             opacity=1,\n",
    "# #                             marker=dict( size=4 ),\n",
    "# #                             showlegend=True),\n",
    "# #                             row=1, col=1)\n",
    "\n",
    "# # fig.add_trace(go.Scattergl(x=ice_df['datescaled'],\n",
    "# #                             y=ice_df['D500_ice'],\n",
    "# #                             name=f'D500_icesat2',\n",
    "# #                             mode='markers',\n",
    "# #                             opacity=1,\n",
    "# #                             marker=dict( size=4 ),\n",
    "# #                             showlegend=True),\n",
    "# #                             row=1, col=1)\n",
    "# (time_avg, den_gfo ) = orbit_avg_generic(gfo_df['Date'], gfo_df['D500_gfo'], gfo_df['lat'])    \n",
    "# fig.add_trace(go.Scattergl(x=time_avg,\n",
    "#                             y=den_gfo,\n",
    "#                             name=f'D500_gfo',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4 ),\n",
    "#                             showlegend=True),\n",
    "#                             row=1, col=1)\n",
    "\n",
    "# (time_avg, den_avg ) = orbit_avg_generic(ice_df['datescaled'],ice_df['D500_ice'],ice_df['lat'])    \n",
    "# # remove the datapoints that are definitely artifacts of the orbit average\n",
    "# # 74, 334,  365  \n",
    "\n",
    "# time_avg[74]=np.nan\n",
    "# time_avg[334]=np.nan\n",
    "# time_avg[365]=np.nan\n",
    "# den_avg[74]=np.nan\n",
    "# den_avg[334]=np.nan\n",
    "# den_avg[365]=np.nan\n",
    "\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=time_avg,\n",
    "#                             y=den_avg,\n",
    "#                             name=f'D500_icesat2',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4 ),\n",
    "#                             showlegend=True),\n",
    "#                             row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "# ### SYLIZE AXES \n",
    "# font_dict=dict(family='Arial',size=11,color='black')\n",
    "# ## automate the specification of the axes for subplots\n",
    "# rownum, colnum = fig._get_subplot_rows_columns()\n",
    "# for i in rownum:\n",
    "#     if len(rownum)==1:\n",
    "#         L_ticklabel = True\n",
    "#     else:\n",
    "#         if i < len(rownum):\n",
    "#             L_ticklabel = True\n",
    "#         else:\n",
    "#             L_ticklabel = True\n",
    "#     fig.update_xaxes(### LINE at axis border\n",
    "#                       showline=True,\n",
    "#                       showticklabels=L_ticklabel,\n",
    "# #                       tickformat= '%m/%d',\n",
    "#                       linecolor='black',\n",
    "#                       linewidth=1,\n",
    "#                      ### Major ticks\n",
    "#                       ticks='inside',\n",
    "#                       tickfont=font_dict,\n",
    "#                       mirror=True,\n",
    "# #                       tickwidth=2,\n",
    "# #                       ticklen=9,\n",
    "#                       tickcolor='grey',\n",
    "# #                       tick0=\"2018-11-9\" ,\n",
    "# #                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "#                       #### Minor Ticks\n",
    "#                        minor=dict(\n",
    "#                          dtick=86400000.0, # milliseconds in a day\n",
    "#                          tickwidth=1,\n",
    "#                          ticklen=4,\n",
    "#                          tickcolor='grey',\n",
    "#                          ticks='inside'),\n",
    "#                       ### GRID\n",
    "#                        gridcolor='gainsboro',\n",
    "#                        gridwidth=1,\n",
    "#                        layer='above traces',\n",
    "#                        tickangle=0,\n",
    "#                        row=i, col=1)\n",
    "#     fig.update_yaxes(showline=True,      # add line at x=0\n",
    "#                          showticklabels=True,\n",
    "#                          linecolor='black',  # line color\n",
    "#                          linewidth=1,        # line size\n",
    "#                      ticks='inside',     # ticks outside axis\n",
    "#                      tickfont=font_dict, # tick label font\n",
    "#                      mirror='allticks',  # add ticks to top/right axes\n",
    "#                      tickwidth=1,      # tick width\n",
    "#                      tickcolor='black',  # tick color\n",
    "#                      gridcolor='gainsboro',\n",
    "#                      gridwidth=1,\n",
    "#                      layer='above traces',\n",
    "#                      row=i, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.update_yaxes(title_text=\"Density\", \n",
    "#                  type=\"log\", \n",
    "#                  exponentformat= 'power',row=1, col=1)\n",
    "# fig.update_layout(#title=\"ICESat2 3-hr Scaled Model Rho vs GRACE-FO Rho\",\n",
    "#                   autosize=True,#    width=1000,    height=700,\n",
    "#                   legend= {'itemsizing': 'trace'},\n",
    "#                   font=font_dict, plot_bgcolor='white', \n",
    "#                  )\n",
    "\n",
    "# fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b04f0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.525912Z",
     "start_time": "2024-04-01T17:27:07.617Z"
    }
   },
   "outputs": [],
   "source": [
    "gfo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9a44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.526657Z",
     "start_time": "2024-04-01T17:27:07.621Z"
    }
   },
   "outputs": [],
   "source": [
    "ice_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ee217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T23:00:06.630040Z",
     "start_time": "2024-03-25T23:00:06.584106Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026c87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:46:35.497276Z",
     "start_time": "2024-02-13T22:46:35.475241Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa018b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.527614Z",
     "start_time": "2024-04-01T17:27:07.626Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=gfo_df['Date'],\n",
    "#                             y=gfo_df['D500_gfo'],\n",
    "#                             name=f'D500_gfo',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4 ),\n",
    "#                             showlegend=True),\n",
    "#                             row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=ice_df['dates'],\n",
    "                            y=ice_df['D500_ice'],\n",
    "                            name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4 ),\n",
    "                            showlegend=True),\n",
    "                            row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=ice_df['dates'],\n",
    "                            y=ice_df['Rho_x'],\n",
    "                            name=f'Rho_x_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4 ),\n",
    "                            showlegend=True),\n",
    "                            row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "### SYLIZE AXES \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = True\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = True\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "                 type=\"log\", \n",
    "                 exponentformat= 'power',row=1, col=1)\n",
    "fig.update_layout(#title=\"ICESat2 3-hr Scaled Model Rho vs GRACE-FO Rho\",\n",
    "                  autosize=True,#    width=1000,    height=700,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0cff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T17:29:26.528636Z",
     "start_time": "2024-04-01T17:27:07.630Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np \n",
    "# import sys  \n",
    "# from scipy.io import loadmat  #allows us to read in .mat files\n",
    "# from datetime import datetime, timedelta\n",
    "# import gc\n",
    "\n",
    "# #### MAKE MSIS Take the 3HR Ap values\n",
    "# from pymsis import msis\n",
    "# SWI_option = [1.0]*25\n",
    "# SWI_option[8] = -1.0\n",
    "#             #  C    AP - MAGNETIC INDEX(DAILY) OR WHEN SW(9)=-1. :\n",
    "#             #  C      - ARRAY CONTAINING:\n",
    "#             #  C       (1) DAILY AP\n",
    "#             #  C       (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "#             #  C       (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "#             #  C       (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "#             #  C       (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "#             #  C       (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "#             #  C          TO CURRENT TIME\n",
    "#             #  C       (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "#             #  C          TO CURRENT TIME\n",
    "\n",
    "\n",
    "# #################################\n",
    "# years =  [2002]#, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\n",
    "# days = np.arange(1,2)\n",
    "# path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "# #################################\n",
    "\n",
    "\n",
    "# def make_champ_timeseries_v2_redorenorm(years, days):\n",
    "\n",
    "#     \"\"\"This function redoes the construction of the CHAMP timeseries for the orbit.\n",
    "\n",
    "#     Changes from the original include:\n",
    "#         - Use F10.7 indicies that are scaled with MgII and re-referenced to the Earth's locatiion (instead of at 1AU)\n",
    "#         - Use MSIS2.0 for the normalization to 400km\n",
    "#         - Use a 3Hour Ap input as required by MSIS for the more granular Stormtime forcing option.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     \"\"\"    \n",
    "\n",
    "\n",
    "#     path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "\n",
    "\n",
    "#     #### Load MgII Scaled F10.7 Values:\n",
    "#     import pickle\n",
    "#     dir_save = '/space/DNR_data/'\n",
    "#     filehandler = open(dir_save+'MgII_F107_KpAp'+'.pkl', 'rb') \n",
    "#     mgII_data = pickle.load(filehandler)\n",
    "#     filehandler.close()\n",
    "\n",
    "#     #### clear up some space        \n",
    "#     truncate_date    = np.logical_and(mgII_data['Date'].year>=2000 , mgII_data['Date'].year<=2011 )\n",
    "#     truncate_date3hr =  np.logical_and(mgII_data['Date_3hrAp'].year>=2000 , mgII_data['Date_3hrAp'].year<=2011 )#np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "\n",
    "#     mgII_data['Date_3hrAp'] = mgII_data['Date_3hrAp'][truncate_date3hr]\n",
    "#     mgII_data['Ap']         = np.array(mgII_data['Ap'])[truncate_date3hr]\n",
    "#     mgII_data['Date']       = mgII_data['Date'][truncate_date]\n",
    "#     mgII_data['Ap_dailyavg'] =  np.array(mgII_data['Ap_dailyavg'])[truncate_date]\n",
    "#     mgII_data['f107d_earth'] = np.array(mgII_data['f107d_earth'])[truncate_date]\n",
    "#     mgII_data['f107a_earth'] = np.array(mgII_data['f107a_earth'])[truncate_date]\n",
    "\n",
    "#     del mgII_data['DOY']\n",
    "#     del mgII_data['kp']\n",
    "#     del mgII_data['f107d']\n",
    "#     del mgII_data['f107a']\n",
    "#     del mgII_data['year_day']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #     noaa = pd.read_pickle('/space/DNR_data/noaa_2002_2010_pickle')\n",
    "#     #     noaa['f107d'][noaa['f107d'].astype(float) <= 60] = noaa['f107a'][noaa['f107d'].astype(float) <= 60]\n",
    "\n",
    "#     tleng = 0\n",
    "#     time_full= []\n",
    "#     Year         = []\n",
    "#     Doy          = []\n",
    "#     Hours        = []\n",
    "#     Lon          = []\n",
    "#     Lat          = []\n",
    "#     LatBin       = []\n",
    "#     Height       = []\n",
    "#     LocTim       = []\n",
    "#     CHAMPDensity = []\n",
    "#     D400_msis00  = []  # normalized quantity\n",
    "#     rhosat_msis00= []\n",
    "#     rhosat_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     rho400_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     D400_msis2   = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "\n",
    "#     rhosat_tiegcm = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     rho400_tiegcm = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     D400_tiegcm   = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "\n",
    "#     #     Cd           = []\n",
    "#     # Ap_dayvals   = []\n",
    "#     # f107a_dayvals= []\n",
    "#     # f107d_dayvals= []\n",
    "#     date = []\n",
    "\n",
    "#     i = 0\n",
    "#     for iyear,year in enumerate(years):\n",
    "#         for iday,day in enumerate(days):\n",
    "#                 ####---------------------------------------------\n",
    "#                 #### Gather the Necessary Flux and Ap Information\n",
    "#                 index_date = np.logical_and(mgII_data['Date'].year==year , mgII_data['Date'].dayofyear==day )\n",
    "#                 f107a = float(np.squeeze(np.asarray(mgII_data['f107a_earth'])[index_date]))\n",
    "#                 f107d = float(np.squeeze(np.asarray(mgII_data['f107d_earth'])[index_date]))\n",
    "#                 Ap_daily_avg = float(np.squeeze(np.asarray(mgII_data['Ap_dailyavg'])[index_date]))\n",
    "\n",
    "\n",
    "#                 ### Construct the necessary 3hr Ap Array to go into MSIS\n",
    "# #                 print(f\"----------------------------------------------------------\")\n",
    "\n",
    "#                 print(f\"Year: {year} / Day: {day}\")\n",
    "#                 for it, itime in enumerate( champ['Hours'][:leng]):\n",
    "\n",
    "#                     lon   = champ['Lon'][it]\n",
    "#                     lat   = champ['Lat'][it]\n",
    "#                     dates = datetime(year, 1, 1) + timedelta(float(day) - 1) + timedelta(hours = champ['Hours'][it]) \n",
    "#                     f107din = [f107d]\n",
    "#                     f107ain = [f107a]\n",
    "#                     time_sat = itime\n",
    "\n",
    "#                     if str(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3)=='-inf':\n",
    "#                         print('-inf FOUND')\n",
    "#                         continue\n",
    "#                     elif str(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3)=='inf':\n",
    "#                         print('inf FOUND')\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         date.append(datetime(year, 1, 1) + timedelta(days = float(day)-1,  hours = itime )) \n",
    "\n",
    "#                         index_date3hr = np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "#                         indexvals =  [i for i, x in enumerate(index_date3hr) if x]\n",
    "#                         Ap_doy_windows = mgII_data['Date_3hrAp'][indexvals]\n",
    "\n",
    "#                         #### Find the Current 3hr Kp window:A\n",
    "#                         Ap_windw_hrs = [i.hour for i in Ap_doy_windows]\n",
    "#                         Ap_windw_hrs = np.append(np.array(Ap_windw_hrs),24)  ## add the final window edge\n",
    "\n",
    "#                         index_current_Ap = int(np.digitize([dates.hour],Ap_windw_hrs))\n",
    "#                         if index_current_Ap==8:\n",
    "#                             index_current_Ap += -1\n",
    "#                         indexglobal_currentAp = indexvals[index_current_Ap]\n",
    "\n",
    "#                         Ap_3HR_current        = mgII_data['Ap'][indexglobal_currentAp]\n",
    "#                         Ap_3HR_prior          = mgII_data['Ap'][indexglobal_currentAp-1]\n",
    "#                         Ap_6HR_prior          = mgII_data['Ap'][indexglobal_currentAp-2]\n",
    "#                         Ap_9HR_prior          = mgII_data['Ap'][indexglobal_currentAp-3]\n",
    "#                         Ap_12hr_33hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-11 :indexglobal_currentAp-3 ] ) ### 33hrs to 12 hours\n",
    "#                         Ap_36hr_57hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-19 :indexglobal_currentAp-11 ])  ### 36hrs to 57 hours\n",
    "\n",
    "#                         apsin = [[Ap_daily_avg,          # (1) DAILY AP\n",
    "#                                   Ap_3HR_current,        # (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "#                                   Ap_3HR_prior,          # (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "#                                   Ap_6HR_prior,          # (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "#                                   Ap_9HR_prior,          # (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "#                                   Ap_12hr_33hr_priorAVG, # (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "#                                   Ap_36hr_57hr_priorAVG]]# (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "\n",
    "#                         output2_sat = msis.run(dates, lon, lat, champ['Height'][it], f107din, f107ain, apsin, version = 2, options=SWI_option)\n",
    "#                         output2_400 = msis.run(dates, lon, lat, 400, f107din, f107ain, apsin                , version = 2, options=SWI_option)\n",
    "\n",
    "\n",
    "\n",
    "#                         ### add the values to the growing lists\n",
    "#                         rhosat_msis2[tleng+it] = output2_sat[0,0,0,0][0]\n",
    "#                         rho400_msis2[tleng+it] = output2_400[0,0,0,0][0]\n",
    "#                         ### RENORMALIZE with MSIS2\n",
    "#                         D400_msis2[tleng+it]   = champ['Density'][it] * (output2_400[0,0,0,0][0] / output2_sat[0,0,0,0][0])   # normalized density to 400km with MSIS2\n",
    "\n",
    "#                         ##### Interpolate the TIEGCM Data to the Satellites Position and Time\n",
    "# #                         print(time_sat)\n",
    "# #                         print(lon)\n",
    "# #                         print(lat)\n",
    "# #                         print(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, champ['Height'][it], 'DEN')*1e3)\n",
    "# #                         print(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3)\n",
    "#                         val_tiegcm_sat = interpolate_tiegcm(TIEGCM, lon, lat, time_sat, champ['Height'][it], 'DEN')*1e3\n",
    "#                         val_tiegcm_400 = interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3\n",
    "#                         rhosat_tiegcm[tleng+it] = val_tiegcm_sat\n",
    "#                         rho400_tiegcm[tleng+it] = val_tiegcm_400\n",
    "#                         ### RENORMALIZE with TIEGCM\n",
    "# #                         print('msis2', champ['Density'][it] * (output2_400[0,0,0,0][0] / output2_sat[0,0,0,0][0]))\n",
    "# #                         print('val_tiegcm_sat', val_tiegcm_sat)\n",
    "# #                         print('val_tiegcm_400', val_tiegcm_400)\n",
    "# #                         print('tiegcm', champ['Density'][it] * (val_tiegcm_400 / val_tiegcm_sat))\n",
    "#                         D400_tiegcm[tleng+it]   = champ['Density'][it] * (val_tiegcm_400 / val_tiegcm_sat)   # normalized density to 400km with TIEGCM\n",
    "\n",
    "\n",
    "\n",
    "#                 tleng = tleng + leng\n",
    "#     #                 print(year,'/',day)\n",
    "#                 i+=1\n",
    "#                 print('Closing', sec_file)\n",
    "#                 ncid =  Dataset(sec_file,\"r+\", format=\"NETCDF4\")        \n",
    "#                 ncid.close()\n",
    "#                 gc.collect()\n",
    "\n",
    "\n",
    "#             elif breakloop == True:\n",
    "#                 i+=1\n",
    "#                 continue\n",
    "\n",
    "#     #### REMOVE NANS\n",
    "#     # From Msis lists\n",
    "#     rhosat_msis2 = rhosat_msis2[~np.isnan(rhosat_msis2)]\n",
    "#     rho400_msis2 = rho400_msis2[~np.isnan(rho400_msis2)]\n",
    "#     D400_msis2   = D400_msis2[~np.isnan(D400_msis2)]\n",
    "#     # From tiegcm lists\n",
    "#     rhosat_tiegcm = rhosat_tiegcm[~np.isnan(rhosat_tiegcm)]\n",
    "#     rho400_tiegcm = rho400_tiegcm[~np.isnan(rho400_tiegcm)]\n",
    "#     D400_tiegcm   = D400_tiegcm[~np.isnan(D400_tiegcm)]\n",
    "\n",
    "\n",
    "#     df = pd.DataFrame(data={'Date' :date ,\n",
    "#                             'Year'          : Year,\n",
    "#                             'Doy'           : Doy,\n",
    "#                             'Hours'         : Hours,\n",
    "#                             'Lon'           : Lon,\n",
    "#                             'Lat'           : Lat, \n",
    "#                             'LatBin'        : LatBin,\n",
    "#                             'Height'        : Height,\n",
    "#                             'LocTim'        : LocTim,\n",
    "#                             'CHAMPDensity'  : CHAMPDensity,\n",
    "#                             'rhosat_msis00' : rhosat_msis00, ### MSIS00 density @Satellite Altitude\n",
    "#                             'rhosat_msis2'  : rhosat_msis2,  ### MSIS2 density @Satellite Altitude\n",
    "#                             'D400_msis00'   : D400_msis00,   ### density normalized to 400km with msis00\n",
    "#                             'rho400_msis2'  : rho400_msis2,  ### MSIS2 density @400km \n",
    "#                             'D400_msis2'    : D400_msis2,    ### density normalized to 400km with msis2\n",
    "#                             #\n",
    "#                             'rhosat_tiegcm'  : rhosat_tiegcm,  ### Tiegcm density @Satellite Altitude\n",
    "#                             'rho400_tiegcm'  : rho400_tiegcm,  ### Tiegcm density @400km \n",
    "#                             'D400_tiegcm'    : D400_tiegcm,    ### density normalized to 400km with Tiegcm\n",
    "#                             #                             'Cd'            : Cd,\n",
    "#     #                         'Ap_dayvals'    : Ap_dayvals,\n",
    "#     #                         'f107a_dayvals' : f107a_dayvals,\n",
    "#     #                         'f107d_dayvals' : f107d_dayvals,\n",
    "#                   } )\n",
    "\n",
    "#     df.to_pickle('parallelize/RenormChampWithMSIS2_'+str(year))\n",
    "\n",
    "    \n",
    "#     return(df)    \n",
    "\n",
    "\n",
    "\n",
    "# # import pandas as pd\n",
    "# # import numpy as np \n",
    "# # import sys  \n",
    "# # from scipy.io import loadmat  #allows us to read in .mat files\n",
    "# # from datetime import datetime, timedelta\n",
    "\n",
    "# # #### MAKE MSIS Take the 3HR Ap values\n",
    "# # from pymsis import msis\n",
    "# # SWI_option = [1.0]*25\n",
    "# # SWI_option[8] = -1.0\n",
    "# #             #  C    AP - MAGNETIC INDEX(DAILY) OR WHEN SW(9)=-1. :\n",
    "# #             #  C      - ARRAY CONTAINING:\n",
    "# #             #  C       (1) DAILY AP\n",
    "# #             #  C       (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "# #             #  C       (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "# #             #  C       (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "# #             #  C       (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "# #             #  C       (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "# #             #  C          TO CURRENT TIME\n",
    "# #             #  C       (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "# #             #  C          TO CURRENT TIME\n",
    "\n",
    "# # import sys  \n",
    "# # sys.path.insert(0, 'util_funcs/')\n",
    "# # from read_CHAMP_data import get_CHAMP_data\n",
    "\n",
    "# # # #################################\n",
    "# # # years =  [2002]#, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\n",
    "# # # days = np.arange(1,2)\n",
    "# # # path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "# # # #################################\n",
    "\n",
    "\n",
    "# # def make_champ_timeseries_v2_redorenorm(years, days):\n",
    "    \n",
    "# #     \"\"\"This function redoes the construction of the CHAMP timeseries for the orbit.\n",
    "    \n",
    "# #     Changes from the original include:\n",
    "# #         - Use F10.7 indicies that are scaled with MgII and re-referenced to the Earth's locatiion (instead of at 1AU)\n",
    "# #         - Use MSIS2.0 for the normalization to 400km\n",
    "# #         - Use a 3Hour Ap input as required by MSIS for the more granular Stormtime forcing option.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #     \"\"\"    \n",
    "\n",
    "    \n",
    "# #     path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "    \n",
    "    \n",
    "# #     #### Load MgII Scaled F10.7 Values:\n",
    "# #     import pickle\n",
    "# #     dir_save = '/space/DNR_data/'\n",
    "# #     filehandler = open(dir_save+'MgII_F107_KpAp'+'.pkl', 'rb') \n",
    "# #     mgII_data = pickle.load(filehandler)\n",
    "# #     filehandler.close()\n",
    "        \n",
    "# #     #### clear up some space        \n",
    "# #     truncate_date    = np.logical_and(mgII_data['Date'].year>=2000 , mgII_data['Date'].year<=2011 )\n",
    "# #     truncate_date3hr =  np.logical_and(mgII_data['Date_3hrAp'].year>=2000 , mgII_data['Date_3hrAp'].year<=2011 )#np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "\n",
    "# #     mgII_data['Date_3hrAp'] = mgII_data['Date_3hrAp'][truncate_date3hr]\n",
    "# #     mgII_data['Ap']         = np.array(mgII_data['Ap'])[truncate_date3hr]\n",
    "# #     mgII_data['Date']       = mgII_data['Date'][truncate_date]\n",
    "# #     mgII_data['Ap_dailyavg'] =  np.array(mgII_data['Ap_dailyavg'])[truncate_date]\n",
    "# #     mgII_data['f107d_earth'] = np.array(mgII_data['f107d_earth'])[truncate_date]\n",
    "# #     mgII_data['f107a_earth'] = np.array(mgII_data['f107a_earth'])[truncate_date]\n",
    "\n",
    "# #     del mgII_data['DOY']\n",
    "# #     del mgII_data['kp']\n",
    "# #     del mgII_data['f107d']\n",
    "# #     del mgII_data['f107a']\n",
    "# #     del mgII_data['year_day']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #     #     noaa = pd.read_pickle('/space/DNR_data/noaa_2002_2010_pickle')\n",
    "# #     #     noaa['f107d'][noaa['f107d'].astype(float) <= 60] = noaa['f107a'][noaa['f107d'].astype(float) <= 60]\n",
    "\n",
    "# #     tleng = 0\n",
    "# #     time_full= []\n",
    "# #     Year         = []\n",
    "# #     Doy          = []\n",
    "# #     Hours        = []\n",
    "# #     Lon          = []\n",
    "# #     Lat          = []\n",
    "# #     LatBin       = []\n",
    "# #     Height       = []\n",
    "# #     LocTim       = []\n",
    "# #     CHAMPDensity = []\n",
    "# #     D400_msis00  = []  # normalized quantity\n",
    "# #     rhosat_msis00= []\n",
    "# #     rhosat_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "# #     rho400_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "# #     D400_msis2   = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "# # #     Cd           = []\n",
    "# #     # Ap_dayvals   = []\n",
    "# #     # f107a_dayvals= []\n",
    "# #     # f107d_dayvals= []\n",
    "# #     date = []\n",
    "\n",
    "# #     i = 0\n",
    "# #     for iyear,year in enumerate(years):\n",
    "# #         for iday,day in enumerate(days):\n",
    "# #             champ, breakloop = get_CHAMP_data(path_champ, year, day) \n",
    "# #             if breakloop == False:\n",
    "# #                 leng = np.size(champ,0)\n",
    "# #     #                 date_index = datetime(year, 1, 1) + timedelta(float(day)) \n",
    "# #     #                 Ap_old    = float(noaa['Ap'][date_index])\n",
    "# #     #                 f107a_old = float(noaa['f107a'][date_index])\n",
    "# #     #                 f107d_old = float(noaa['f107d'][date_index])\n",
    "# #                 leng2 = leng+tleng\n",
    "\n",
    "# #                 Year[tleng:leng2]          = champ['Year'][:leng]\n",
    "# #                 Doy[tleng:leng2]           = champ['Doy'][:leng]\n",
    "# #                 Hours[tleng:leng2]         = champ['Hours'][:leng]\n",
    "# #                 Lon[tleng:leng2]           = champ['Lon'][:leng]\n",
    "# #                 Lat[tleng:leng2]           = champ['Lat'][:leng]\n",
    "# #                 LatBin[tleng:leng2]        = champ['LatBin'][:leng]\n",
    "# #                 Height[tleng:leng2]        = champ['Height'][:leng]\n",
    "# #                 LocTim[tleng:leng2]        = champ['LocTim'][:leng]\n",
    "# #                 CHAMPDensity[tleng:leng2]  = champ['Density'][:leng]\n",
    "# #                 D400_msis00[tleng:leng2]   = champ['D400'][:leng]\n",
    "# #                 rhosat_msis00[tleng:leng2] = champ['Dmsis'][:leng]\n",
    "# # #                 Cd[tleng:leng2]            = champ['Cd'][:leng]\n",
    "\n",
    "\n",
    "# #                 ####---------------------------------------------\n",
    "# #                 #### Gather the Necessary Flux and Ap Information\n",
    "# #                 index_date = np.logical_and(mgII_data['Date'].year==year , mgII_data['Date'].dayofyear==day )\n",
    "# #                 f107a = float(np.squeeze(np.asarray(mgII_data['f107a_earth'])[index_date]))\n",
    "# #                 f107d = float(np.squeeze(np.asarray(mgII_data['f107d_earth'])[index_date]))\n",
    "# #                 Ap_daily_avg = float(np.squeeze(np.asarray(mgII_data['Ap_dailyavg'])[index_date]))\n",
    "\n",
    "\n",
    "# #                 ### Construct the necessary 3hr Ap Array to go into MSIS\n",
    "# #     #             print('---------------------------------------------')\n",
    "# #                 print(f\"Year: {year} / Day: {day}\")\n",
    "# #     #             daily_avg_ap = np.mean(Ap)\n",
    "# #                 for it, itime in enumerate( champ['Hours'][:leng]):\n",
    "# #                     date.append(datetime(year, 1, 1) + timedelta(days = float(day)-1,  hours = itime )) \n",
    "\n",
    "# #                     lon   = champ['Lon'][it]\n",
    "# #                     lat   = champ['Lat'][it]\n",
    "# #                     dates = datetime(year, 1, 1) + timedelta(float(day) - 1) + timedelta(hours = champ['Hours'][it]) \n",
    "# #                     f107din = [f107d]\n",
    "# #                     f107ain = [f107a]\n",
    "\n",
    "# #                     index_date3hr = np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "# #                     indexvals =  [i for i, x in enumerate(index_date3hr) if x]\n",
    "# #                     Ap_doy_windows = mgII_data['Date_3hrAp'][indexvals]\n",
    "\n",
    "# #                     #### Find the Current 3hr Kp window:A\n",
    "# #                     Ap_windw_hrs = [i.hour for i in Ap_doy_windows]\n",
    "# #                     Ap_windw_hrs = np.append(np.array(Ap_windw_hrs),24)  ## add the final window edge\n",
    "\n",
    "# #                     index_current_Ap = int(np.digitize([dates.hour],Ap_windw_hrs))\n",
    "# #                     if index_current_Ap==8:\n",
    "# #                         index_current_Ap += -1\n",
    "# #                     indexglobal_currentAp = indexvals[index_current_Ap]\n",
    "\n",
    "# #                     Ap_3HR_current        = mgII_data['Ap'][indexglobal_currentAp]\n",
    "# #                     Ap_3HR_prior          = mgII_data['Ap'][indexglobal_currentAp-1]\n",
    "# #                     Ap_6HR_prior          = mgII_data['Ap'][indexglobal_currentAp-2]\n",
    "# #                     Ap_9HR_prior          = mgII_data['Ap'][indexglobal_currentAp-3]\n",
    "# #                     Ap_12hr_33hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-11 :indexglobal_currentAp-3 ] ) ### 33hrs to 12 hours\n",
    "# #                     Ap_36hr_57hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-19 :indexglobal_currentAp-11 ])  ### 36hrs to 57 hours\n",
    "\n",
    "# #                     #### CHECK THE DATES!!!!\n",
    "# #                     # print(f'      Current Date: {dates}')\n",
    "# #                     # print('3HR_current ', mgII_data['Date_3hrAp'][indexglobal_currentAp])\n",
    "# #                     # print('3HR_prior ', mgII_data['Date_3hrAp'][indexglobal_currentAp-1])\n",
    "# #                     # print('6HR_prior ', mgII_data['Date_3hrAp'][indexglobal_currentAp-2])\n",
    "# #                     # print('9HR_prior ', mgII_data['Date_3hrAp'][indexglobal_currentAp-3])\n",
    "# #                     # print('12hr_33hr_priorAVG ', mgII_data['Date_3hrAp'][indexglobal_currentAp-11 :indexglobal_currentAp-3 ]  )\n",
    "# #                     # print('36hr_57hr_priorAVG ', mgII_data['Date_3hrAp'][indexglobal_currentAp-19 :indexglobal_currentAp-11 ] )\n",
    "\n",
    "# #                     apsin = [[Ap_daily_avg,          # (1) DAILY AP\n",
    "# #                               Ap_3HR_current,        # (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "# #                               Ap_3HR_prior,          # (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "# #                               Ap_6HR_prior,          # (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "# #                               Ap_9HR_prior,          # (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "# #                               Ap_12hr_33hr_priorAVG, # (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "# #                               Ap_36hr_57hr_priorAVG]]# (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "\n",
    "# #                     output2_sat = msis.run(dates, lon, lat, champ['Height'][it], f107din, f107ain, apsin, version = 2, options=SWI_option)\n",
    "# #                     output2_400 = msis.run(dates, lon, lat, 400, f107din, f107ain, apsin                , version = 2, options=SWI_option)\n",
    "\n",
    "\n",
    "# #                     ### add the values to the growing lists\n",
    "# #                     rhosat_msis2[tleng+it] = output2_sat[0,0,0,0][0]\n",
    "# #                     rho400_msis2[tleng+it] = output2_400[0,0,0,0][0]\n",
    "# #                     D400_msis2[tleng+it]   = champ['Density'][it] * (output2_400[0,0,0,0][0] / output2_sat[0,0,0,0][0])   # normalized density to 400km\n",
    "\n",
    "# #                 tleng = tleng + leng\n",
    "# #     #                 print(year,'/',day)\n",
    "# #                 i+=1\n",
    "\n",
    "# #             elif breakloop == True:\n",
    "# #                 i+=1\n",
    "# #                 continue\n",
    "\n",
    "# #     rhosat_msis2 = rhosat_msis2[~np.isnan(rhosat_msis2)]\n",
    "# #     rho400_msis2 = rho400_msis2[~np.isnan(rho400_msis2)]\n",
    "# #     D400_msis2 = D400_msis2[~np.isnan(D400_msis2)]\n",
    "\n",
    "# #     df = pd.DataFrame(data={'Date' :date ,\n",
    "# #                             'Year'          : Year,\n",
    "# #                             'Doy'           : Doy,\n",
    "# #                             'Hours'         : Hours,\n",
    "# #                             'Lon'           : Lon,\n",
    "# #                             'Lat'           : Lat, \n",
    "# #                             'LatBin'        : LatBin,\n",
    "# #                             'Height'        : Height,\n",
    "# #                             'LocTim'        : LocTim,\n",
    "# #                             'CHAMPDensity'  : CHAMPDensity,\n",
    "# #                             'rhosat_msis00' : rhosat_msis00, ### MSIS00 density @Satellite Altitude\n",
    "# #                             'rhosat_msis2'  : rhosat_msis2,  ### MSIS2 density @Satellite Altitude\n",
    "# #                             'D400_msis00'   : D400_msis00,   ### density normalized to 400km with msis00\n",
    "# #                             'rho400_msis2'  : rho400_msis2,  ### MSIS2 density @400km \n",
    "# #                             'D400_msis2'    : D400_msis2,    ### density normalized to 400km with msis2\n",
    "# # #                             'Cd'            : Cd,\n",
    "# #     #                         'Ap_dayvals'    : Ap_dayvals,\n",
    "# #     #                         'f107a_dayvals' : f107a_dayvals,\n",
    "# #     #                         'f107d_dayvals' : f107d_dayvals,\n",
    "# #                   } )\n",
    "\n",
    "# #     df.to_pickle('parallelize/RenormChampWithMSIS2_'+str(year))\n",
    "# # #     df.to_pickle('constructed_files/RenormChampWithMSIS2_'+year)\n",
    "\n",
    "    \n",
    "# #     return    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "428.812px",
    "left": "822.875px",
    "right": "20px",
    "top": "202px",
    "width": "504px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
