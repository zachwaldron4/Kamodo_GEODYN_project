{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d7777a",
   "metadata": {},
   "source": [
    "Goal here:\n",
    "\n",
    "- Show how to modify a method within the pygeodyn class.\n",
    "- increase the number of drag outputs\n",
    "\n",
    "Figure out:\n",
    "- extend the prediction window\n",
    "- extend the arc length??\n",
    "\n",
    "\n",
    "Put the following plots together:\n",
    "- In-track component orbit determination + prediction\n",
    "- drag coefficient adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196db6dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:28:07.312969Z",
     "start_time": "2021-06-23T16:28:07.208092Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260a029f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:28:07.319338Z",
     "start_time": "2021-06-23T16:28:07.315251Z"
    }
   },
   "outputs": [],
   "source": [
    "### Identify which arcs you want to run:\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/data/geodyn_proj/pygeodyn/pygeodyn_develop/')\n",
    "\n",
    "#------ A dictionary containing the run parameters ------  \n",
    "run_params = {} \n",
    "run_params['arc']              =   ['2018.317_321']    #['2018.317','2018.318','2018.319', '2018.320' ]\n",
    "run_params['satellite']        =  'icesat2'  \n",
    "run_params['SpecialRun_name']  =  '_TrajAnalysis_5dayArc'  \n",
    "run_params['verbose']          =  False\n",
    "run_params['action']           =  'run'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e488471f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:28:07.552945Z",
     "start_time": "2021-06-23T16:28:07.320944Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.insert(0, '/data/geodyn_proj/pygeodyn/pygeodyn_develop/')\n",
    "from PYGEODYN import Pygeodyn\n",
    "\n",
    "#### ---------------------------------------------\n",
    "#### ----------------- RUN MSIS2.0 --------------- \n",
    "#### ---------------------------------------------\n",
    "\n",
    "##### Use copy.deepcopy to copy all levels of dictionary and \n",
    "###       allow modification of new variable\n",
    "run_params1 = copy.deepcopy(run_params)\n",
    "run_params1['den_model'] =  'msis2'  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa6ce0",
   "metadata": {},
   "source": [
    "## Modify the clean_iisset_file() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fbcf9",
   "metadata": {},
   "source": [
    "**Things to consider to extend the arc length:**\n",
    "\n",
    "1. Epoch start date/time\n",
    "2. Epoch end date/time\n",
    "3. ATGRAV time period\n",
    "4. ORBFIL end time (if want prediction, we should extend it and EPOCH further\n",
    "5. EXAT file must be set to the correct days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611a204b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:28:07.663993Z",
     "start_time": "2021-06-23T16:28:07.555539Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### Computer/Command Line functions\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import linecache\n",
    "import time\n",
    "\n",
    "\n",
    "class edit_ICESat2_setup(Pygeodyn):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "    def clean_iisset_file(self):\n",
    "        '''\n",
    "        Overwrite the setup file with the icesat2 specific run parameters.\n",
    "\n",
    "        To make major changes to this function (i.e. implemement a NON-PCE based run of ICESat2)\n",
    "            construct a new class to inherit this one, and overwrite this method in that class. \n",
    "        \n",
    "        This function does the following:\n",
    "            - copies setup file to a temporoary file\n",
    "            - Adds the GLOBAL TITLE Cards (3 strings at top)\n",
    "            - Removes all instances of the GPS satellites\n",
    "            - Deletes specified cards in the cards_to_remove list\n",
    "            - Modifies the cards in card_strings dict\n",
    "            - Includes the time dependent DRAG options in the card_drag_strings dict\n",
    "            - Adds cards that are wanted that are not in the file.\n",
    "\n",
    "\n",
    "        '''\n",
    "#         print('REDIFINE WORKED!')\n",
    "        self.verboseprint('ICESat2 -- clean_iisset_file()')\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        #### Initialize our variables from user input\n",
    "        (path_to_setupfiles, setup_file_arc, SAT_ID, den_model_setupval) = ( self.INPUTDIR,  self.setup_file_arc, self.SATID, self.iisset_den)      \n",
    "        \n",
    "        ORIG_iisset_file = self._INPUT_filename \n",
    "        iisset_file      = 'cleaned_setup'+'_'  + self.arcdate_for_files\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### COPY THE FILE SO THAT YOU DON'T OVERWRITE THE ORIGINAL\n",
    "        ####    We copy to a temporary file \"cleaned_setup_file\"\n",
    "        \n",
    "            \n",
    "        shutil.copyfile(ORIG_iisset_file, self.TMPDIR_arc +'/'+iisset_file+'.bz2')\n",
    "        \n",
    "        os.chdir(self.TMPDIR_arc)\n",
    "        os.system('bunzip2 -v '+ '*.bz2')\n",
    "        os.chdir('/data/geodyn_proj/pygeodyn')\n",
    "        \n",
    "        iisset_file = self.TMPDIR_arc+'/' +'cleaned_setup'+'_'  + self.arcdate_for_files\n",
    "#         print(iisset_file)\n",
    "        \n",
    "        #### --------------------------------------------------------------------\n",
    "        #### identify the cards we do not want in the setup file\n",
    "        cards_to_remove = [ 'ACCEL9',\n",
    "                            'XEPHEM',\n",
    "                            'REFRAC',\n",
    "                            'GPSMOD',\n",
    "                            'OFFSET',\n",
    "                            'OFFADJ',\n",
    "                            'ANTPHC',\n",
    "                            'ANTPH2',\n",
    "                            'CGMASS',\n",
    "                            'OLOAD',\n",
    "                            'DRAG             5041144 ',       # remove the drag effects on the GPS satellites  \n",
    "                            'DRAG             5044284',\n",
    "                            'DRAG             5051204',\n",
    "                            'DRAG             5154184',\n",
    "                            'DRAG             5345214',\n",
    "                            'DRAG             5347224',\n",
    "                            'DRAG             5356164',\n",
    "                            'DRAG             5459194',\n",
    "                            'DRAG             5460234',\n",
    "                            'DRAG             5461024',\n",
    "                            'DRAG             5553175',\n",
    "                            'DRAG             5652315',\n",
    "                            'DRAG             5658125',\n",
    "                            'DRAG             5755155',\n",
    "                            'DRAG             5757295',\n",
    "                            'DRAG             5848075',\n",
    "                            'DRAG             5950055',\n",
    "                            'DRAG             6062256',\n",
    "                            'DRAG             6163016',\n",
    "                            'DRAG             6265246',\n",
    "                            'DRAG             6366276',\n",
    "                            'DRAG             6464306',\n",
    "                            'DRAG             6467066',\n",
    "                            'DRAG             6468096',\n",
    "                            'DRAG             6469036',\n",
    "                            'DRAG             6571266',\n",
    "                            'DRAG             6572086',\n",
    "                            'DRAG             6573106',\n",
    "                            'DRAG             6649045',\n",
    "                            'DRAG             6670326',\n",
    "                            'DRAG             9743134',\n",
    "                            'DRAG             9946114',\n",
    "                            'DRAG   0 0',\n",
    "                            'MBIAS',\n",
    "                           # \n",
    "                            'SATPAR',\n",
    "                            'EPOCH',\n",
    "                            'ELEMS1',\n",
    "                            'ELEMS2',\n",
    "                           #\n",
    "                            'ORBTVU',\n",
    "                            'RESID',\n",
    "                          ] \n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### Grab the EPOCH start and end times\n",
    "        \n",
    "        dt_1days = pd.Series(pd.to_timedelta(24,'h'))\n",
    "        dt_2days = pd.Series(pd.to_timedelta(48,'h'))\n",
    "        dt_5days = pd.Series(pd.to_timedelta(24*5,'h'))\n",
    "        dt_6days = pd.Series(pd.to_timedelta(24*6,'h'))\n",
    "\n",
    "        EPOCH_lines = []\n",
    "        with open(iisset_file, 'r') as f:\n",
    "            for line_no, line_text in enumerate(f):\n",
    "                if 'EPOCH         ' in line_text:\n",
    "                    EPOCH_lines.append(line_no) \n",
    "        \n",
    "        \n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### Identify and save the EPOCH start and end times\n",
    "        for i,val in enumerate(EPOCH_lines):\n",
    "            satpar_line = linecache.getline(iisset_file,val) # Check the above SATPAR line get the correct satellite ID (i.e. NOT GPS)\n",
    "\n",
    "            ##### only do this for the main satellite, so look for the correct SATID in the SATPAR card above EPOCH\n",
    "            if SAT_ID in satpar_line:\n",
    "#                 epoch_start = linecache.getline(iisset_file,val + 1)[20:40].strip() #181013210000.0000000\n",
    "                \n",
    "                #### The dates come in at 2100H the day before the desired day.\n",
    "                ####  We reset the epoch start to be 0000 on the day we want and end 5 days later\n",
    "    \n",
    "                epoch_start_YYMMDD =  linecache.getline(iisset_file,val + 1)[20:26].strip()       # 181013\n",
    "                epoch_start_addday =  pd.to_datetime( epoch_start_YYMMDD, format='%y%m%d') \n",
    "                epoch_start_YYMMDD =  (epoch_start_addday+dt_1days).dt.strftime('%y%m%d')\n",
    "                \n",
    "                epoch_start_HHMM   = '0000'   #linecache.getline(iisset_file,val + 1)[26:30].strip()         # 2100\n",
    "                epoch_start_SS_SSSSSSS = '00.0000000' #linecache.getline(iisset_file,val + 1)[30:40].strip()   # 00.0000000     \n",
    "                epoch_start = (epoch_start_YYMMDD + epoch_start_HHMM+ epoch_start_SS_SSSSSSS).values[0]\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                epoch_end_YYMMDD     = (pd.to_datetime(epoch_start_YYMMDD, format='%y%m%d')+dt_5days).dt.strftime('%y%m%d')\n",
    "                epoch_end_HHMM       = '0000' \n",
    "                epoch_end_SS_SSSSSSS = '00.0000000'\n",
    "                epoch_end            = (epoch_end_YYMMDD+epoch_end_HHMM+epoch_end_SS_SSSSSSS).values[0]\n",
    "\n",
    "#                 epoch_end   = linecache.getline(iisset_file,val + 1)[60:80].strip() #1810160300 00.000\n",
    "#                 epoch_end_YYMMDD = linecache.getline(iisset_file,val + 1)[60:66].strip()       # 181016\n",
    "#                 epoch_end_HHMM = linecache.getline(iisset_file,val + 1)[66:70].strip()         # 210000.0000000\n",
    "#                 epoch_end_SS_SSSSSSS = linecache.getline(iisset_file,val + 1)[70:80].strip()   # 00.0000000     \n",
    "\n",
    "\n",
    "#         print('epoch_start_YYMMDD',epoch_start_YYMMDD)\n",
    "#         print('epoch_start_HHMM',epoch_start_HHMM)\n",
    "#         print('epoch_start_SS_SSSSSSS',epoch_start_SS_SSSSSSS)\n",
    "        print('epoch_start',epoch_start)\n",
    "        print('epoch_end',epoch_end)\n",
    "#         print('epoch_end_YYMMDD',epoch_end_YYMMDD)\n",
    "#         print('epoch_end_HHMM',epoch_end_HHMM)\n",
    "#         print('epoch_end_SS_SSSSSSS',epoch_end_SS_SSSSSSS)\n",
    "\n",
    "\n",
    "#         import sys\n",
    "#         sys.exit(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### --------------------------------------------------------------------\n",
    "        #### Use pandas datetime and time delta to make adjustments to the dates on the ATGRAV and DRAG cards\n",
    "        #### --------------------------------------------------------------------\n",
    "        epoch_start_dt = pd.to_datetime( epoch_start_YYMMDD+epoch_start_HHMM, format='%y%m%d%H%M%S')\n",
    "        epoch_end_dt = pd.to_datetime( epoch_end_YYMMDD+epoch_end_HHMM, format='%y%m%d%H%M%S')\n",
    "\n",
    "        ##### ICESat2 has an orbit period of 94.22 minutes\n",
    "        #### Lets adjust the drag coefficient every 96 minutes\n",
    "        \n",
    "        \n",
    "        \n",
    "        dt_epoch_start_minus2days = (epoch_start_dt - dt_2days).dt.strftime('%y%m%d%H%M%S.0000000').values[0]\n",
    "        dt_epoch_end_plus5days    = (epoch_end_dt + dt_5days).dt.strftime('%y%m%d%H%M%S.000').values[0]\n",
    "        \n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        ###       FIND THE X,Y,Z,Xdot,Ydot,Zdot for this epoch start in the PCE data.\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "#         os.system('bunzip2'+' '+self.StateVector_epochs_datafile+'.bz2')\n",
    "        \n",
    "        epoch_start_dt_STR = str(epoch_start_dt)\n",
    "        date_in_file_flag = False\n",
    "        \n",
    "#         print(\"Epoch Start: \", epoch_start_dt_STR)\n",
    "\n",
    "        with open(self.StateVector_epochs_datafile, 'r') as f:\n",
    "            for line_no, line_text in enumerate(f):\n",
    "                \n",
    "                if epoch_start_dt_STR in line_text:\n",
    "                    date_in_file_flag= True\n",
    "#                     print('    ','xyzline',line_no,line_text)\n",
    "\n",
    "                    break\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        if date_in_file_flag == False:\n",
    "            change_elems_flag = False\n",
    "            print(epoch_start_dt_STR,'not found in file.  Leaving ELEMS as is.')\n",
    "#             print('Check that the start date:',epoch_start_dt_STR)\n",
    "#             print('    is within the PCE date range saved in the file')\n",
    "#             print('       ',self.StateVector_epochs_datafile)\n",
    "#                     os.system('bzip2'+' '+'/data/data_geodyn/inputs/icesat2/setups/StateVector_epochs.txt')\n",
    "#             sys.exit()\n",
    "\n",
    "        else:\n",
    "            change_elems_flag = True\n",
    "            xyzline = pd.read_csv(self.StateVector_epochs_datafile, \n",
    "                        skiprows = line_no, \n",
    "                        nrows=1,           \n",
    "                        sep = '\\s+',\n",
    "                        dtype=str,\n",
    "                        names = [\n",
    "                            'Date',\n",
    "                            'MJDSECs', \n",
    "                            'RSECS', #(fractional secs)\n",
    "                            'GPS offset', # (UTC - GPS offset (secs))\n",
    "                            'X',\n",
    "                            'Y',\n",
    "                            'Z',\n",
    "                            'X_dot',\n",
    "                            'Y_dot',\n",
    "                            'Z_dot',\n",
    "                            'YYMMDDhhmmss',\n",
    "                                ],)\n",
    "\n",
    "            X     =  xyzline['X'].values[0].ljust(20)     #'  -745933.8926940708'\n",
    "            Y     =  xyzline['Y'].values[0].ljust(20)     #'  -4864983.834066438'\n",
    "            Z     =  xyzline['Z'].values[0].ljust(20)     #'    4769954.60524261'\n",
    "            X_dot =  xyzline['X_dot'].values[0].ljust(20) #'  457.44564954037634'\n",
    "            Y_dot =  xyzline['Y_dot'].values[0].ljust(20) #'   5302.381564886811'\n",
    "            Z_dot =  xyzline['Z_dot'].values[0].ljust(20) #'    5463.55571622269'\n",
    "\n",
    "    #         os.system('bzip2'+' '+self.StateVector_epochs_datafile)\n",
    "            ##### -------------------------------------------------------------------------------------------\n",
    "            #### --------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        ####   INPUT THE OPTIONS ON THE SPECIFIC CARDS YOU WANT TO CHANGE\n",
    "        ##### Putting in the options is one of the hardest parts of using GEODYN\n",
    "        #####    They require VERY specific inputs depending on the run type.  \n",
    "        card_strings = {}\n",
    "        \n",
    "        \n",
    "            #####  ORBFIL KEY ------ Requests output of trajectory file(s) on specified unit(s) \n",
    "            #####                           on the last iteration of the run.\n",
    "            #####\n",
    "            #####   columns      Orbit output option\n",
    "            #####    7           Coordinate system of output\n",
    "            #####                      0 - True of date (default)\n",
    "            #####                      1 - True of reference date \n",
    "            #####                   ** 2 - Mean of year 2000    \n",
    "            #####    8           Switch indicating whether trajectory file is for a single \n",
    "            #####                  satellite or a set of satellites.\n",
    "            #####                   ** 0 - Single satellite 0 0\n",
    "            #####                      1 - Set of satellites. This option has meaning \n",
    "            #####                            only when used in conjunction with sets of \n",
    "            #####                            satellites (See EPOCH and SLAVE option cards\n",
    "            #####                            for more details ). If satellite ID in columns\n",
    "            #####                            18-24 is a master satellite , then the trajectory\n",
    "            #####                          for all satellites in the set will be output.\n",
    "            #####  9-11           Mandatory unit number for trajectory file. All trajectory \n",
    "            #####                  files within an arc must have unique unit numbers. \n",
    "            #####                  The suggested unit number starts at 130.\n",
    "            #####  18-25        Satellite ID. This field must contain a valid ID.\n",
    "            #####  25-44        START date and time for trajectory output (YYMMDDHHMMSS.SS).\n",
    "            #####  45-59        STOP  date and time for trajectory output (YYMMDDHHMMSS.SS).\n",
    "            #####  60-72        Time interval between successive trajectory outputs.\n",
    "\n",
    "            \n",
    "#                                  12345678901234567 \n",
    "        card_strings['ORBFIL'] =  'ORBFIL20131      '+SAT_ID+'     '+str(epoch_start)[:-6]+'  '+str(epoch_end)[:6]+' 24200.00          60'\n",
    "        card_strings['RESID']  =  'RESIDU12'\n",
    "        card_strings['OBSVU']  =  'OBSVU 2'  # print residuals on last iteration only\n",
    "        #       card_strings['PRNTVU'] =  'PRNTVU55212222    22122'  # original\n",
    "        card_strings['PRNTVU'] =  'PRNTVU5521111211 121122'  # suppress some IIS/IIE outputs.\n",
    "#                                  1234567890 \n",
    "        card_strings['ORBTVU'] =  'ORBTVU1201       '+SAT_ID+'     '+str(epoch_start)[:-6]+'  '+str(epoch_end)[:6]+' 24200.00 .100000D+01'\n",
    "        card_strings['ATMDEN'] =  'ATMDEN  '+ den_model_setupval\n",
    "        card_strings['ATGRAV']  =  'ATGRAV9090              '+dt_epoch_start_minus2days +''+dt_epoch_end_plus5days[:-1]   \n",
    "        card_strings['I64G2E']  =  'I64G2E         25'  # using 30 like in st-SLR run maxed out the memory usage\n",
    "        card_strings['SIGMA           1']  =  'SIGMA           1               1.0                 1.0'    \n",
    "        card_strings['SIGMA           2']  =  'SIGMA           2               1.0                 1.0'    \n",
    "        card_strings['SIGMA           3']  =  'SIGMA           3               1.0                 1.0'   \n",
    "        card_strings['SIGMA          51']  =  'SIGMA          51               10.0D+25             0.10'  \n",
    "        card_strings['SIGMA          85']  =  'SIGMA          85               0.010000            0.010000'  \n",
    "        \n",
    "\n",
    "        ### Fix the coordinate system... PCE Data was in J2000\n",
    "#         card_strings['REFSYS1933 0        ']  = 'REFSYS193310        '+epoch_start+'0'\n",
    "#         card_strings['SATPAR   13']  =  'SATPAR   139     '+SAT_ID+'          9.53000000       1514.000'\n",
    "        card_strings['REFSYS']  = 'REFSYS193310        '+epoch_start+'0'\n",
    "        card_strings['EPOCH'] = 'EPOCH               '+epoch_start+epoch_start+epoch_end\n",
    "        card_strings['SATPAR']  =  'SATPAR   139     '+SAT_ID+'          9.53000000       1514.000'\n",
    "        \n",
    "        if change_elems_flag == True:\n",
    "            card_strings['ELEMS1']  = 'ELEMS11             '+X+''+Y+''+Z+''   \n",
    "            card_strings['ELEMS2']  = 'ELEMS2              '+X_dot+''+Y_dot+''+Z_dot+''\n",
    "                \n",
    "        \n",
    "        #### Suppress the printing of the flux model\n",
    "        card_strings['FLUX  1']  =  'FLUX  0'\n",
    "\n",
    "    \n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ####    Search through the file to see if any of the cards we WANT are NOT in the file\n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### read in all lines of the file and save them\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()                \n",
    "        ##### card flags to see if certain cards are present in the file\n",
    "        card_flag = {}\n",
    "        for card in card_strings:\n",
    "            ### Set the default flag to be False,  if the card is in the file, flip the flag to True\n",
    "            card_flag[card] = False\n",
    "            for line in lines_all:\n",
    "                if card in line:\n",
    "                    card_flag[card] = True\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ####    Edit the cards that exist in the file that we want to modify\n",
    "        #### --------------------------------------------------------------------\n",
    "        ###### Re-write the file line-by-line and EDIT the cards that need to be modified    \n",
    "        lines_replace = {}\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line_num, line in enumerate(lines):\n",
    "                for card in card_strings:\n",
    "                    if card in line:\n",
    "                        lines_replace[line_num] = card_strings[card]\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()\n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for line_num, line in enumerate(lines_all):\n",
    "                if line_num in lines_replace:\n",
    "#                     print('replacing line',lines_replace[line_num])\n",
    "                    f.write(lines_replace[line_num]+'\\n')\n",
    "                else:\n",
    "                     f.write(line)\n",
    "\n",
    "\n",
    "        #### for adding time dependent drag estimations.  We need to do a few things:\n",
    "        ###       Find the drag card that is already in the file:\n",
    "        ###       Add CONDRAG before all drag cards\n",
    "        ###       Add DRAG cards with TIME periods after the first drag card\n",
    "        \n",
    "        \n",
    "                #### --------------------------------------------------------------------\n",
    "        ####   INPUT THE DRAG OPTIONS  for time dependent drag\n",
    "        \n",
    "        #         add_hours_dt = pd.Series(pd.to_timedelta(9,'h'))\n",
    "#         add_mins_dt = pd.Series(pd.to_timedelta(94.22,'m'))\n",
    "        add_mins_dt = pd.Series(pd.to_timedelta(8,'h'))\n",
    "    \n",
    "        #####   Let the first Cd correction happen 8 hours after the start of arc\n",
    "        ####    Adjust drag every 8 hours until end of desired epoch (5 days)\n",
    "        \n",
    "        drag_date_1 =  (epoch_start_dt+ add_mins_dt*1).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_2 =  (epoch_start_dt+ add_mins_dt*2).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_3 =  (epoch_start_dt+ add_mins_dt*3).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_4 =  (epoch_start_dt+ add_mins_dt*4).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_5 =  (epoch_start_dt+ add_mins_dt*5).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_6 =  (epoch_start_dt+ add_mins_dt*6).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_7 =  (epoch_start_dt+ add_mins_dt*7).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_8 =  (epoch_start_dt+ add_mins_dt*8).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_9 =  (epoch_start_dt+ add_mins_dt*9).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_10 = (epoch_start_dt+add_mins_dt*10).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_11 = (epoch_start_dt+add_mins_dt*11).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_12 = (epoch_start_dt+add_mins_dt*12).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_13 = (epoch_start_dt+add_mins_dt*13).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_14 = (epoch_start_dt+add_mins_dt*14).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_15 = (epoch_start_dt+add_mins_dt*15).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('epoch end   :',epoch_end[:-5])\n",
    "        print('epoch start :',epoch_start[:-5])\n",
    "                \n",
    "        card_drag_strings={}                                                                                                           #11307.\n",
    "        card_drag_strings['CONDRG']  =  'CONDRG  1        '+SAT_ID+'     '+str(epoch_start[:-5])+str(epoch_end[:-5])+'         0.50000  28800.'\n",
    "#         card_drag_strings['DRAG   0 0       '+SAT_ID+' 2.3000000000000E+00']  =  'DRAG   0 0       '+SAT_ID+' 2.3000000000000E+00'\n",
    "        card_drag_strings[drag_date_1]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_1[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_2]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_2[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_3]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_3[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_4]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_4[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_5]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_5[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_6]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_6[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_7]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_7[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_8]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_8[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_9]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_9[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_10]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_10[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_11]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_11[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_12]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_12[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_13]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_13[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_14]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_14[:10]+' 0.00    0.100D+02'\n",
    "        card_drag_strings[drag_date_15]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_15[:10]+' 0.00    0.100D+02'\n",
    "        \n",
    "        print('drag date 1',drag_date_1)\n",
    "        print('drag date 2',drag_date_2)\n",
    "        print('drag date 3',drag_date_3)\n",
    "        print('drag date 4',drag_date_4)\n",
    "        print('drag date 5',drag_date_5)\n",
    "        print('drag date 6',drag_date_6)\n",
    "        print('drag date 7',drag_date_7)\n",
    "        print('drag date 8',drag_date_8)\n",
    "        print('drag date 9',drag_date_9)\n",
    "        print('drag date 10',drag_date_10)\n",
    "        print('drag date 11',drag_date_11)\n",
    "        print('drag date 12',drag_date_12)\n",
    "        print('drag date 13',drag_date_13)\n",
    "        print('drag date 14',drag_date_14)\n",
    "        print('drag date 15',drag_date_15)\n",
    "        \n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()                \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for line in lines_all:\n",
    "                if 'DRAG   0 0       '+SAT_ID+' 2.3000000000000E+00' in line:  #this finds the DRAG line.  \n",
    "                    f.write(card_drag_strings['CONDRG'] + ' \\n')\n",
    "                    f.write('DRAG             '+SAT_ID+' 2.2000000000000E+00'+ ' \\n')\n",
    "                    f.write(card_drag_strings[drag_date_1] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_2] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_3] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_4] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_5] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_6] + ' \\n') \n",
    "                    f.write(card_drag_strings[drag_date_7] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_8] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_9] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_10] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_11] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_12] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_13] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_14] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_15] + ' \\n')                 \n",
    "\n",
    "                else:\n",
    "                    f.write(line)\n",
    "                    \n",
    "              \n",
    "        #####-----------------------------------------------------------------------------\n",
    "        #####   Delete the SATPAR GPS, EPOCH, ELEMS110, and ELEMS2 lines after the SATPAR GPS\n",
    "        #####   Do this by finding the SATPAR for our sat and then saving it and the next 3 lines\n",
    "        #####   then delete all the SATPAR,EPOCH,ELEMS110, ELEMS2 and restore the ones we saved\n",
    "        #####-----------------------------------------------------------------------------\n",
    "\n",
    "        if change_elems_flag == False:\n",
    "            ##### read in all lines of the file and save them\n",
    "            with open(iisset_file, \"r\") as f:\n",
    "                lines_all = f.readlines()    \n",
    "            \n",
    "            for iline, line in enumerate(lines_all):\n",
    "                if 'ELEMS1'in line:\n",
    "                    save_ELEMS1 = iline+1\n",
    "                elif 'ELEMS2' in line:\n",
    "                    save_ELEMS2 = iline+1\n",
    "                \n",
    "            line_ELEMS1 = linecache.getline(iisset_file, save_ELEMS1)\n",
    "            line_ELEMS2  = linecache.getline(iisset_file, save_ELEMS2)\n",
    "            \n",
    "            card_strings['ELEMS1']  = line_ELEMS1\n",
    "            card_strings['ELEMS2']  = line_ELEMS2\n",
    "            \n",
    "#             print(line_ELEMS1)\n",
    "#             print(line_ELEMS2)\n",
    "\n",
    "        ####----------------------------------------------------------------------\n",
    "        #### REMOVE CARDS:: rewrite the file without the cards we specified in the cards_to_remove dict\n",
    "        ####----------------------------------------------------------------------\n",
    "\n",
    "        ##### read in all lines of the file and save them\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()    \n",
    "        ##### Re-write the file line-by-line WITHOUT the cards that we want to remove    \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for iline, line in enumerate(lines_all):\n",
    "                if any(card in line for card in cards_to_remove):\n",
    "                    # IF the any of the cards in the list are in the line, dont add it\n",
    "                    pass\n",
    "                else:\n",
    "                    f.write(line)                \n",
    "        \n",
    "                        \n",
    "        \n",
    "        ####----------------------------------------------------\n",
    "        #### Add any cards that we want that are not in the file\n",
    "        ##### this INCLUDES our saved \n",
    "        #####      SATPAR, EPOCH, ELEMS1, ELEMS2\n",
    "        #####---------------------------------------------------------\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()                  \n",
    "\n",
    "        # use some switches to determine if things have already been written in the loop and avoid writing too many\n",
    "        switch_cardcount = 0\n",
    "        switch_2     = True\n",
    "        \n",
    "        for card in card_flag:\n",
    "            if card_flag[card] == False:\n",
    "                with open(iisset_file, \"w\") as f:\n",
    "                    for line in lines_all:\n",
    "                        if 'ALBEDO' in line:  #this overwrites the line after albedo. \n",
    "                            # MAYBE TODO:  this may not write multiple cards that aren't in the file\n",
    "                            if switch_cardcount == 0:\n",
    "                                f.write(line)\n",
    "                                f.write(card_strings[card] + ' \\n') \n",
    "                            else: \n",
    "                                f.write(card_strings[card] + ' \\n')\n",
    "                                switch_cardcount += 1\n",
    "                        else:\n",
    "                            f.write(line)\n",
    "                            \n",
    "        ##### Write our satellite parameters back in         \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for line in lines_all:\n",
    "                if (('REFSYS' in line) and (switch_2 == True)):\n",
    "                    f.write(card_strings['REFSYS']  + ' \\n')\n",
    "                    f.write(card_strings['SATPAR']  + ' \\n')\n",
    "                    f.write(card_strings['EPOCH']   + ' \\n')\n",
    "                    if change_elems_flag == True:\n",
    "                        f.write(card_strings['ELEMS1']  + ' \\n')\n",
    "                        f.write(card_strings['ELEMS2']  + ' \\n')\n",
    "                    elif change_elems_flag == False:\n",
    "                        f.write(card_strings['ELEMS1'])\n",
    "                        f.write(card_strings['ELEMS2'])\n",
    "\n",
    "                    switch_2 = False\n",
    "                else:\n",
    "                    f.write(line)\n",
    "\n",
    "                    \n",
    "#         self.verboseprint('    ','Orig:')\n",
    "#         self.verboseprint('    ','    ',line_ELEMS11.rstrip('\\n'))\n",
    "#         self.verboseprint('    ','    ',line_ELEMS2.rstrip('\\n'))\n",
    "        self.verboseprint('    ','PCE Update:')\n",
    "        self.verboseprint('    ','    ',card_strings['ELEMS1'])\n",
    "        self.verboseprint('    ','    ',card_strings['ELEMS2'])\n",
    "        \n",
    "        ####----------------------------------------------------------------------\n",
    "        #### Add three lines to the start of the file.  This is the GLOBAL TITLE\n",
    "        ####----------------------------------------------------------------------\n",
    "\n",
    "        with open(iisset_file, 'r+') as f:\n",
    "            content = f.read()\n",
    "            f.seek(0, 0)   # find the first lines\n",
    "            f.write('### \\n') \n",
    "            f.write('###   '+self.arc_name_id+'  \\n') \n",
    "            f.write('### \\n') \n",
    "            f.write(content) \n",
    "            \n",
    "            \n",
    "        ####----------------------------------------------------------------------\n",
    "        #### Try doing a complete job of removing the GPS satellites.\n",
    "        ####----------------------------------------------------------------------\n",
    "            \n",
    "        delete_gps_sats = [ '5041144',\n",
    "                            '5044284',\n",
    "                            '5051204',\n",
    "                            '5154184',\n",
    "                            '5345214',\n",
    "                            '5347224',\n",
    "                            '5356164',\n",
    "                            '5459194',\n",
    "                            '5460234',\n",
    "                            '5461024',\n",
    "                            '5553175',\n",
    "                            '5652315',\n",
    "                            '5658125', \n",
    "                            '5755155',\n",
    "                            '5757295',\n",
    "                            '5848075',\n",
    "                            '5950055',\n",
    "                            '6062256',\n",
    "                            '6163016',\n",
    "                            '6265246',\n",
    "                            '6366276',\n",
    "                            '6464306',\n",
    "                            '6467066',\n",
    "                            '6468096',\n",
    "                            '6469036',\n",
    "                            '6571266',            \n",
    "                            '6572086',\n",
    "                            '6573106',\n",
    "                            '6649045',\n",
    "                            '6670326',\n",
    "                            '9743134',\n",
    "                            '9946114',        \n",
    "                            ]\n",
    "        ##### read in all lines of the file and save them\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()    \n",
    "        ##### Re-write the file line-by-line WITHOUT the cards that we want to remove    \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for iline, line in enumerate(lines_all):\n",
    "                if any(gps in line for gps in delete_gps_sats):\n",
    "                    # IF the any of GPS IDs in the list are in the line, dont add it the line\n",
    "                    pass\n",
    "                else:\n",
    "                    f.write(line)      \n",
    "\n",
    "    def set_file_paths_for_multiple_arcs(self, arc_val, iarc, unzip_and_loadpaths=False):\n",
    "        '''\n",
    "        Handles the Arc naming conventions\n",
    "        Construct a way to read in the satellite specific filenames.\n",
    "        '''\n",
    "        \n",
    "#         print('4 icesat ---- check ---- init set_file_paths_for_multiple_arcs class')\n",
    "\n",
    "        self.run_ID = 'Run # '+ str(iarc+1)\n",
    "        \n",
    "        self.arc_name_id = arc_val\n",
    "        self.YR  = self.arc_name_id[0:4]\n",
    "        doy = self.arc_name_id[5:]\n",
    "        self.arcdate_for_files = self.YR + doy\n",
    "        ####\n",
    "        #### The setup files and the external attitutde files have the same naming convention.\n",
    "#         print('self.arc_name_id',self.arc_name_id)\n",
    "        self.setup_file_arc    = 'iisset.'+self.arc_name_id\n",
    "        \n",
    "        #(epoch_end_dt + dt_5days).dt.strftime('%y%m%d%H%M%S.000').values[0]\n",
    "        # (pd.to_datetime(epoch_start_YYMMDD, format='%y%m%d')+dt_5days).dt.strftime('%y%m%d')\n",
    "#         arc1_name1date = (pd.to_datetime(self.arc_name_id[:-4], format='%y%m%d')+pd.Series(pd.to_timedelta(24*1,'h'))).dt.strftime('%y%m%d')\n",
    "        \n",
    "        arc_name1 = (pd.to_datetime(self.arc_name_id[:-4], format='%Y.%j')+pd.Series(pd.to_timedelta(24*0,'h'))).dt.strftime('%Y.%j')\n",
    "        arc_name2 = (pd.to_datetime(self.arc_name_id[:-4], format='%Y.%j')+pd.Series(pd.to_timedelta(24*1,'h'))).dt.strftime('%Y.%j')\n",
    "        arc_name3 = (pd.to_datetime(self.arc_name_id[:-4], format='%Y.%j')+pd.Series(pd.to_timedelta(24*2,'h'))).dt.strftime('%Y.%j')\n",
    "        arc_name4 = (pd.to_datetime(self.arc_name_id[:-4], format='%Y.%j')+pd.Series(pd.to_timedelta(24*3,'h'))).dt.strftime('%Y.%j')\n",
    "        arc_name5 = (pd.to_datetime(self.arc_name_id[:-4], format='%Y.%j')+pd.Series(pd.to_timedelta(24*4,'h'))).dt.strftime('%Y.%j')\n",
    "        \n",
    "        \n",
    "        self.external_attitude = {}\n",
    "        \n",
    "        self.external_attitude[1] = 'EXAT01.'+arc_name1+'.gz'\n",
    "        self.external_attitude[2] = 'EXAT01.'+arc_name2+'.gz'\n",
    "        self.external_attitude[3] = 'EXAT01.'+arc_name3+'.gz'\n",
    "        self.external_attitude[4] = 'EXAT01.'+arc_name4+'.gz'\n",
    "        self.external_attitude[5] = 'EXAT01.'+arc_name5+'.gz'\n",
    "\n",
    "        print('self.external_attitude[1]',self.external_attitude[1])\n",
    "        print('self.external_attitude[2]',self.external_attitude[2])\n",
    "        print('self.external_attitude[3]',self.external_attitude[3])\n",
    "        print('self.external_attitude[4]',self.external_attitude[4])\n",
    "        print('self.external_attitude[5]',self.external_attitude[5])\n",
    "        ####\n",
    "        ### Now specify what we what the output arcs to be named.\n",
    "        self.ARC = (self.SATELLITE_dir    + '_' + \n",
    "                    self.arcdate_for_files+ '_' + \n",
    "                    self.arc_length + '.' +  \n",
    "                    self.DEN_DIR)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.path_to_model = ('/data/data_geodyn/results/'+\n",
    "                                   self.SATELLITE_dir +'/'+\n",
    "                                   self.den_model+'/'+  \n",
    "                                   self.den_model+'_'+ self.ACCELS + self.SpecialRun_name +'/')\n",
    "        file_name =   self.ARC         \n",
    "#         print('        ')\n",
    "        if unzip_and_loadpaths == True:\n",
    "            pass\n",
    "        elif self.action == 'read':\n",
    "            print('     Loading ... ', file_name,' ' ,sep = '')\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        ####  save the specific file names as \"private members\" with the _filename convention\n",
    "        self._asciixyz_filename = self.path_to_model + 'XYZ_TRAJ/'+ file_name\n",
    "        self._orbfil_filename = self.path_to_model + 'ORBITS/'+ file_name+'_orb1'\n",
    "        self._iieout_filename   = self.path_to_model + 'IIEOUT/'  + file_name\n",
    "        self._density_filename  = self.path_to_model + 'DENSITY/' + file_name     \n",
    "#         self._EXTATTITUDE_filename = self.EXATDIR +'/' +self.external_attitude\n",
    "\n",
    "        \n",
    "        time.sleep(1)\n",
    "#     def prepare_tmpdir_for_geodyn_run(self):\n",
    "#         '''  This it the ICESAT2 version of this method.\n",
    "             \n",
    "#              it is being overridden to INCLUDE the external attitude\n",
    "#         '''\n",
    "#         self.verboseprint('ICESat2 -- prepare_tmpdir_for_geodyn_run()')\n",
    "# #         print(self.TMPDIR_arc)\n",
    "#         self.verboseprint(self.tabtab,'Current DIR: ',os.getcwd())\n",
    "     \n",
    "#         #### Navigate TO the TMPDIR\n",
    "#         os.chdir(self.TMPDIR_arc)\n",
    "        \n",
    "#         ####-------------------------------------------------------------\n",
    "#         ####     Construct Common Setup of a GEODYN RUN\n",
    "#         ####         this is run in the TMPDIR_arc\n",
    "#         ####-------------------------------------------------------------\n",
    "#         self.verboseprint('-------------------------------------------------')\n",
    "#         self.verboseprint('       Linking files with the command line       ')\n",
    "#         self.verboseprint('-------------------------------------------------')\n",
    "        \n",
    "#         self.verboseprint(self.tabtab,'Current DIR',os.getcwd())\n",
    "\n",
    "#         #### make copy to the External attitude file and save as EXAT01\n",
    "#         if not os.path.exists(self.TMPDIR_arc +'/EXAT01'+'.gz'):\n",
    "#             shutil.copyfile(self._EXTATTITUDE_filename, self.TMPDIR_arc +'/EXAT01'+'.gz')\n",
    "# #                 os.symlink(self._EXTATTITUDE_filename, self.TMPDIR_arc +'/EXAT01')\n",
    "# #                 self.verboseprint(self.tabtab,'EXAT01:',self._EXTATTITUDE_filename)\n",
    "#             self.verboseprint(self.tabtab,'copied:   exat file  > EXAT01'+'.gz')\n",
    "#             self.verboseprint(self.tabtab,'copied:   '+self._EXTATTITUDE_filename+' > EXAT01'+'.gz')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copy is set up: EXAT01 file')\n",
    "\n",
    "        \n",
    "#         #### make symlink to the G2B file and save as ftn40\n",
    "#         if not os.path.exists(self.TMPDIR_arc +'/ftn40'+''):\n",
    "# #             os.symlink(self._G2B_filename, self.TMPDIR_arc +'/ftn40')\n",
    "#             shutil.copyfile(self._G2B_filename, self.TMPDIR_arc +'/ftn40'+'')\n",
    "#             self.verboseprint(self.tabtab,'copied:   g2b file   > ftn40'+'')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copy:  g2b file')\n",
    "\n",
    "#         #### make symlink to the gravity field and save as ftn12\n",
    "#         if not os.path.exists(self.TMPDIR_arc +'/ftn12'+''):\n",
    "#             shutil.copyfile(self._grav_field_filename, self.TMPDIR_arc +'/ftn12'+'')\n",
    "# #             self.verboseprint(self.tabtab,'gravfield:',self._grav_field_filename)\n",
    "#             self.verboseprint(self.tabtab,'copied:   grav field > ftn12'+'')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copy is set up: grav_field file')\n",
    "\n",
    "#         #### make symlink to the ephemerides and save as ftn01\n",
    "#         if not os.path.exists(self.TMPDIR_arc +'/ftn01'+''):\n",
    "# #             os.symlink(self._ephem_filename, self.TMPDIR_arc +'/ftn01')\n",
    "#             shutil.copyfile(self._ephem_filename, self.TMPDIR_arc +'/ftn01'+'')\n",
    "#             self.verboseprint(self.tabtab,'copied:   ephem file > ftn01'+'')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copy is set up: ephem file'+'')\n",
    "\n",
    "#         #### make symlink to the gdntable and save as ftn02\n",
    "#         if not os.path.exists(self.TMPDIR_arc +'/ftn02'):\n",
    "#             shutil.copyfile(self._gdntable_filename, self.TMPDIR_arc +'/ftn02')\n",
    "#             self.verboseprint(self.tabtab,'copied:   gdntable   > ftn02')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copy is set up: gdntable file')\n",
    "\n",
    "\n",
    "#         #### make symlink to the ATGRAVFIL and save as fort.18\n",
    "#         if not os.path.exists(self.TMPDIR_arc +'/fort.18'+''):\n",
    "# #             os.symlink(self._ATGRAV_filename, self.TMPDIR_arc +'/fort.18')\n",
    "#             shutil.copyfile(self._ATGRAV_filename, self.TMPDIR_arc +'/fort.18'+'')\n",
    "# #             shutil.copyfile(self._ATGRAV_filename, self.TMPDIR_arc +'/ftn18')\n",
    "# #             self.verboseprint(self.tabtab,'ATGRAV:',self._ATGRAV_filename)\n",
    "#             self.verboseprint(self.tabtab,'copied:  atgrav     > fort.18'+'')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'symlink is set up: atgrav file')\n",
    "\n",
    "            \n",
    "#         if not os.path.exists(self.TMPDIR_arc+'/ftn05'):\n",
    "#             os.system('cp '+self._INPUT_filename+' '+self.TMPDIR_arc+'/ftn05')\n",
    "#             self.verboseprint(self.tabtab,'copying          : iieout file')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copied           : iieout file')\n",
    "\n",
    "#         if not os.path.exists(self.TMPDIR_arc+'/giis.input'):\n",
    "#             os.system('cp  '+self.TMPDIR_arc+'/ftn05 '+self.TMPDIR_arc+'/giis.input')\n",
    "#             self.verboseprint(self.tabtab,'copying          : giis.input file')\n",
    "#         else:\n",
    "#             self.verboseprint(self.tabtab,'copied              : giis.input file')   \n",
    "\n",
    "#         self.verboseprint('-------------------------------------------------------------------------')\n",
    "#         self.verboseprint('-------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "#         #### GUNZIP the files:  gzip is a very fast compression option.\n",
    "# #         self.verboseprint(self.tabtab, \"gunzipping the input data files\")\n",
    "#         self.verboseprint(self.tabtab, \"gunzipping the input data files\")\n",
    "\n",
    "#         os.system('gunzip -vr *.gz')\n",
    "# #         os.system('gunzip -ftn01.gz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bb8c8",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53c94e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.243338Z",
     "start_time": "2021-06-23T16:28:07.668271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ......... RUNNING GEODYN\n",
      "self.external_attitude[1] 0    EXAT01.2018.317.gz\n",
      "dtype: object\n",
      "self.external_attitude[2] 0    EXAT01.2018.318.gz\n",
      "dtype: object\n",
      "self.external_attitude[3] 0    EXAT01.2018.319.gz\n",
      "dtype: object\n",
      "self.external_attitude[4] 0    EXAT01.2018.320.gz\n",
      "dtype: object\n",
      "self.external_attitude[5] 0    EXAT01.2018.321.gz\n",
      "dtype: object\n",
      "Run # 1     Current Time =      16:28:08\n",
      "Run # 1\n",
      "dict of EXATfilename:  1\n",
      "dict of EXATfilename:  1\n",
      "self._EXTATTITUDE_filename[5] /data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.321.gz\n",
      "Run # 1     Cleaning iisset... :    /data/data_geodyn/inputs/icesat2/setups/iisset.2018.317_321.bz2\n",
      "epoch_start 181113000000.0000000\n",
      "epoch_end 181118000000.0000000\n",
      "0   2018-11-13\n",
      "dtype: datetime64[ns] not found in file.  Leaving ELEMS as is.\n",
      "epoch end   : 181118000000.00\n",
      "epoch start : 181113000000.00\n",
      "drag date 1 181113080000\n",
      "drag date 2 181113160000\n",
      "drag date 3 181114000000\n",
      "drag date 4 181114080000\n",
      "drag date 5 181114160000\n",
      "drag date 6 181115000000\n",
      "drag date 7 181115080000\n",
      "drag date 8 181115160000\n",
      "drag date 9 181116000000\n",
      "drag date 10 181116080000\n",
      "drag date 11 181116160000\n",
      "drag date 12 181117000000\n",
      "drag date 13 181117080000\n",
      "drag date 14 181117160000\n",
      "drag date 15 181118000000\n",
      "+ ————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "|\n",
      "| ---------------------- Some run information ----------------------\n",
      "|\n",
      "|  Run # 1     IISSET Cleaned      tmp/.../cleaned_setup_2018317_321\n",
      "|  Run # 1     Density Model:      msis2\n",
      "|  Run # 1     GEODYN Version:     Kamodo_pygeodyn_MODS\n",
      "|  Run # 1     ARC run:            icesat2_2018317_321_54hr.msis2\n",
      "|  Run # 1     Output directory:   /data/data_geodyn/results/icesat2/msis2/msis2_acceloff_TrajAnalysis_5dayArc\n",
      "|  Run # 1     EXAT File:          {1: '/data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.317.gz', 2: '/data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.318.gz', 3: '/data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.319.gz', 4: '/data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.320.gz', 5: '/data/data_geodyn/inputs/icesat2/external_attitude/EXAT01.2018.321.gz'}\n",
      "|\n",
      "+ ————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Copied 5 EXAT files\n",
      "\n",
      "Run # 1          Running IIS\n",
      "ERRORS FOUND IN IIS: /data/data_geodyn/tmp/msis2_acceloff_TrajAnalysis_5dayArc/icesat2_2018317_321_54hr.msis2/iiserr\n",
      "STOP 15\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/pygeodyn/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "## Construct object with run params\n",
    "Obj_Geodyn = edit_ICESat2_setup(run_params1)\n",
    "\n",
    "## Run pyeodyn for the arcs in the above set.\n",
    "Obj_Geodyn.RUN_GEODYN()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c564c",
   "metadata": {},
   "source": [
    "## Read the Data back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9e2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.255665Z",
     "start_time": "2021-06-23T16:28:07.118Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PYGEODYN import Pygeodyn\n",
    "\n",
    "\n",
    "read_params = copy.deepcopy(run_params1)\n",
    "read_params['action']  =  'read'  \n",
    "read_params['den_model']  =  'msis2'  \n",
    "read_params['request_data']      = ['AdjustedParams',\n",
    "                                    'Trajectory_orbfil',\n",
    "                                    'Density',\n",
    "                                    'Residuals_obs',\n",
    "#                                     'Residuals_summary',\n",
    "                                   ]\n",
    "### Load the data into an object\n",
    "Obj_Geodyn = 0  # clear out the object variable if its got stuff in it.\n",
    "Obj_Geodyn = Pygeodyn(read_params)\n",
    "Obj_Geodyn.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1649e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.256773Z",
     "start_time": "2021-06-23T16:28:07.120Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "config = dict({\n",
    "                'displayModeBar': True,\n",
    "                'responsive': False,\n",
    "#                 'staticPlot': True,\n",
    "                'displaylogo': False,\n",
    "                'showTips': False,\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "# Simplify Plotting Schemes:\n",
    "col1 = px.colors.qualitative.Plotly[0]\n",
    "col2 = px.colors.qualitative.Plotly[1]\n",
    "col3 = px.colors.qualitative.Plotly[2]\n",
    "\n",
    "def Calc_Cd_percent_diff_apriori(obj_m1):\n",
    "    '''\n",
    "    CALCULATE:  \n",
    "    '''\n",
    "\n",
    "    SAT_ID = int(obj_m1.__dict__['global_params']['SATID'])\n",
    "    which_stat = 'CURRENT_VALUE'\n",
    "    model_m1 = obj_m1.__dict__['global_params']['den_model']\n",
    "\n",
    "    all_cd_m1    = []\n",
    "    all_dates_m1 = []\n",
    "\n",
    "    for ii,arc in enumerate(obj_m1.__dict__['global_params']['arc_input']):\n",
    "        i_arc = ii+1\n",
    "#         print(obj_m1.AdjustedParams[arc])\n",
    "        last_iter = list(obj_m1.AdjustedParams[arc].keys())[-1]\n",
    "        labels = list(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'].keys())\n",
    "\n",
    "        for i,val in enumerate(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'].keys()):\n",
    "            all_cd_m1.append(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][val][which_stat])\n",
    "            all_dates_m1.append(labels[i])\n",
    "\n",
    "    # take % difference from a priori\n",
    "    cd_apriori= 2.2\n",
    "    percdiff_cd_m1 = ((np.array(all_cd_m1) - cd_apriori)/ cd_apriori)*100\n",
    "\n",
    "    obj_m1_stats = {}\n",
    "    obj_m1_stats['cd_apriori'] = cd_apriori\n",
    "    obj_m1_stats['cd_percdiff_from_apriori'] = percdiff_cd_m1\n",
    "    obj_m1_stats['all_cd'] = all_cd_m1\n",
    "    obj_m1_stats['all_dates'] = all_dates_m1\n",
    "\n",
    "    return(obj_m1_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6618144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.257631Z",
     "start_time": "2021-06-23T16:28:07.122Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obj_m1  = Obj_Geodyn\n",
    "plot_num = 0\n",
    "\n",
    "\n",
    "# def plot_cd_and_cdratio(fig, obj_m1, plot_num):\n",
    "obj_m1_stats = Calc_Cd_percent_diff_apriori(obj_m1)\n",
    "\n",
    "if plot_num == 0:\n",
    "    col = col1\n",
    "    x_annot = 1.05\n",
    "    y_annot = .90\n",
    "    m_size = 2\n",
    "\n",
    "elif plot_num == 1:\n",
    "    x_annot = 1.05\n",
    "    y_annot = .8\n",
    "    col = col2\n",
    "    m_size = 2\n",
    "\n",
    "elif plot_num == 2:\n",
    "    x_annot = 1.05\n",
    "    y_annot = .65 \n",
    "    col = col3    \n",
    "    m_size = 2\n",
    "\n",
    "SAT_ID = int(obj_m1.__dict__['global_params']['SATID'])\n",
    "which_stat = 'CURRENT_VALUE'\n",
    "\n",
    "\n",
    "model_m1 = obj_m1.__dict__['global_params']['den_model']\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=([\"Timeseries of Cd\", \"A priori-to-adjusted Cd Ratio\"]),\n",
    "    vertical_spacing = 0.15,\n",
    "    )\n",
    "\n",
    "for ii,arc in enumerate(obj_m1.__dict__['global_params']['arc_input']):\n",
    "#         print(arc)\n",
    "\n",
    "    str_run_param = 'run_parameters'+ arc\n",
    "    final_iter = obj_m1.__dict__[str_run_param]['str_iteration']\n",
    "\n",
    "    i_arc = ii+1\n",
    "    last_iter = list(obj_m1.AdjustedParams[arc].keys())[-1]\n",
    "    time_dep_cd_dates = list(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'].keys())\n",
    "    \n",
    "    val_list_1 = []\n",
    "\n",
    "\n",
    "    for i in time_dep_cd_dates:\n",
    "#         print(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][i][which_stat])\n",
    "        val_list_1.append(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][i][which_stat])\n",
    "\n",
    "        cd_ratio =  obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][i][which_stat]/ obj_m1_stats['cd_apriori'] \n",
    "        fig.add_trace(go.Scattergl(x=  [pd.to_datetime(i)],\n",
    "                                   y=  [cd_ratio],\n",
    "                           name= model_m1,\n",
    "                           mode='markers',\n",
    "                           marker=dict(\n",
    "                           color=col,\n",
    "                           size=7,\n",
    "                           ),\n",
    "                           showlegend=False,\n",
    "                           ),\n",
    "                           row=2, col=1,)\n",
    "\n",
    "    #### FIRST PLOT (CD TIMESERIES and APRIORI CD)\n",
    "    fig.add_trace(go.Scattergl(x=time_dep_cd_dates,\n",
    "                               y=val_list_1,\n",
    "                               name= model_m1 ,#+ ' |  Arc ' +str(i_arc) +' | Iters: '+ str(last_iter),\n",
    "                               mode='markers',\n",
    "                               marker=dict(\n",
    "                               color=col,\n",
    "                               size=7,),\n",
    "                               showlegend=False,\n",
    "                               ),\n",
    "                               row=1, col=1,\n",
    "                               )\n",
    "    fig.add_trace(go.Scattergl(x=time_dep_cd_dates,\n",
    "                               y=obj_m1_stats['cd_apriori']*np.ones(np.size(time_dep_cd_dates)),\n",
    "                               name= 'A priori Cd',\n",
    "                               mode='lines+markers',\n",
    "                               marker=dict(\n",
    "                               color='black',\n",
    "                               size=5,),\n",
    "                               showlegend=False,\n",
    "                               ),\n",
    "                               row=1, col=1,\n",
    "                               )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    arc_date_1 = obj_m1.__dict__['global_params']['arc_input'][ii]\n",
    "\n",
    "    arc_date_1 = datetime.strptime(arc_date_1, '%Y.%j')\n",
    "    arc_date_2 = arc_date_1 + (pd.to_timedelta(24,'h'))\n",
    "#         fig = add_arc_background_w_text(fig, 2.2, arc_date_1, arc_date_2, i_arc, False)\n",
    "\n",
    "#### SECOND PLOT (PERC diff b/w apriori and Cd)\n",
    "#     fig = legend_as_annotation(fig, obj_m1.__dict__['global_params']['den_model'], col, x_annot, y_annot)\n",
    "\n",
    "\n",
    "# fix layout info:\n",
    "fig.update_yaxes( title=\"Cd \",  exponentformat= 'power',row=1, col=1)\n",
    "fig.update_yaxes( title=\"ratio\",exponentformat= 'power',row=2, col=1)\n",
    "fig.update_xaxes( title=\"Date\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(title=\"Time Dependent Drag Coefficient \")\n",
    "fig.update_layout(\n",
    "                autosize=False,\n",
    "                width=900,\n",
    "                height=700,\n",
    "                font=dict(size=14))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e4f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T20:47:37.033843Z",
     "start_time": "2021-06-18T20:47:36.958495Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc14c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.258410Z",
     "start_time": "2021-06-23T16:28:07.125Z"
    }
   },
   "outputs": [],
   "source": [
    "def orb_avg(den_df, arc):\n",
    "    lat = np.asarray(den_df[arc]['Lat'])\n",
    "    time_pd = pd.to_datetime(den_df[arc]['Date'])\n",
    "    i = np.nonzero( lat[1:]*lat[0:-1]  <  np.logical_and(0 , lat[1:] > lat[0:-1] )  )\n",
    "    i = i[0]\n",
    "\n",
    "    d_avg = np.zeros(np.size(i))\n",
    "    height_avg = np.zeros(np.size(i))\n",
    "    time_avg = []\n",
    "\n",
    "    d_avg_rolling = []\n",
    "    \n",
    "    roll_avg_count = 0\n",
    "    for j in range(np.size(i)-1):\n",
    "    #     print(j+1)\n",
    "        d_avg[j] = np.mean(  den_df[arc]['rho (kg/m**3)'][   i[j] : i[j+1]-1  ]  )\n",
    "        height_avg[j] = np.mean(  den_df[arc]['Height (meters)'][   i[j] : i[j+1]-1  ]  )\n",
    "        mean_time=time_pd[   i[j] : i[j+1]-1  ].mean() \n",
    "        time_avg.append(  mean_time)\n",
    "\n",
    "        if roll_avg_count ==2:\n",
    "            d_avg_rolling.append(np.mean([ d_avg[j],  d_avg[j-1]]))\n",
    "            \n",
    "            roll_avg_count =0\n",
    "            \n",
    "        roll_avg_count+=1 \n",
    "        \n",
    "    return(time_avg, d_avg, d_avg_rolling )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56a7af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.259230Z",
     "start_time": "2021-06-23T16:28:07.127Z"
    }
   },
   "outputs": [],
   "source": [
    "# obj_m1  = Obj_Geodyn\n",
    "# plot_num = 0\n",
    "\n",
    "\n",
    "# # def plot_cd_and_cdratio(fig, obj_m1, plot_num):\n",
    "# obj_m1_stats = Calc_Cd_percent_diff_apriori(obj_m1)\n",
    "\n",
    "# if plot_num == 0:\n",
    "#     col = col1\n",
    "#     x_annot = 1.05\n",
    "#     y_annot = .90\n",
    "#     m_size = 2\n",
    "\n",
    "# elif plot_num == 1:\n",
    "#     x_annot = 1.05\n",
    "#     y_annot = .8\n",
    "#     col = col2\n",
    "#     m_size = 2\n",
    "\n",
    "# elif plot_num == 2:\n",
    "#     x_annot = 1.05\n",
    "#     y_annot = .65 \n",
    "#     col = col3    \n",
    "#     m_size = 2\n",
    "\n",
    "# SAT_ID = int(obj_m1.__dict__['global_params']['SATID'])\n",
    "# which_stat = 'CURRENT_VALUE'\n",
    "\n",
    "\n",
    "# model_m1 = obj_m1.__dict__['global_params']['den_model']\n",
    "# fig = make_subplots(\n",
    "#     rows=3, cols=1,\n",
    "#     subplot_titles=([\"Density\", 'F10.7 Flux', 'Kp']),\n",
    "#     vertical_spacing = 0.15,\n",
    "#     )\n",
    "\n",
    "# for ii,arc in enumerate(obj_m1.__dict__['global_params']['arc_input']):\n",
    "# #         print(arc)\n",
    "#     time_avg, d_avg, d_avg_rolling = orb_avg(obj_m1.__dict__['Density'], arc)\n",
    "#     fig.add_trace(go.Scattergl(x=obj_m1.__dict__['Density'][arc]['Date'],\n",
    "#                                y=obj_m1.__dict__['Density'][arc]['rho (kg/m**3)'],\n",
    "#                                name= 'Density',\n",
    "#                                mode='markers',\n",
    "#                                marker=dict(\n",
    "#                                color=col,\n",
    "#                                size=3,),\n",
    "#                                showlegend=False,\n",
    "#                                ),\n",
    "#                                row=1, col=1,\n",
    "#                                )\n",
    "#     fig.add_trace(go.Scattergl(x=obj_m1.__dict__['Density'][arc]['Date'],\n",
    "#                                y=obj_m1.__dict__['Density'][arc]['flux_daily'],\n",
    "#                                name= 'Daily',\n",
    "#                                mode='markers',\n",
    "#                                marker=dict(\n",
    "#                                color='grey',\n",
    "#                                size=4,),\n",
    "#                                showlegend=False,\n",
    "#                                ),\n",
    "#                                row=2, col=1,\n",
    "#                                )\n",
    "#     fig.add_trace(go.Scattergl(x=obj_m1.__dict__['Density'][arc]['Date'],\n",
    "#                                y=obj_m1.__dict__['Density'][arc]['flux_avg'],\n",
    "#                                name= '81 day avg',\n",
    "#                                mode='markers',\n",
    "#                                marker=dict(\n",
    "#                                color='black',\n",
    "#                                size=4,),\n",
    "#                                showlegend=False,\n",
    "#                                ),\n",
    "#                                row=2, col=1,\n",
    "#                                )\n",
    "#     fig.add_trace(go.Scattergl(x=obj_m1.__dict__['Density'][arc]['Date'],\n",
    "#                                y=obj_m1.__dict__['Density'][arc]['Kp'],\n",
    "#                                name= '81 day avg',\n",
    "#                                mode='markers',\n",
    "#                                marker=dict(\n",
    "#                                color='black',\n",
    "#                                size=4,),\n",
    "#                                showlegend=False,\n",
    "#                                ),\n",
    "#                                row=3, col=1,\n",
    "#                                )\n",
    "# #     fig.add_trace(go.Scattergl(x=time_avg,\n",
    "# #                                y=d_avg,\n",
    "# #                                name= 'A priori Cd',\n",
    "# #                                mode='markers',\n",
    "# #                                marker=dict(\n",
    "# #                                color='red',\n",
    "# #                                size=8,),\n",
    "# #                                showlegend=False,\n",
    "# #                                ),\n",
    "# #                                row=1, col=1,\n",
    "# #                                )\n",
    "\n",
    "\n",
    "#     str_run_param = 'run_parameters'+ arc\n",
    "#     final_iter = obj_m1.__dict__[str_run_param]['str_iteration']\n",
    "\n",
    "    \n",
    "# # fix layout info:\n",
    "# fig.update_yaxes(type=\"log\", title=\"Cd \",  exponentformat= 'power',row=1, col=1)\n",
    "# fig.update_yaxes( title=\"ratio\",exponentformat= 'power',row=2, col=1)\n",
    "# fig.update_xaxes( title=\"Date\", row=2, col=1)\n",
    "\n",
    "# fig.update_layout(title=\"Time Dependent Drag Coefficient \")\n",
    "# fig.update_layout(\n",
    "#                 autosize=False,\n",
    "#                 width=900,\n",
    "#                 height=600,\n",
    "#                 font=dict(size=14))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35de713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.259977Z",
     "start_time": "2021-06-23T16:28:07.130Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c489df3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.260708Z",
     "start_time": "2021-06-23T16:28:07.132Z"
    }
   },
   "outputs": [],
   "source": [
    "# time_dep_cd_dates = list(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'].keys())\n",
    "# for i in time_dep_cd_dates:\n",
    "#     print(obj_m1.AdjustedParams['2018.318'][13][SAT_ID]['0CD'][i]['CURRENT_VALUE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f94dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.261494Z",
     "start_time": "2021-06-23T16:28:07.135Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from PYGEODYNAnalysis_icesat2PCEtrajectory import plot_cd_and_percdiff_from_apriori\n",
    "\n",
    "\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=1,\n",
    "#     subplot_titles=([\"Timeseries of Cd\", \"Cd Ratio (a priori/adjusted value)\"]),\n",
    "#     vertical_spacing = 0.08,\n",
    "#     )\n",
    "# fig = plot_cd_and_cdratio(fig,  Obj_Geodyn, 0)\n",
    "# # fig = plot_cd_and_cdratio(fig,  Obj_Geodyn3, 1)\n",
    "# # fig = plot_cd_and_cdratio(fig,  Obj_Geodyn1, 2)\n",
    "\n",
    "\n",
    "# fig.show(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352fbfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.262282Z",
     "start_time": "2021-06-23T16:28:07.137Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from PYGEODYNAnalysis_icesat2PCEtrajectory import plot_cd_and_percdiff_from_apriori\n",
    "\n",
    "\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=1,\n",
    "#     subplot_titles=([\"Timeseries of Cd\", \"Percent difference from a priori (Cd=2.2)\"]),\n",
    "#     vertical_spacing = 0.08,\n",
    "#     )\n",
    "# fig = plot_cd_and_percdiff_from_apriori(fig,  Obj_Geodyn, 0)\n",
    "# # fig = plot_cd_and_percdiff_from_apriori(fig,  Obj_Geodyn3, 1)\n",
    "# # fig = plot_cd_and_percdiff_from_apriori(fig,  Obj_Geodyn1, 2)\n",
    "\n",
    "\n",
    "# fig.show(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f8885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.263058Z",
     "start_time": "2021-06-23T16:28:07.139Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from PYGEODYNAnalysis_icesat2PCEtrajectory import ARCOVERLAP_2arcs_ObsResids_NTW_intrack\n",
    "\n",
    "# fig = make_subplots(rows=2, cols=1, \n",
    "#             subplot_titles=(['In-Track Component', 'Residual (PCE-ORBFIL)']),\n",
    "#             vertical_spacing = 0.2,\n",
    "#             specs=[ [{\"secondary_y\": False }],\n",
    "#                     [{\"secondary_y\": False }]],)\n",
    "\n",
    "# arc1 = '2018.319'\n",
    "# arc2 = arc1\n",
    "\n",
    "# fig = ARCOVERLAP_2arcs_ObsResids_NTW_intrack(fig, Obj_Geodyn, 0, arc1, arc2)\n",
    "# # fig = ARCOVERLAP_2arcs_ObsResids_NTW_intrack(fig, Obj_Geodyn3, 1, arc1, arc2)\n",
    "# # fig = ARCOVERLAP_2arcs_ObsResids_NTW_intrack(fig, Obj_Geodyn1, 2, arc1, arc2)\n",
    "\n",
    "# fig.show(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15989b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.263795Z",
     "start_time": "2021-06-23T16:28:07.140Z"
    }
   },
   "outputs": [],
   "source": [
    "# last_nonpredicted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb9d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T00:11:02.164088Z",
     "start_time": "2021-06-23T00:11:02.133813Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68cfcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T16:29:23.264684Z",
     "start_time": "2021-06-23T16:28:07.143Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def ARCOVERLAP_2arcs_ObsResids_NTW_intrack(fig, obj_m1, plot_num, arc1, arc2):\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/data/geodyn_proj/pygeodyn/utils_pygeodyn_develop/util_dir/')\n",
    "from common_functions          import Convert_cartesian_to_RSW, Convert_cartesian_to_NTW\n",
    "\n",
    "\n",
    "\n",
    "###### GET THE PCE DATA:\n",
    "StateVector_PCE_datafile = '/data/data_geodyn/inputs/icesat2/setups/PCE_ascii.txt'\n",
    "#     os.system('bunzip2 -v '+ StateVector_epochs_datafile +'.bz2')\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            subplot_titles=(['Adjusted Cd ratio to a priori', 'In-Track Component Residuals (PCE-ORBFIL)']),\n",
    "            vertical_spacing = 0.1,\n",
    "            specs=[ [{\"secondary_y\": False }],\n",
    "                    [{\"secondary_y\": False }]],)\n",
    "\n",
    "arc1 = '2018.319'\n",
    "arc2 = arc1\n",
    "\n",
    "\n",
    "first_arc = arc1\n",
    "last_arc  = arc2\n",
    "first_arc_first_time = obj_m1.__dict__['Trajectory_orbfil'][first_arc]['data_record']['Date_UTC'].iloc[0],\n",
    "last_arc_last_time   = obj_m1.__dict__['Trajectory_orbfil'][last_arc]['data_record']['Date_UTC'].iloc[-2]\n",
    "first_arc_first_time_str =  str(first_arc_first_time[0])#.replace( \"'\",' ') \n",
    "last_arc_last_time =  str(last_arc_last_time)#.replace( \"'\",' ') \n",
    "\n",
    "print('first_arc_first_time',first_arc_first_time)\n",
    "####---------------------------------------------------------\n",
    "with open(StateVector_PCE_datafile, 'r') as f:\n",
    "    for line_no, line_text in enumerate(f):\n",
    "\n",
    "        if first_arc_first_time_str in line_text:\n",
    "            first_line = line_no\n",
    "        elif last_arc_last_time in line_text:\n",
    "            last_line = line_no\n",
    "            break\n",
    "PCE_data = pd.read_csv(StateVector_PCE_datafile, \n",
    "            skiprows = first_line, \n",
    "            nrows=last_line- first_line,           \n",
    "            sep = '\\s+',\n",
    "            dtype=str,\n",
    "            names = [\n",
    "                    'Date',\n",
    "                    'MJDSECs', \n",
    "                    'RSECS', #(fractional secs)\n",
    "                    'GPS offset', # (UTC - GPS offset (secs))\n",
    "                    'X',\n",
    "                    'Y',\n",
    "                    'Z',\n",
    "                    'X_dot',\n",
    "                    'Y_dot',\n",
    "                    'Z_dot',\n",
    "                    'YYMMDDhhmmss',\n",
    "                        ],)\n",
    "#     os.system('bzip2 -v '+ StateVector_epochs_datafile )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####---------------------------------------------------------\n",
    "\n",
    "PCE_data['Date_pd'] = pd.to_datetime(PCE_data['Date'])\n",
    "\n",
    "orbfil_arc1 = obj_m1.__dict__['Trajectory_orbfil'][arc1]['data_record']\n",
    "orbfil_arc1['Date_pd'] = pd.to_datetime(orbfil_arc1 ['Date_UTC'])\n",
    "\n",
    "orbfil_arc2 = obj_m1.__dict__['Trajectory_orbfil'][arc2]['data_record']\n",
    "orbfil_arc2['Date_pd'] = pd.to_datetime(orbfil_arc2 ['Date_UTC'])\n",
    "\n",
    "\n",
    "\n",
    "C_1 = pd.merge(left=orbfil_arc1, left_on='Date_pd',\n",
    "     right=PCE_data, right_on='Date_pd')\n",
    "C_2 = pd.merge(left=orbfil_arc2, left_on='Date_pd',\n",
    "         right=PCE_data, right_on='Date_pd')\n",
    "\n",
    "\n",
    "\n",
    "X = C_1['Satellite Inertial X coordinate']\n",
    "Y = C_1['Satellite Inertial Y coordinate']\n",
    "Z = C_1['Satellite Inertial Z coordinate']\n",
    "Xdot = C_1['Satellite Inertial X velocity']\n",
    "Ydot = C_1['Satellite Inertial Y velocity']\n",
    "Zdot = C_1['Satellite Inertial Z velocity']\n",
    "state_vector = np.transpose(np.array([X, Y, Z, Xdot, Ydot, Zdot]))\n",
    "\n",
    "InTrack_comp_orbfil = [Convert_cartesian_to_NTW(x) for x in state_vector]\n",
    "\n",
    "X = C_1['X'].astype(float)\n",
    "Y = C_1['Y'].astype(float)\n",
    "Z = C_1['Z'].astype(float)\n",
    "Xdot = C_1['X_dot'].astype(float)\n",
    "Ydot = C_1['Y_dot'].astype(float)\n",
    "Zdot = C_1['Z_dot'].astype(float)\n",
    "state_vector = np.transpose(np.array([X, Y, Z, Xdot, Ydot, Zdot]))\n",
    "\n",
    "InTrack_comp_PCE = [Convert_cartesian_to_NTW(x) for x in state_vector]\n",
    "\n",
    "model_m1 = obj_m1.__dict__['global_params']['den_model']\n",
    "\n",
    "if plot_num == 0:\n",
    "    col = col1\n",
    "    x_annot = 1.05\n",
    "    y_annot = .97\n",
    "    m_size = 3\n",
    "elif plot_num == 1:\n",
    "    x_annot = 1.05\n",
    "    y_annot = .8\n",
    "    col = col2\n",
    "    m_size = 2.5\n",
    "elif plot_num == 2:\n",
    "    x_annot = 1.05\n",
    "    y_annot = .55 \n",
    "    col = col3    \n",
    "    m_size = 2\n",
    "\n",
    "\n",
    "#     for i, arc in enumerate([arc1 , arc2]):\n",
    "#         i_arc = i+1\n",
    "data_skip = 7\n",
    "####--------------------- INTRACK Component  ---------------------\n",
    "# fig.add_trace(go.Scattergl(x=C_1['Date_pd'][::data_skip],\n",
    "#                          y=InTrack_comp_orbfil[::data_skip],\n",
    "#                          name= 'Orbfil',\n",
    "#                          mode='markers',\n",
    "#                          marker=dict(color=col,\n",
    "#                          size=m_size,),\n",
    "#                          showlegend=False,\n",
    "#                          ),\n",
    "#                          secondary_y=False,\n",
    "#                          row=1, col=1,\n",
    "#                          )\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=C_1['Date_pd'][::data_skip],\n",
    "#                          y=InTrack_comp_PCE[::data_skip],\n",
    "#                          name= 'PCE',\n",
    "#                          mode='markers',\n",
    "#                          marker=dict(color=col, opacity=0.3,\n",
    "#                          size=m_size+1,),\n",
    "#                          showlegend=False,\n",
    "#                          ),\n",
    "#                          secondary_y=False,\n",
    "#                          row=1, col=1,\n",
    "#                          )        \n",
    "\n",
    "####--------------------- Residual  ---------------------\n",
    "for ii,arc in enumerate(obj_m1.__dict__['global_params']['arc_input']):\n",
    "#         print(arc)\n",
    "\n",
    "    str_run_param = 'run_parameters'+ arc\n",
    "    final_iter = obj_m1.__dict__[str_run_param]['str_iteration']\n",
    "\n",
    "    i_arc = ii+1\n",
    "    last_iter = list(obj_m1.AdjustedParams[arc].keys())[-1]\n",
    "    time_dep_cd_dates = list(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'].keys())\n",
    "    \n",
    "    val_list_1 = []\n",
    "\n",
    "\n",
    "    for i in time_dep_cd_dates:\n",
    "#         print(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][i][which_stat])\n",
    "        val_list_1.append(obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][i][which_stat])\n",
    "\n",
    "        cd_ratio =  obj_m1.AdjustedParams[arc][last_iter][SAT_ID]['0CD'][i][which_stat]/ obj_m1_stats['cd_apriori'] \n",
    "        fig.add_trace(go.Scattergl(x=  [pd.to_datetime(i)],\n",
    "                                   y=  [cd_ratio],\n",
    "                           name= model_m1,\n",
    "                           mode='markers',\n",
    "                           marker=dict(\n",
    "                           color=col,\n",
    "                           size=7,\n",
    "                           ),\n",
    "                           showlegend=False,\n",
    "                           ),\n",
    "                           row=1, col=1,)\n",
    "        \n",
    "        \n",
    "add_dt = pd.to_timedelta(180,'m')\n",
    "overlap_end   = obj_m1.__dict__['Trajectory_orbfil'][arc1]['data_record']['Date_UTC'].iloc[-1]\n",
    "\n",
    "date = pd.to_datetime(i)\n",
    "\n",
    "while date < overlap_end:\n",
    "    date += add_dt\n",
    "    fig.add_trace(go.Scattergl(x= [date] ,\n",
    "                               y=  list(np.ones(1)),\n",
    "                               name= model_m1,\n",
    "                               mode='markers+lines',\n",
    "                               marker=dict(\n",
    "                               color=col,\n",
    "                               size=7,\n",
    "                               ),\n",
    "                               showlegend=False,\n",
    "                               ),\n",
    "                               row=1, col=1,)\n",
    "\n",
    "\n",
    "\n",
    "resid = (np.array(InTrack_comp_PCE) - np.array(InTrack_comp_orbfil))*1e2\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=C_1['Date_pd'][::data_skip],\n",
    "                           y=resid[::data_skip],\n",
    "                         name= '(PCE-orbfil)',\n",
    "                         mode='markers',\n",
    "                         marker=dict(color=col,\n",
    "                         size=m_size,),\n",
    "                         showlegend=False,\n",
    "                         ),\n",
    "                         secondary_y=False,\n",
    "                         row=2, col=1,\n",
    "                         )\n",
    "\n",
    "### Start of second arc\n",
    "overlap_start = obj_m1.__dict__['Residuals_obs'][arc1]['Date'].iloc[-1]\n",
    "### End of first arc\n",
    "overlap_end   = obj_m1.__dict__['Trajectory_orbfil'][arc1]['data_record']['Date_UTC'].iloc[-1]\n",
    "fig.add_vrect(  x0=overlap_start, x1=overlap_end,\n",
    "                fillcolor='LightSkyBlue', opacity=0.2,\n",
    "                layer=\"below\", line_width=0)\n",
    "\n",
    "# fig = legend_as_annotation(fig, obj_m1.__dict__['global_params']['den_model'], col, x_annot, y_annot)\n",
    "\n",
    "fig.update_layout(title=\"NTW Coord. System + Predicted Window (light blue window)\")\n",
    "fig.update_layout(\n",
    "                autosize=False,\n",
    "                width=800,\n",
    "                height=600,\n",
    "                font=dict(size=12),\n",
    "                legend= {'itemsizing': 'constant'})\n",
    "\n",
    "fig.update_yaxes( title=\"Cd Ratio \",exponentformat= 'power',row=1, col=1)\n",
    "fig.update_yaxes( title=\"Residual (cm)\",       exponentformat= 'power',row=2, col=1)\n",
    "\n",
    "fig.update_xaxes( title=\"Date\", row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Residuals (cm)\", row=1, col=1, secondary_y=True, color='SkyBlue')\n",
    "\n",
    "\n",
    "#     return(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4daacb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T22:37:35.280533Z",
     "start_time": "2021-06-22T22:37:35.260760Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1a899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T22:37:36.335534Z",
     "start_time": "2021-06-22T22:37:36.315442Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba28f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
