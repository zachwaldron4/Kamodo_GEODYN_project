{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d7777a",
   "metadata": {},
   "source": [
    "Goal here:\n",
    "\n",
    "- Show how to modify a method within the pygeodyn class.\n",
    "- increase the number of drag outputs\n",
    "\n",
    "Figure out:\n",
    "- extend the prediction window\n",
    "- extend the arc length??\n",
    "\n",
    "\n",
    "Put the following plots together:\n",
    "- In-track component orbit determination + prediction\n",
    "- drag coefficient adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196db6dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:01.974774Z",
     "start_time": "2021-07-30T21:17:01.856421Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a029f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T17:58:25.525540Z",
     "start_time": "2021-07-14T17:58:25.518929Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e488471f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:02.246838Z",
     "start_time": "2021-07-30T21:17:01.976872Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.insert(0, '/data/geodyn_proj/pygeodyn/pygeodyn_develop/')\n",
    "from PYGEODYN import Pygeodyn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa6ce0",
   "metadata": {},
   "source": [
    "## Modify the clean_iisset_file() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611a204b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:02.338731Z",
     "start_time": "2021-07-30T21:17:02.249488Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### Computer/Command Line functions\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import linecache\n",
    "import time\n",
    "\n",
    "\n",
    "class edit_ICESat2_setup(Pygeodyn):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "    def clean_iisset_file(self):\n",
    "        '''\n",
    "        Overwrite the setup file with the icesat2 specific run parameters.\n",
    "\n",
    "        To make major changes to this function (i.e. implemement a NON-PCE based run of ICESat2)\n",
    "            construct a new class to inherit this one, and overwrite this method in that class. \n",
    "        \n",
    "        This function does the following:\n",
    "            - copies setup file to a temporoary file\n",
    "            - Adds the GLOBAL TITLE Cards (3 strings at top)\n",
    "            - Removes all instances of the GPS satellites\n",
    "            - Deletes specified cards in the cards_to_remove list\n",
    "            - Modifies the cards in card_strings dict\n",
    "            - Includes the time dependent DRAG options in the card_drag_strings dict\n",
    "            - Adds cards that are wanted that are not in the file.\n",
    "\n",
    "            * this\n",
    "\n",
    "        '''\n",
    "#         print('REDIFINE WORKED!')\n",
    "        self.verboseprint('ICESat2 -- clean_iisset_file()')\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        #### Initialize our variables from user input\n",
    "        (path_to_setupfiles, setup_file_arc, SAT_ID, den_model_setupval) = ( self.INPUTDIR,  self.setup_file_arc, self.SATID, self.iisset_den)      \n",
    "        \n",
    "        ORIG_iisset_file = self._INPUT_filename \n",
    "        iisset_file      = 'cleaned_setup'+'_'  + self.arcdate_for_files\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### COPY THE FILE SO THAT YOU DON'T OVERWRITE THE ORIGINAL\n",
    "        ####    We copy to a temporary file \"cleaned_setup_file\"\n",
    "        \n",
    "            \n",
    "        shutil.copyfile(ORIG_iisset_file, self.TMPDIR_arc +'/'+iisset_file+'.bz2')\n",
    "        \n",
    "        os.chdir(self.TMPDIR_arc)\n",
    "        os.system('bunzip2 -v '+ '*.bz2')\n",
    "        os.chdir('/data/geodyn_proj/pygeodyn')\n",
    "        \n",
    "        iisset_file = self.TMPDIR_arc+'/' +'cleaned_setup'+'_'  + self.arcdate_for_files\n",
    "#         print(iisset_file)\n",
    "        \n",
    "        #### --------------------------------------------------------------------\n",
    "        #### identify the cards we do not want in the setup file\n",
    "        cards_to_remove = [ 'XEPHEM',\n",
    "                            'REFRAC',\n",
    "                            'GPSMOD',\n",
    "                            'OFFSET',\n",
    "                            'OFFADJ',\n",
    "                            'ANTPHC',\n",
    "                            'ANTPH2',\n",
    "                            'CGMASS',\n",
    "                            'OLOAD',\n",
    "                            'DRAG             5041144 ',       # remove the drag effects on the GPS satellites  \n",
    "                            'DRAG             5044284',\n",
    "                            'DRAG             5051204',\n",
    "                            'DRAG             5154184',\n",
    "                            'DRAG             5345214',\n",
    "                            'DRAG             5347224',\n",
    "                            'DRAG             5356164',\n",
    "                            'DRAG             5459194',\n",
    "                            'DRAG             5460234',\n",
    "                            'DRAG             5461024',\n",
    "                            'DRAG             5553175',\n",
    "                            'DRAG             5652315',\n",
    "                            'DRAG             5658125',\n",
    "                            'DRAG             5755155',\n",
    "                            'DRAG             5757295',\n",
    "                            'DRAG             5848075',\n",
    "                            'DRAG             5950055',\n",
    "                            'DRAG             6062256',\n",
    "                            'DRAG             6163016',\n",
    "                            'DRAG             6265246',\n",
    "                            'DRAG             6366276',\n",
    "                            'DRAG             6464306',\n",
    "                            'DRAG             6467066',\n",
    "                            'DRAG             6468096',\n",
    "                            'DRAG             6469036',\n",
    "                            'DRAG             6571266',\n",
    "                            'DRAG             6572086',\n",
    "                            'DRAG             6573106',\n",
    "                            'DRAG             6649045',\n",
    "                            'DRAG             6670326',\n",
    "                            'DRAG             9743134',\n",
    "                            'DRAG             9946114',\n",
    "                            'DRAG   0 0',\n",
    "                            'MBIAS',\n",
    "                           # \n",
    "                            'SATPAR',\n",
    "                            'EPOCH',\n",
    "                            'ELEMS1',\n",
    "                            'ELEMS2',\n",
    "                           #\n",
    "                            'ORBTVU',\n",
    "                            'RESID',\n",
    "                          ] \n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### Grab the EPOCH start and end times\n",
    "        EPOCH_lines = []\n",
    "        with open(iisset_file, 'r') as f:\n",
    "            for line_no, line_text in enumerate(f):\n",
    "                if 'EPOCH         ' in line_text:\n",
    "                    EPOCH_lines.append(line_no) \n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### Identify and save the EPOCH start and end times\n",
    "        for i,val in enumerate(EPOCH_lines):\n",
    "            satpar_line = linecache.getline(iisset_file,val) # Check the above SATPAR line get the correct satellite ID (i.e. NOT GPS)\n",
    "\n",
    "            ##### only do this for the main satellite, so look for the correct SATID in the SATPAR card above EPOCH\n",
    "            if SAT_ID in satpar_line:\n",
    "                epoch_start = linecache.getline(iisset_file,val + 1)[20:40].strip() #181013210000.0000000\n",
    "                epoch_start_YYMMDD = linecache.getline(iisset_file,val + 1)[20:26].strip()       # 181013\n",
    "                epoch_start_HHMM = linecache.getline(iisset_file,val + 1)[26:30].strip()         # 2100\n",
    "                epoch_start_SS_SSSSSSS = linecache.getline(iisset_file,val + 1)[30:40].strip()   # 00.0000000     \n",
    "\n",
    "                epoch_end   = linecache.getline(iisset_file,val + 1)[60:80].strip() #1810160300 00.000\n",
    "                epoch_end_YYMMDD = linecache.getline(iisset_file,val + 1)[60:66].strip()       # 181016\n",
    "                epoch_end_HHMM = linecache.getline(iisset_file,val + 1)[66:70].strip()         # 210000.0000000\n",
    "                epoch_end_SS_SSSSSSS = linecache.getline(iisset_file,val + 1)[70:80].strip()   # 00.0000000     \n",
    "\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        #### Use pandas datetime and time delta to make adjustments to the dates on the ATGRAV and DRAG cards\n",
    "        #### --------------------------------------------------------------------\n",
    "        epoch_start_dt = pd.to_datetime( epoch_start_YYMMDD+epoch_start_HHMM, format='%y%m%d%H%M%S')\n",
    "        epoch_end_dt = pd.to_datetime( epoch_end_YYMMDD+epoch_end_HHMM, format='%y%m%d%H%M%S')\n",
    "\n",
    "        ##### ICESat2 has an orbit period of 94.22 minutes\n",
    "        #### Lets adjust the drag coefficient every 96 minutes\n",
    "        \n",
    "        \n",
    "\n",
    "        dt_2days = pd.Series(pd.to_timedelta(48,'h'))\n",
    "        dt_1days = pd.Series(pd.to_timedelta(24,'h'))\n",
    "        \n",
    "        dt_epoch_start_minus2days = (epoch_start_dt - dt_2days).dt.strftime('%y%m%d%H%M%S.0000000').values[0]\n",
    "        dt_epoch_end_plus1days    = (epoch_end_dt + dt_1days).dt.strftime('%y%m%d%H%M%S.000').values[0]\n",
    "        \n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        ###       FIND THE X,Y,Z,Xdot,Ydot,Zdot for this epoch start in the PCE data.\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "#         os.system('bunzip2'+' '+self.StateVector_epochs_datafile+'.bz2')\n",
    "        \n",
    "        epoch_start_dt_STR = str(epoch_start_dt)\n",
    "        date_in_file_flag = False\n",
    "        \n",
    "        print(\"Epoch Start: \", epoch_start_dt_STR)\n",
    "\n",
    "        with open(self.StateVector_epochs_datafile, 'r') as f:\n",
    "            for line_no, line_text in enumerate(f):\n",
    "                \n",
    "                if epoch_start_dt_STR in line_text:\n",
    "                    date_in_file_flag= True\n",
    "#                     print('    ','xyzline',line_no,line_text)\n",
    "\n",
    "                    break\n",
    "           \n",
    "        if date_in_file_flag == False:\n",
    "            change_elems_flag = False\n",
    "            print(epoch_start_dt_STR,'not found in file.  Leaving ELEMS as is.')\n",
    "#             print('Check that the start date:',epoch_start_dt_STR)\n",
    "#             print('    is within the PCE date range saved in the file')\n",
    "#             print('       ',self.StateVector_epochs_datafile)\n",
    "#                     os.system('bzip2'+' '+'/data/data_geodyn/inputs/icesat2/setups/StateVector_epochs.txt')\n",
    "#             sys.exit()\n",
    "\n",
    "        else:\n",
    "            change_elems_flag = True\n",
    "            xyzline = pd.read_csv(self.StateVector_epochs_datafile, \n",
    "                        skiprows = line_no, \n",
    "                        nrows=1,           \n",
    "                        sep = '\\s+',\n",
    "                        dtype=str,\n",
    "                        names = [\n",
    "                            'Date',\n",
    "                            'MJDSECs', \n",
    "                            'RSECS', #(fractional secs)\n",
    "                            'GPS offset', # (UTC - GPS offset (secs))\n",
    "                            'X',\n",
    "                            'Y',\n",
    "                            'Z',\n",
    "                            'X_dot',\n",
    "                            'Y_dot',\n",
    "                            'Z_dot',\n",
    "                            'YYMMDDhhmmss',\n",
    "                                ],)\n",
    "\n",
    "            X     =  xyzline['X'].values[0].ljust(20)     #'  -745933.8926940708'\n",
    "            Y     =  xyzline['Y'].values[0].ljust(20)     #'  -4864983.834066438'\n",
    "            Z     =  xyzline['Z'].values[0].ljust(20)     #'    4769954.60524261'\n",
    "            X_dot =  xyzline['X_dot'].values[0].ljust(20) #'  457.44564954037634'\n",
    "            Y_dot =  xyzline['Y_dot'].values[0].ljust(20) #'   5302.381564886811'\n",
    "            Z_dot =  xyzline['Z_dot'].values[0].ljust(20) #'    5463.55571622269'\n",
    "\n",
    "    #         os.system('bzip2'+' '+self.StateVector_epochs_datafile)\n",
    "        ##### -------------------------------------------------------------------------------------------\n",
    "        \n",
    "        ####   INPUT THE OPTIONS ON THE SPECIFIC CARDS YOU WANT TO CHANGE\n",
    "        ##### Putting in the options is one of the hardest parts of using GEODYN\n",
    "        #####    They require VERY specific inputs depending on the run type.  \n",
    "        card_strings = {}\n",
    "    \n",
    "            \n",
    "#                                  12345678901234567 \n",
    "        card_strings['ORBFIL'] =  'ORBFIL20131      '+SAT_ID+'     '+str(epoch_start)[:-6]+'  '+str(epoch_end)[:6]+' 24200.00          60'\n",
    "        card_strings['RESID']  =  'RESIDU12'\n",
    "        card_strings['OBSVU']  =  'OBSVU 2'  # print residuals on last iteration only\n",
    "        #       card_strings['PRNTVU'] =  'PRNTVU55212222    22122'  # original\n",
    "        card_strings['PRNTVU'] =  'PRNTVU5521111211 121122'  # suppress some IIS/IIE outputs.\n",
    "#                                  1234567890 \n",
    "        card_strings['ORBTVU'] =  'ORBTVU1201       '+SAT_ID+'     '+str(epoch_start)[:-6]+'  '+str(epoch_end)[:6]+' 24200.00 .100000D+01'\n",
    "        card_strings['ATMDEN'] =  'ATMDEN  '+ den_model_setupval\n",
    "        card_strings['ATGRAV']  =  'ATGRAV9090              '+dt_epoch_start_minus2days +''+dt_epoch_end_plus1days[:-1]   \n",
    "        card_strings['I64G2E']  =  'I64G2E         25'  # using 30 like in st-SLR run maxed out the memory usage\n",
    "        card_strings['SIGMA           1']  =  'SIGMA           1               1.0                 1.0'    \n",
    "        card_strings['SIGMA           2']  =  'SIGMA           2               1.0                 1.0'    \n",
    "        card_strings['SIGMA           3']  =  'SIGMA           3               1.0                 1.0'   \n",
    "        card_strings['SIGMA          51']  =  'SIGMA          51               10.0D+25             0.10'  \n",
    "        card_strings['SIGMA          85']  =  'SIGMA          85               0.010000            0.010000'  \n",
    "        \n",
    "        ##### MODIFY THE INTEGRATION STEP SIZE\n",
    "#                                 123456789012345678901234567890 \n",
    "        card_strings['STEP']  =  'STEP             '+SAT_ID+'           10.' # modify this from 10s -> 60s\n",
    "\n",
    "\n",
    "        ### Fix the coordinate system... PCE Data was in J2000\n",
    "#         card_strings['REFSYS1933 0        ']  = 'REFSYS193310        '+epoch_start+'0'\n",
    "#         card_strings['SATPAR   13']  =  'SATPAR   139     '+SAT_ID+'          9.53000000       1514.000'\n",
    "        card_strings['REFSYS']  = 'REFSYS193310        '+epoch_start+'0'\n",
    "        card_strings['EPOCH']   = 'EPOCH               '+epoch_start+epoch_start+epoch_end\n",
    "        card_strings['SATPAR']  =  'SATPAR   139     '+SAT_ID+'          9.53000000       1514.000'\n",
    "        \n",
    "        if change_elems_flag == True:\n",
    "            card_strings['ELEMS1']  = 'ELEMS11             '+X+''+Y+''+Z+''   \n",
    "            card_strings['ELEMS2']  = 'ELEMS2              '+X_dot+''+Y_dot+''+Z_dot+''\n",
    "                \n",
    "        #### Suppress the printing of the flux model\n",
    "        card_strings['FLUX  1']  =  'FLUX  0'\n",
    "\n",
    "    \n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ####    Search through the file to see if any of the cards we WANT are NOT in the file\n",
    "        #### --------------------------------------------------------------------\n",
    "        ##### read in all lines of the file and save them\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()                \n",
    "        ##### card flags to see if certain cards are present in the file\n",
    "        card_flag = {}\n",
    "        for card in card_strings:\n",
    "            ### Set the default flag to be False,  if the card is in the file, flip the flag to True\n",
    "            card_flag[card] = False\n",
    "            for line in lines_all:\n",
    "                if card in line:\n",
    "                    card_flag[card] = True\n",
    "\n",
    "        #### --------------------------------------------------------------------\n",
    "        ####    Edit the cards that exist in the file that we want to modify\n",
    "        #### --------------------------------------------------------------------\n",
    "        ###### Re-write the file line-by-line and EDIT the cards that need to be modified    \n",
    "        lines_replace = {}\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line_num, line in enumerate(lines):\n",
    "                for card in card_strings:\n",
    "                    if card in line:\n",
    "                        lines_replace[line_num] = card_strings[card]\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()\n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for line_num, line in enumerate(lines_all):\n",
    "                if line_num in lines_replace:\n",
    "#                     print('replacing line',lines_replace[line_num])\n",
    "                    f.write(lines_replace[line_num]+'\\n')\n",
    "                else:\n",
    "                     f.write(line)\n",
    "\n",
    "\n",
    "        #### for adding time dependent drag estimations.  We need to do a few things:\n",
    "        ###       Find the drag card that is already in the file:\n",
    "        ###       Add CONDRAG before all drag cards\n",
    "        ###       Add DRAG cards with TIME periods after the first drag card\n",
    "\n",
    "        add_mins_dt = pd.Series(pd.to_timedelta(9,'h'))\n",
    "#         add_mins_dt = pd.Series(pd.to_timedelta(94.22,'m'))\n",
    "#         add_mins_dt = pd.Series(pd.to_timedelta(4,'h'))\n",
    "    \n",
    "        drag_date_1 =  (epoch_start_dt+ add_mins_dt*1).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_2 =  (epoch_start_dt+ add_mins_dt*2).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_3 =  (epoch_start_dt+ add_mins_dt*3).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_4 =  (epoch_start_dt+ add_mins_dt*4).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_5 =  (epoch_start_dt+ add_mins_dt*5).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        drag_date_6 =  (epoch_start_dt+ add_mins_dt*6).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_7 =  (epoch_start_dt+ add_mins_dt*7).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_8 =  (epoch_start_dt+ add_mins_dt*8).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_9 =  (epoch_start_dt+ add_mins_dt*9).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_10 = (epoch_start_dt+add_mins_dt*10).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_11 = (epoch_start_dt+add_mins_dt*11).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_12 = (epoch_start_dt+add_mins_dt*12).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_13 = (epoch_start_dt+add_mins_dt*13).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_14 = (epoch_start_dt+add_mins_dt*14).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "#         drag_date_15 = (epoch_start_dt+add_mins_dt*15).dt.strftime('%y%m%d%H%M%S').values[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('epoch end   :',epoch_end[:-5])\n",
    "        print('epoch start :',epoch_start[:-5])\n",
    "                \n",
    "        card_drag_strings={}                                                                                                           #11307.\n",
    "        card_drag_strings['CONDRG']  =  'CONDRG  1        '+SAT_ID+'     '+str(epoch_start[:-5])+str(epoch_end[:-5])+'         0.50000  28800.'\n",
    "#         card_drag_strings['DRAG   0 0       '+SAT_ID+' 2.3000000000000E+00']  =  'DRAG   0 0       '+SAT_ID+' 2.3000000000000E+00'\n",
    "        card_drag_strings[drag_date_1]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_1[:10]+' 0.00    1.000000E+00'\n",
    "        card_drag_strings[drag_date_2]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_2[:10]+' 0.00    1.000000E+00'\n",
    "        card_drag_strings[drag_date_3]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_3[:10]+' 0.00    1.000000E+00'\n",
    "        card_drag_strings[drag_date_4]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_4[:10]+' 0.00    1.000000E+00'\n",
    "        card_drag_strings[drag_date_5]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_5[:10]+' 0.00    1.000000E+00'\n",
    "        card_drag_strings[drag_date_6]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_6[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_7]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_7[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_8]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_8[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_9]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_9[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_10]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_10[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_11]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_11[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_12]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_12[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_13]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_13[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_14]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_14[:10]+' 0.00    1.000000E+00'\n",
    "#         card_drag_strings[drag_date_15]  = 'DRAG             '+SAT_ID+' 2.2000000000000D+00'+drag_date_15[:10]+' 0.00    1.000000E+00'\n",
    "        \n",
    "        \n",
    "        print(drag_date_1)\n",
    "        print(drag_date_2)\n",
    "        print(drag_date_3)\n",
    "        print(drag_date_4)\n",
    "        print(drag_date_5)\n",
    "        print(drag_date_6)\n",
    "#         print(drag_date_7)\n",
    "#         print(drag_date_8)\n",
    "#         print(drag_date_9)\n",
    "#         print(drag_date_10)\n",
    "#         print(drag_date_11)\n",
    "#         print(drag_date_12)\n",
    "#         print(drag_date_13)\n",
    "#         print(drag_date_14)\n",
    "#         print(drag_date_15)\n",
    "        \n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()                \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for line in lines_all:\n",
    "                if 'DRAG   0 0       '+SAT_ID+' 2.3000000000000E+00' in line:  #this finds the DRAG line.  \n",
    "                    f.write(card_drag_strings['CONDRG'] + ' \\n')\n",
    "                    f.write('DRAG             '+SAT_ID+' 2.2000000000000E+00'+ ' \\n')\n",
    "                    f.write(card_drag_strings[drag_date_1] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_2] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_3] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_4] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_5] + ' \\n')                 \n",
    "                    f.write(card_drag_strings[drag_date_6] + ' \\n') \n",
    "#                     f.write(card_drag_strings[drag_date_7] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_8] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_9] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_10] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_11] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_12] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_13] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_14] + ' \\n')                 \n",
    "#                     f.write(card_drag_strings[drag_date_15] + ' \\n')                 \n",
    "\n",
    "                else:\n",
    "                    f.write(line)\n",
    "                    \n",
    "              \n",
    "        #####-----------------------------------------------------------------------------\n",
    "        #####   Delete the SATPAR GPS, EPOCH, ELEMS110, and ELEMS2 lines after the SATPAR GPS\n",
    "        #####   Do this by finding the SATPAR for our sat and then saving it and the next 3 lines\n",
    "        #####   then delete all the SATPAR,EPOCH,ELEMS110, ELEMS2 and restore the ones we saved\n",
    "        #####-----------------------------------------------------------------------------\n",
    "\n",
    "        if change_elems_flag == False:\n",
    "            ##### read in all lines of the file and save them\n",
    "            with open(iisset_file, \"r\") as f:\n",
    "                lines_all = f.readlines()    \n",
    "            \n",
    "            for iline, line in enumerate(lines_all):\n",
    "                if 'ELEMS1'in line:\n",
    "                    save_ELEMS1 = iline+1\n",
    "                elif 'ELEMS2' in line:\n",
    "                    save_ELEMS2 = iline+1\n",
    "                \n",
    "            line_ELEMS1 = linecache.getline(iisset_file, save_ELEMS1)\n",
    "            line_ELEMS2  = linecache.getline(iisset_file, save_ELEMS2)\n",
    "            \n",
    "            card_strings['ELEMS1']  = line_ELEMS1\n",
    "            card_strings['ELEMS2']  = line_ELEMS2\n",
    "            \n",
    "        ####----------------------------------------------------------------------\n",
    "        #### REMOVE CARDS:: rewrite the file without the cards we specified in the cards_to_remove dict\n",
    "        ####----------------------------------------------------------------------\n",
    "        ##### read in all lines of the file and save them\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()    \n",
    "        ##### Re-write the file line-by-line WITHOUT the cards that we want to remove    \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for iline, line in enumerate(lines_all):\n",
    "                if any(card in line for card in cards_to_remove):\n",
    "                    # IF the any of the cards in the list are in the line, dont add it\n",
    "                    pass\n",
    "                else:\n",
    "                    f.write(line)                \n",
    "        \n",
    "                        \n",
    "        \n",
    "        ####----------------------------------------------------\n",
    "        #### Add any cards that we want that are not in the file\n",
    "        ##### this INCLUDES our saved \n",
    "        #####      SATPAR, EPOCH, ELEMS1, ELEMS2\n",
    "        #####---------------------------------------------------------\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()                  \n",
    "\n",
    "        # use some switches to determine if things have already been written in the loop and avoid writing too many\n",
    "        switch_cardcount = 0\n",
    "        switch_2     = True\n",
    "        \n",
    "        for card in card_flag:\n",
    "            if card_flag[card] == False:\n",
    "                with open(iisset_file, \"w\") as f:\n",
    "                    for line in lines_all:\n",
    "                        if 'ALBEDO' in line:  #this overwrites the line after albedo. \n",
    "                            # MAYBE TODO:  this may not write multiple cards that aren't in the file\n",
    "                            if switch_cardcount == 0:\n",
    "                                f.write(line)\n",
    "                                f.write(card_strings[card] + ' \\n') \n",
    "                            else: \n",
    "                                f.write(card_strings[card] + ' \\n')\n",
    "                                switch_cardcount += 1\n",
    "                        else:\n",
    "                            f.write(line)\n",
    "                            \n",
    "        ##### Write our satellite parameters back in         \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for line in lines_all:\n",
    "                if (('REFSYS' in line) and (switch_2 == True)):\n",
    "                    f.write(card_strings['REFSYS']  + ' \\n')\n",
    "                    f.write(card_strings['SATPAR']  + ' \\n')\n",
    "                    f.write(card_strings['EPOCH']   + ' \\n')\n",
    "                    if change_elems_flag == True:\n",
    "                        f.write(card_strings['ELEMS1']  + ' \\n')\n",
    "                        f.write(card_strings['ELEMS2']  + ' \\n')\n",
    "                    elif change_elems_flag == False:\n",
    "                        f.write(card_strings['ELEMS1'])\n",
    "                        f.write(card_strings['ELEMS2'])\n",
    "\n",
    "                    switch_2 = False\n",
    "                else:\n",
    "                    f.write(line)\n",
    "\n",
    "        self.verboseprint('    ','PCE Update:')\n",
    "        self.verboseprint('    ','    ',card_strings['ELEMS1'])\n",
    "        self.verboseprint('    ','    ',card_strings['ELEMS2'])\n",
    "        \n",
    "        ####----------------------------------------------------------------------\n",
    "        #### Add three lines to the start of the file.  This is the GLOBAL TITLE\n",
    "        ####----------------------------------------------------------------------\n",
    "\n",
    "        with open(iisset_file, 'r+') as f:\n",
    "            content = f.read()\n",
    "            f.seek(0, 0)   # find the first lines\n",
    "            f.write('### \\n') \n",
    "            f.write('###   '+self.arc_name_id+'  \\n') \n",
    "            f.write('### \\n') \n",
    "            f.write(content) \n",
    "               \n",
    "        ####----------------------------------------------------------------------\n",
    "        #### Try doing a complete job of removing the GPS satellites.\n",
    "        ####----------------------------------------------------------------------  \n",
    "        delete_gps_sats = [ '5041144',\n",
    "                            '5044284',\n",
    "                            '5051204',\n",
    "                            '5154184',\n",
    "                            '5345214',\n",
    "                            '5347224',\n",
    "                            '5356164',\n",
    "                            '5459194',\n",
    "                            '5460234',\n",
    "                            '5461024',\n",
    "                            '5553175',\n",
    "                            '5652315',\n",
    "                            '5658125', \n",
    "                            '5755155',\n",
    "                            '5757295',\n",
    "                            '5848075',\n",
    "                            '5950055',\n",
    "                            '6062256',\n",
    "                            '6163016',\n",
    "                            '6265246',\n",
    "                            '6366276',\n",
    "                            '6464306',\n",
    "                            '6467066',\n",
    "                            '6468096',\n",
    "                            '6469036',\n",
    "                            '6571266',            \n",
    "                            '6572086',\n",
    "                            '6573106',\n",
    "                            '6649045',\n",
    "                            '6670326',\n",
    "                            '9743134',\n",
    "                            '9946114',        \n",
    "                            ]\n",
    "        ##### read in all lines of the file and save them\n",
    "        with open(iisset_file, \"r\") as f:\n",
    "            lines_all = f.readlines()    \n",
    "        ##### Re-write the file line-by-line WITHOUT the cards that we want to remove    \n",
    "        with open(iisset_file, \"w\") as f:\n",
    "            for iline, line in enumerate(lines_all):\n",
    "                if any(gps in line for gps in delete_gps_sats):\n",
    "                    # IF the any of GPS IDs in the list are in the line, dont add it the line\n",
    "                    pass\n",
    "                else:\n",
    "                    f.write(line)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bb8c8",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53c94e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:02.360922Z",
     "start_time": "2021-07-30T21:17:02.340809Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Identify which arcs you want to run:\n",
    "\n",
    "\n",
    "\n",
    "#------ A dictionary containing the run parameters ------  \n",
    "run_params = {} \n",
    "run_params['arc'] = ['2018.313',\n",
    "                         '2018.314',\n",
    "#                          '2018.315',\n",
    "#                          '2018.316',\n",
    "#                          '2018.317',\n",
    "#                          '2018.318',\n",
    "#                          '2018.319',\n",
    "#                          '2018.320',\n",
    "#                          '2018.321',\n",
    "#                          '2018.322',\n",
    "#                          '2018.323',\n",
    "#                          '2018.324',\n",
    "#                          '2018.325',\n",
    "#                          '2018.326',\n",
    "#                          '2018.327'\n",
    "                    ]\n",
    "run_params['satellite']        =  'icesat2'  \n",
    "run_params['SpecialRun_name']  =  '_TrajAnalysis_accel_comparison'  \n",
    "run_params['verbose']          =  False\n",
    "run_params['action']           =  'run'\n",
    "\n",
    "#### ---------------------------------------------\n",
    "\n",
    "##### Use copy.deepcopy to copy all levels of dictionary and \n",
    "###       allow modification of new variable\n",
    "# run_params1 = copy.deepcopy(run_params)\n",
    "# run_params1['den_model'] =  'msis2'  \n",
    "# run_params1['accels']  =  True\n",
    "\n",
    "\n",
    "\n",
    "for imodel,val_model in enumerate( ['jaachia71' ]): #'msis86' #'msis2' ,'msis00'\n",
    "    run_params1 = copy.deepcopy(run_params)\n",
    "    run_params1['den_model']  =  val_model  \n",
    "    run_params1['action']           =  'run'\n",
    "    run_params1['accels']  =  True\n",
    "\n",
    "    ### Load the data into an object\n",
    "#     Obj_Geodyn = edit_ICESat2_setup(run_params1)\n",
    "#     Obj_Geodyn.RUN_GEODYN()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c564c",
   "metadata": {},
   "source": [
    "## Read the Data back in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e1181",
   "metadata": {},
   "source": [
    "Read in data with accelerations on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed9e2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.292495Z",
     "start_time": "2021-07-30T21:17:02.362640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Did this run?\n",
      "                      ......... READING GEODYN output\n",
      "     Loading ... icesat2_2018313_54hr.msis2 \n",
      "     Loading ... icesat2_2018314_54hr.msis2 \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "Obj_accelon = {}\n",
    "from PYGEODYN import Pygeodyn\n",
    "for imodel,val_model in enumerate( ['msis2'  ,'msis00' ,'msis86', 'dtm87','jaachia71']):#'dtm87','jaachia71']):  \n",
    "\n",
    "    params_accelon = copy.deepcopy(run_params1)\n",
    "    params_accelon['action']  =  'read'  \n",
    "    params_accelon['den_model']  =  val_model\n",
    "    params_accelon['arc'] = ['2018.313',\n",
    "                             '2018.314' ]\n",
    "\n",
    "    params_accelon['accels']  =  True\n",
    "    params_accelon['SpecialRun_name']  =  '_TrajAnalysis_accel_comparison'  \n",
    "\n",
    "    params_accelon['request_data']      = ['AdjustedParams',\n",
    "                                        'Trajectory_orbfil',\n",
    "                                        'Density',\n",
    "                                        'Residuals_obs',\n",
    "                                        'Residuals_summary',\n",
    "                                       ]\n",
    "    ### Load the data into an object\n",
    "#     Obj_accelon = 0  # clear out the object variable if its got stuff in it.\n",
    "    Obj_accelon[val_model] = Pygeodyn(params_accelon)\n",
    "    Obj_accelon[val_model].getData()\n",
    "    \n",
    "        \n",
    "Obj_accelon_msis2     = Obj_accelon['msis2']\n",
    "Obj_accelon_msis00    = Obj_accelon['msis00']\n",
    "Obj_accelon_msis86    = Obj_accelon['msis86']\n",
    "Obj_accelon_dtm87     = Obj_accelon['dtm87']\n",
    "Obj_accelon_jaachia71 = Obj_accelon['jaachia71']\n",
    "\n",
    "del Obj_accelon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1a81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T14:33:58.494201Z",
     "start_time": "2021-07-14T14:33:58.489847Z"
    }
   },
   "source": [
    "Read in data with accelerations off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbcbad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.605710Z",
     "start_time": "2021-07-30T21:17:01.488Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "Obj_acceloff = {}\n",
    "from PYGEODYN import Pygeodyn\n",
    "for imodel,val_model in enumerate( ['msis2','msis00' ,'msis86','dtm87','jaachia71']):  # ,'dtm87','jaachia71'\n",
    "\n",
    "    params_acceloff = copy.deepcopy(run_params1)\n",
    "    params_acceloff['action']  =  'read'  \n",
    "    params_acceloff['den_model']  = val_model\n",
    "    params_acceloff['arc'] = ['2018.313',\n",
    "                             '2018.314' ]\n",
    "    params_acceloff['accels']  =  False\n",
    "    params_acceloff['SpecialRun_name']  =  '_TrajAnalysis' #'_TrajAnalysis_fixed_cd'  \n",
    "    params_acceloff['request_data']      = ['AdjustedParams',\n",
    "                                        'Trajectory_orbfil',\n",
    "                                        'Density',\n",
    "                                        'Residuals_obs',\n",
    "                                        'Residuals_summary',\n",
    "                                       ]\n",
    "    ### Load the data into an object\n",
    "#     Obj_acceloff = 0  # clear out the object variable if its got stuff in it.\n",
    "    Obj_acceloff[val_model] = Pygeodyn(params_acceloff)\n",
    "    Obj_acceloff[val_model].getData()\n",
    "    \n",
    "    \n",
    "Obj_acceloff_msis2     = Obj_acceloff['msis2']\n",
    "Obj_acceloff_msis00    = Obj_acceloff['msis00']\n",
    "Obj_acceloff_msis86    = Obj_acceloff['msis86']\n",
    "Obj_acceloff_dtm87     = Obj_acceloff['dtm87']\n",
    "Obj_acceloff_jaachia71 = Obj_acceloff['jaachia71']\n",
    "\n",
    "\n",
    "del Obj_acceloff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7b8be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.606560Z",
     "start_time": "2021-07-30T21:17:01.490Z"
    }
   },
   "outputs": [],
   "source": [
    "Obj_acceloff_msis2.__dict__['Density']['2018.313']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79203e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.607344Z",
     "start_time": "2021-07-30T21:17:01.492Z"
    }
   },
   "outputs": [],
   "source": [
    "# sys.path.insert(0,'/data/geodyn_proj/pygeodyn/utils_pygeodyn_develop/util_dir/')\n",
    "# from common_functions          import Pygeodyn_OBJECT_freeupmemory\n",
    "\n",
    "def Pygeodyn_OBJECT_freeupmemory(OBJ):\n",
    "    for i,val in enumerate(OBJ.__dict__['Density'].keys()):\n",
    "        \n",
    "        ####-----------------------------------------------------------------\n",
    "        #### DELETE UNNECESSARY VARS IN DENSITY\n",
    "        \n",
    "        del OBJ.__dict__['Density'][val]['Lat']\n",
    "        del OBJ.__dict__['Density'][val]['Lon']\n",
    "        del OBJ.__dict__['Density'][val]['Height (meters)']\n",
    "        del OBJ.__dict__['Density'][val]['drhodz (kg/m**3/m)']\n",
    "        \n",
    "        \n",
    "        ####-----------------------------------------------------------------\n",
    "        #### DELETE UNNECESSARY VARS IN Residuals_obs\n",
    "        \n",
    "#         del OBJ.__dict__['Residuals_obs'][val]['StatSatConfig']\n",
    "        del OBJ.__dict__['Residuals_obs'][val]['Sat_main']\n",
    "        del OBJ.__dict__['Residuals_obs'][val]['track_1']\n",
    "        del OBJ.__dict__['Residuals_obs'][val]['track_2']\n",
    "        del OBJ.__dict__['Residuals_obs'][val]['Note']\n",
    "        del OBJ.__dict__['Residuals_obs'][val]['Elev1']\n",
    "        del OBJ.__dict__['Residuals_obs'][val]['Elev2']\n",
    "        \n",
    "        \n",
    "        ####-----------------------------------------------------------------\n",
    "        #### DELETE UNNECESSARY VARIABLES IN AdjustedParams\n",
    "        \n",
    "        iterations = OBJ.__dict__['run_parameters'+val]['total_iterations']\n",
    "        for iters in np.arange(1, iterations):\n",
    "#             print(iters)\n",
    "            if iters == iterations:\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    del OBJ.__dict__['AdjustedParams'][val][iters]\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        ####-----------------------------------------------------------------\n",
    "        #### DELETE UNNECESSARY VARIABLES IN Trajectory_orbfil\n",
    "        \n",
    "        del OBJ.__dict__['Trajectory_orbfil'][val]['header']\n",
    "        del OBJ.__dict__['Trajectory_orbfil'][val]['data_record']['Satellite Geodetic Latitude']\n",
    "        del OBJ.__dict__['Trajectory_orbfil'][val]['data_record']['Satellite East Longitude']\n",
    "        del OBJ.__dict__['Trajectory_orbfil'][val]['data_record']['Satellite Height']\n",
    "        del OBJ.__dict__['Trajectory_orbfil'][val]['data_record']['MJDSEC ET']\n",
    "    return(OBJ)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "Obj_accelon_msis2  = Pygeodyn_OBJECT_freeupmemory(Obj_accelon_msis2)\n",
    "Obj_accelon_msis00 = Pygeodyn_OBJECT_freeupmemory(Obj_accelon_msis00)\n",
    "Obj_accelon_msis86 = Pygeodyn_OBJECT_freeupmemory(Obj_accelon_msis86)\n",
    "Obj_accelon_dtm87     = Pygeodyn_OBJECT_freeupmemory(Obj_accelon_dtm87)\n",
    "Obj_accelon_jaachia71 = Pygeodyn_OBJECT_freeupmemory(Obj_accelon_jaachia71)\n",
    "\n",
    "# ###\n",
    "\n",
    "Obj_acceloff_msis2  = Pygeodyn_OBJECT_freeupmemory(Obj_acceloff_msis2)\n",
    "Obj_acceloff_msis00 = Pygeodyn_OBJECT_freeupmemory(Obj_acceloff_msis00)\n",
    "Obj_acceloff_msis86 = Pygeodyn_OBJECT_freeupmemory(Obj_acceloff_msis86)\n",
    "Obj_acceloff_dtm87     = Pygeodyn_OBJECT_freeupmemory(Obj_acceloff_dtm87)\n",
    "Obj_acceloff_jaachia71 = Pygeodyn_OBJECT_freeupmemory(Obj_acceloff_jaachia71)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165bce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.608738Z",
     "start_time": "2021-07-30T21:17:01.494Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8bf91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.609480Z",
     "start_time": "2021-07-30T21:17:01.498Z"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import numpy as np\n",
    "\n",
    "# ga_9p_21 = []\n",
    "\n",
    "# x = list(Obj_Geodyn.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 21'].keys())\n",
    "\n",
    "# for i, val in enumerate(x):\n",
    "\n",
    "#     ga_9p_21.append(Obj_Geodyn.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 21'][val]['CURRENT_VALUE'])\n",
    "    \n",
    "    \n",
    "\n",
    "# y = ga_9p_21\n",
    "\n",
    "# fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers'))\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245b4fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T22:06:01.302473Z",
     "start_time": "2021-07-12T22:06:01.276138Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778ab63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.610213Z",
     "start_time": "2021-07-30T21:17:01.501Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "config = dict({\n",
    "                'displayModeBar': True,\n",
    "                'responsive': False,\n",
    "                'staticPlot': False,\n",
    "                'displaylogo': False,\n",
    "                'showTips': False,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97643ade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.610918Z",
     "start_time": "2021-07-30T21:17:01.503Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from PYGEODYNAnalysis_icesat2PCEtrajectory import plot_cd_and_percdiff_from_apriori\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=([\"Timeseries of Cd\", \"Ratio of Adjusted Cd to a priori (Cd=2.2)\"]),\n",
    "    vertical_spacing = 0.08,\n",
    "    )\n",
    "fig = plot_cd_and_percdiff_from_apriori(fig,  Obj_accelon_msis2      , 0)\n",
    "fig = plot_cd_and_percdiff_from_apriori(fig,  Obj_acceloff_msis2     , 1)\n",
    "fig.update_layout(title=\"How do the GA adjustements impact the Cd Adjustment?\")\n",
    "fig.update_layout(\n",
    "                autosize=False,\n",
    "                width=900,\n",
    "                height=900,\n",
    "                font=dict(size=14))\n",
    "\n",
    "fig.show(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecb340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.611646Z",
     "start_time": "2021-07-30T21:17:01.506Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PYGEODYNAnalysis_icesat2PCEtrajectory import plot_residual_meas_summary\n",
    "from PYGEODYNAnalysis_icesat2PCEtrajectory import rms_summary_table\n",
    "\n",
    "\n",
    "Obj_list = [Obj_accelon_msis2, Obj_acceloff_msis2]\n",
    "\n",
    "rms_summary_table(Obj_list)\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, \n",
    "     subplot_titles=([\"Mean Residuals per Arc\", 'RMS of Fit per Arc']),\n",
    "     vertical_spacing = 0.1)\n",
    "fig = plot_residual_meas_summary(fig, Obj_accelon_msis2   , 0)\n",
    "fig = plot_residual_meas_summary(fig, Obj_acceloff_msis2   , 1)\n",
    "\n",
    "fig.show(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f830a0",
   "metadata": {},
   "source": [
    "Assuming that the below has the form:\n",
    "\n",
    "$$accel_{\\text{track}}(x) = A_{track} sin(x) + B_{track} cos(x) $$\n",
    "\n",
    "then, $$ \\text{Amplitude}_{track} =\\sqrt{A_{track}^2 + B_{track}^2} $$\n",
    "$$\\phi_{track} = \\arctan{\\bigg(\\frac{A_{track}}{B_{track}}\\bigg)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f259690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T14:54:22.889141Z",
     "start_time": "2021-07-14T14:54:22.864853Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773a495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.612352Z",
     "start_time": "2021-07-30T21:17:01.509Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(Obj_accelon.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 11'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cd9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.613100Z",
     "start_time": "2021-07-30T21:17:01.511Z"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import numpy as np\n",
    "# from plotly.offline import plot, iplot\n",
    "# %matplotlib inline\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# fig = make_subplots(\n",
    "#     rows=2, cols=1,\n",
    "#     subplot_titles=(\"Along Track Accleration Amplitude\",\n",
    "#                     \"Cross Track Accleration Amplitude\"))\n",
    "\n",
    "# along_amp_list = []\n",
    "# cross_amp_list = []\n",
    "# along_phase_list = []\n",
    "# cross_phase_list = []\n",
    "\n",
    "# for i,val in enumerate(Obj_accelon.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 11'].keys()):\n",
    "#     alongtrack_sin_coeff = Obj_accelon.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 12'][val]['CURRENT_VALUE'] # ['0GA 9P 12'] \n",
    "#     crosstrack_sin_coeff = Obj_accelon.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 22'][val]['CURRENT_VALUE'] # ['0GA 9P 22'] \n",
    "#     alongtrack_cos_coeff = Obj_accelon.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 11'][val]['CURRENT_VALUE'] # ['0GA 9P 11']\n",
    "#     crosstrack_cos_coeff = Obj_accelon.__dict__['AdjustedParams']['2018.336'][9][1807001]['0GA 9P 21'][val]['CURRENT_VALUE'] # ['0GA 9P 21']\n",
    "    \n",
    "#     alongtrack_amp = np.sqrt( np.square(alongtrack_sin_coeff) + np.square(alongtrack_cos_coeff) )\n",
    "#     crosstrack_amp = np.sqrt( np.square(crosstrack_sin_coeff) + np.square(crosstrack_cos_coeff) )\n",
    "\n",
    "#     alongtrack_phase = np.arctan(alongtrack_cos_coeff / alongtrack_sin_coeff )\n",
    "#     crosstrack_phase = np.arctan(crosstrack_cos_coeff / crosstrack_sin_coeff )\n",
    "\n",
    "#     along_amp_list.append(alongtrack_amp)\n",
    "#     cross_amp_list.append(crosstrack_amp)\n",
    "#     along_phase_list.append(alongtrack_phase)\n",
    "#     cross_phase_list.append(crosstrack_phase)\n",
    "\n",
    "# dates = list(Obj_accelon.__dict__['AdjustedParams'][arc1][9][1807001]['0GA 9P 11'].keys())\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=dates,\n",
    "#                         y=along_amp_list,\n",
    "# #                         name= 'MSIS 86',\n",
    "#                         mode='markers',\n",
    "#                         marker=dict(\n",
    "#                         size=10,),\n",
    "#                         ), row=1, col=1,\n",
    "#                         )\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=dates,\n",
    "#                         y=cross_amp_list,\n",
    "# #                         name= 'MSIS 86',\n",
    "#                         mode='markers',\n",
    "#                         marker=dict(\n",
    "#                         size=10,),\n",
    "#                         ), row=2, col=1,\n",
    "#                         )\n",
    "\n",
    "# fig.update_yaxes( title=r\"$\\frac{m}{s^2}$\",exponentformat= 'power',row=1, col=1)\n",
    "# fig.update_yaxes( title=r\"$\\frac{m}{s^2}$\",exponentformat= 'power',row=2, col=1)\n",
    "# fig.update_xaxes( title=\"Date\", row=2, col=1)\n",
    "\n",
    "\n",
    "\n",
    "# overlap_start = Obj_accelon.__dict__['Residuals_obs'][arc1]['Date'].iloc[-1]\n",
    "# overlap_end   = Obj_accelon.__dict__['Trajectory_orbfil'][arc1]['data_record']['Date_UTC'].iloc[-1]\n",
    "# fig.add_vrect(  x0=overlap_start, x1=overlap_end,\n",
    "#                 fillcolor='LightSkyBlue', opacity=0.2,\n",
    "#                 layer=\"below\", line_width=0)\n",
    "# fig.update_layout(title=\"Single Arc POD + Prediction\")\n",
    "\n",
    "# iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd240a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.613781Z",
     "start_time": "2021-07-30T21:17:01.513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obj_accelon.__dict__['run_parameters2018.336']['total_iterations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5998311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.614567Z",
     "start_time": "2021-07-30T21:17:01.516Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from plotly.offline import plot, iplot\n",
    "%matplotlib inline\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#### ----------------------------------------\n",
    "#### Plotting modules:\n",
    "#### -----------------\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "#### ----------------------------------------\n",
    "#### ----------------------------------------\n",
    "#### ----------------------------------------\n",
    "\n",
    "# Simplify Plotting Schemes:\n",
    "col1 = px.colors.qualitative.Plotly[0]\n",
    "col2 = px.colors.qualitative.Plotly[1]\n",
    "col3 = px.colors.qualitative.Plotly[2]\n",
    "col4 = px.colors.qualitative.Plotly[3]\n",
    "col5 = px.colors.qualitative.Plotly[4]\n",
    "\n",
    "\n",
    "\n",
    "def plot_gen_accels(fig, obj_m1, plot_num):\n",
    "    # fig = make_subplots(\n",
    "    #     rows=2, cols=1,\n",
    "    #     subplot_titles=(\"Along Track Accleration Amplitude\",\n",
    "    #                     \"Cross Track Accleration Amplitude\"))\n",
    "\n",
    "    along_amp_list = []\n",
    "    cross_amp_list = []\n",
    "    along_phase_list = []\n",
    "    cross_phase_list = []\n",
    "    \n",
    "    if plot_num == 0:\n",
    "        col = col1\n",
    "        x_annot = 1.09\n",
    "        y_annot = .95\n",
    "        m_size = 4\n",
    "    elif plot_num == 1:\n",
    "        x_annot = 1.09\n",
    "        y_annot = .85\n",
    "        col = col2\n",
    "        m_size = 3.5\n",
    "    elif plot_num == 2:\n",
    "        x_annot = 1.09\n",
    "        y_annot = .75\n",
    "        col = col3\n",
    "        m_size = 2.5\n",
    "    elif plot_num == 3:\n",
    "        x_annot = 1.09\n",
    "        y_annot = .56\n",
    "        col = col4\n",
    "        m_size = 1.5\n",
    "    elif plot_num == 4:\n",
    "        x_annot = 1.09\n",
    "        y_annot = .45\n",
    "        col = col5\n",
    "        m_size = 1\n",
    "    for iarc, arcval in enumerate(obj_m1.__dict__['global_params']['arc_input']):\n",
    "        final_iter = obj_m1.__dict__['run_parameters'+arcval]['total_iterations']\n",
    "        \n",
    "        for i,val in enumerate(obj_m1.__dict__['AdjustedParams'][arcval][final_iter][1807001]['0GA 9P 11'].keys()):\n",
    "            alongtrack_sin_coeff = obj_m1.__dict__['AdjustedParams'][arcval][final_iter][1807001]['0GA 9P 12'][val]['CURRENT_VALUE'] # ['0GA 9P 12'] \n",
    "            crosstrack_sin_coeff = obj_m1.__dict__['AdjustedParams'][arcval][final_iter][1807001]['0GA 9P 22'][val]['CURRENT_VALUE'] # ['0GA 9P 22'] \n",
    "            alongtrack_cos_coeff = obj_m1.__dict__['AdjustedParams'][arcval][final_iter][1807001]['0GA 9P 11'][val]['CURRENT_VALUE'] # ['0GA 9P 11']\n",
    "            crosstrack_cos_coeff = obj_m1.__dict__['AdjustedParams'][arcval][final_iter][1807001]['0GA 9P 21'][val]['CURRENT_VALUE'] # ['0GA 9P 21']\n",
    "\n",
    "            alongtrack_amp = np.sqrt( np.square(alongtrack_sin_coeff) + np.square(alongtrack_cos_coeff) )\n",
    "            crosstrack_amp = np.sqrt( np.square(crosstrack_sin_coeff) + np.square(crosstrack_cos_coeff) )\n",
    "\n",
    "            alongtrack_phase = np.arctan(alongtrack_cos_coeff / alongtrack_sin_coeff )\n",
    "            crosstrack_phase = np.arctan(crosstrack_cos_coeff / crosstrack_sin_coeff )\n",
    "\n",
    "            along_amp_list.append(alongtrack_amp)\n",
    "            cross_amp_list.append(crosstrack_amp)\n",
    "            along_phase_list.append(alongtrack_phase)\n",
    "            cross_phase_list.append(crosstrack_phase)\n",
    "\n",
    "        dates = list(obj_m1.__dict__['AdjustedParams'][arcval][final_iter][1807001]['0GA 9P 11'].keys())\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=dates,\n",
    "                                y=along_amp_list,\n",
    "                                name= obj_m1.__dict__['global_params']['den_model'],\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=col,\n",
    "                                size=5,),\n",
    "                                ), row=1, col=1,\n",
    "                                )\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=dates,\n",
    "                                y=cross_amp_list,\n",
    "        #                         name= 'MSIS 86',\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=col,\n",
    "                                size=5,),\n",
    "                                showlegend=False,\n",
    "                                ), row=2, col=1,\n",
    "                                )\n",
    "\n",
    "        fig.update_yaxes( title=r\"$\\frac{m}{s^2}$\",exponentformat= 'power',row=1, col=1)\n",
    "        fig.update_yaxes( title=r\"$\\frac{m}{s^2}$\",exponentformat= 'power',row=2, col=1)\n",
    "        fig.update_xaxes( title=\"Date\", row=2, col=1)\n",
    "\n",
    "\n",
    "\n",
    "        overlap_start = obj_m1.__dict__['Residuals_obs'][arcval]['Date'].iloc[-1]\n",
    "        overlap_end   = obj_m1.__dict__['Trajectory_orbfil'][arcval]['data_record']['Date_UTC'].iloc[-1]\n",
    "        fig.add_vrect(  x0=overlap_start, x1=overlap_end,\n",
    "                        fillcolor='LightSkyBlue', opacity=0.2,\n",
    "                        layer=\"below\", line_width=0)\n",
    "    fig.update_layout(title=\"Single Arc POD + Prediction\")\n",
    "    return(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10973019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T15:55:43.923015Z",
     "start_time": "2021-07-19T15:55:43.901309Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61490db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.615287Z",
     "start_time": "2021-07-30T21:17:01.520Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\"Along Track Accleration Amplitude\",\n",
    "                    \"Cross Track Accleration Amplitude\"))\n",
    "fig = plot_gen_accels(fig, Obj_accelon_msis2, 0)\n",
    "fig = plot_gen_accels(fig, Obj_accelon_msis00, 1)\n",
    "fig = plot_gen_accels(fig, Obj_accelon_msis86, 2)\n",
    "# fig = plot_gen_accels(fig, Obj_accelon_msis, 2)\n",
    "# fig = plot_gen_accels(fig, Obj_accelon_msis86, 2)\n",
    "fig.show(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd911c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157db750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6add4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.616011Z",
     "start_time": "2021-07-30T21:17:01.523Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from PYGEODYNAnalysis_icesat2PCEtrajectory import plot_ScaleDensity_with_CdScaleFactor__2\n",
    "\n",
    "# fig = make_subplots(rows=2, cols=1,\n",
    "#                 subplot_titles=([\"Model Ouptut Density\", \"Model Density * Cd Scaling Factor\"]),\n",
    "#                 shared_yaxes=True,\n",
    "#                 vertical_spacing = 0.1,\n",
    "#                 specs=[\n",
    "#                 [{\"secondary_y\": False}],\n",
    "#                 [{\"secondary_y\": False}], ])\n",
    "\n",
    "\n",
    "# fig = plot_ScaleDensity_with_CdScaleFactor__2(fig,  Obj_Geodyn     , 0, 5)\n",
    "# # fig = plot_ScaleDensity_with_CdScaleFactor__2(fig,  Obj_Geodyn_msis00     , 1, 200)\n",
    "# # fig = plot_ScaleDensity_with_CdScaleFactor__2(fig,  Obj_Geodyn_msis2      , 2, 200)\n",
    "# # fig = plot_ScaleDensity_with_CdScaleFactor__2(fig,  Obj_Geodyn_dtm87      , 3, 200)\n",
    "# # fig = plot_ScaleDensity_with_CdScaleFactor__2(fig,  Obj_Geodyn_jaachia71  , 4, 200)\n",
    "\n",
    "# # min_y = 1*1e-16\n",
    "# # max_y = 9*1e-12\n",
    "# # fig.update_yaxes(range=[min_y, max_y], row=1, col=1)\n",
    "# # fig.update_yaxes(range=[min_y, max_y], row=2, col=1)\n",
    "\n",
    "# fig.show(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d0017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T21:17:51.616785Z",
     "start_time": "2021-07-30T21:17:01.525Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from PYGEODYNAnalysis_icesat2PCEtrajectory import NTW_CDratio_IntrackResids\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            subplot_titles=(['Adjusted Cd ratio to a priori (2.2)', 'In-Track Component Residuals (PCE-ORBFIL)']),\n",
    "            vertical_spacing = 0.1,\n",
    "            specs=[ [{\"secondary_y\": False }],\n",
    "                    [{\"secondary_y\": False }]],)\n",
    "\n",
    "\n",
    "fig = NTW_CDratio_IntrackResids(fig,  Obj_Geodyn    , 0)\n",
    "# fig = NTW_CDratio_IntrackResids(fig,  Obj_Geodyn_msis00     , 1)\n",
    "# fig = NTW_CDratio_IntrackResids(fig,  Obj_Geodyn_msis2      , 2)\n",
    "# fig = NTW_CDratio_IntrackResids(fig,  Obj_Geodyn_dtm87      , 3)\n",
    "# fig = NTW_CDratio_IntrackResids(fig,  Obj_Geodyn_jaachia71  , 4)\n",
    "\n",
    "\n",
    "fig.show(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fb732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T21:39:53.813834Z",
     "start_time": "2021-07-12T21:39:53.788159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac32f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T21:39:53.839009Z",
     "start_time": "2021-07-12T21:39:53.815459Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac7b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T21:39:53.862001Z",
     "start_time": "2021-07-12T21:39:53.840880Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
