{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-extension",
   "metadata": {},
   "source": [
    "# Reading .RVG binary files.\n",
    "Hello and welcome to my attempt to read some \"simple\" binary files. \n",
    "\n",
    "## The goal here:\n",
    "\n",
    "I need to perform some pre-processing on a set of binary .rvg files (containing trajectory output from a reduced dynamics run of GEODYN) and stitch them together to construct a GEODYN-specific, trajectory-based, tracking data type (called PCE).\n",
    "\n",
    "Each binary (.rvg) file contains a **54-hour arc of ICESat2 trajectory data**.  These datasets are the output from a very precise run of GEODYN in which the orbit has been determined very well (*to a few centimeters accuracy*) using a **reduced dynamics technique** (*empirical accelerations and other parameters are adjusted to account for any mismodelled forces)*.\n",
    "\n",
    "\n",
    "### Methodology for pre-processing the binary files: \n",
    "\n",
    "  1. dump the data from each arc into some usable format\n",
    "  2. chop of the 3 hour padding on the ends to eliminate discontinuities from end effects\n",
    "  3. stitch together all the files (i provide some for this notebook as examples)\n",
    "  4. smooth over any existing disconiuties between arc gaps or maneuvers.\n",
    "  5. Put all data into a single `TRAJ.txt` file to be converted to a GEODYN-specific tracking datatype.\n",
    "\n",
    "### Info on the binary files:\n",
    "(from the GEODYN folks at Goddard)\n",
    "1. These files are in what is called the **RVG format**. The RVG files are pretty simple to unpack (lol)\n",
    "2. Each **record has 29 words**\n",
    "3. Each **word is a 64 bit floating point number**\n",
    "4. The first record is a *header record* with information about the file.\n",
    "\n",
    "    ```\n",
    "    #|   Header Record Format:\n",
    "    #|   ---------------------\n",
    "    #|   \n",
    "    #|   WORD   | Type | Description\n",
    "    #|   ----     ----   -----------\n",
    "    #|   1         DP     Coord. Sys. Flag\n",
    "    #|                        0 = TOD\n",
    "    #|                        1 = TOR\n",
    "    #|                        2 = J2000\n",
    "    #|   2         DP     Traj start date MJDSEC GPS \n",
    "    #|   3         DP     Traj start frac sec \n",
    "    #|   4         DP     Traj start date (YYMMDDHHMMSS) UTC \n",
    "    #|   5         DP     Traj stop date MJDSEC GPS \n",
    "    #|   6         DP     Traj stop frac sec \n",
    "    #|   7         DP     Traj stop date (YYMMDDHHMMSS) UTC \n",
    "    #|   8         DP     Traj interval sec \n",
    "    #|   9         DP     GEODYN 2s version no. \n",
    "    #|   10        DP     GEODYN 2s run date \n",
    "    #|   11        DP     GEODYN 2s run time \n",
    "    #|   12        DP     GEODYN 2e version no.w \n",
    "    #|   13        DP     GEODYN 2e run date \n",
    "    #|   14        DP     GEODYN 2e run time \n",
    "    #|   15        DP     Speed of light \n",
    "    #|   16        DP     GM for Earth \n",
    "    #|   17        DP     Semi-major axis of Earth ref. ellipsoid \n",
    "    #|   18        DP     Equatorial Flattening of Earth ref. ellipsoid \n",
    "    #|   19        DP     Gravitational Potential Checksum \n",
    "    #|   20        DP     Maximum Degree of Gravitational Expansion \n",
    "    #|   21        DP     Maximum Order Of Gravitational Expansion \n",
    "    #|   22-29     DP       spares\n",
    "    ```\n",
    "5.  The last record is a *sentinal record* to tell you that you have reached the end of the file. \n",
    "    ```\n",
    "    #|   Sentinel Record Format:\n",
    "    #|   ---------------------\n",
    "    #|   \n",
    "    #|   WORD | Type | Description\n",
    "    #|   ----   ----   -----------\n",
    "    #|   1       DP     999.0\n",
    "    #|   2       DP     Satellite ID \n",
    "    #|   3       DP     GEODYN IIS Versions\n",
    "    #|   4       DP     GEODYN IIE Versions \n",
    "    #|   5-29    DP     0.0 \n",
    "    ```\n",
    "  - The first word of that record has the value 999.0.  In other words, when you encounter a record whose first word has the value 999.0, you know you have reached the end of the file.\n",
    "\n",
    "6. All the records in the file except the first and last records, are data records.\n",
    "```\n",
    "#|   Data Record Format:\n",
    "#|   ---------------------\n",
    "#|   \n",
    "#|   WORD   | Type | Description\n",
    "#|   ----     ----   -----------\n",
    "#|   1         DP     MJDSEC (secs)  % time is in GPS \n",
    "#|   2         DP     RSEC (fractional secs) \n",
    "#|   3         DP     UTC - GPS offset (secs) \n",
    "#|   4         DP     spare_4 \n",
    "#|   5         DP     X Inertial sat. S.Vec (m) \n",
    "#|   6         DP     Y Inertial sat. S.Vec (m) \n",
    "#|   7         DP     Z Inertial sat. S.Vec (m) \n",
    "#|   8         DP     X_dot Inertial sat. S.Vec (m/sec) \n",
    "#|   9         DP     Y_dot Inertial sat. S.Vec (m/sec) \n",
    "#|   10        DP     Z_dot Inertial sat. S.Vec (m/sec) \n",
    "#|   11        DP     Satellite latitude (degrees) \n",
    "#|   12        DP     Satellite longitude (degrees) \n",
    "#|   13        DP     Satellite height (m) \n",
    "#|   14        DP     X-component ECF position (m) \n",
    "#|   15        DP     Y-component ECF position (m) \n",
    "#|   16        DP     Z-component ECF position (m) \n",
    "#|   17        DP     X_dot-component ECF velocity (m/sec) \n",
    "#|   18        DP     Y_dot-component ECF velocity (m/sec) \n",
    "#|   19        DP     Z_dot-component ECF velocity (m/sec) \n",
    "#|   20        DP     X component of polar motion (milliarcsec) \n",
    "#|   21        DP     Y component of polar motion (milliarcsec) \n",
    "#|   22        DP     beta angle (degrees) \n",
    "#|   23        DP     yaw angle (degrees) \n",
    "#|   24        DP     orbit angle (degrees) \n",
    "#|   25        DP     Quaternion QI for J2000 to ITRF (ECF) \n",
    "#|   26        DP     Quaternion 02 for J2000 to ITRF (ECF) \n",
    "#|   27        DP     Quaternion 03 for J2000 to ITRF (ECF) \n",
    "#|   28        DP     Quaternion 04 for J2000 to ITRF (ECF) \n",
    "#|   29        DP     Greenwich HR angle \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-ancient",
   "metadata": {},
   "source": [
    "## 1) Dump the data into usable format with `scipy.io.FortranFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solved-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# from read_rvg_binary import read_rvg_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "derived-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_binaryrvg = '/data/data_geodyn/inputs/icesat2/pre_processing/traj_files_rvg/'\n",
    "\n",
    "# rvg_data1 = read_rvg_binary(29, path_to_binaryrvg + 'orbit.1807001.2018.287')\n",
    "# rvg_data2 = read_rvg_binary(29, path_to_binaryrvg + 'orbit.1807001.2018.288')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-shuttle",
   "metadata": {},
   "source": [
    "## 1.a Look at the data for a few files to see the discontinuities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-breach",
   "metadata": {},
   "source": [
    "**Convert MJDS to YYMMDDHHMMSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chemical-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from ModJulianDaySeconds__to__YYMMDDHHMMSS import MJDS_to_YYMMDDHHMMSS\n",
    "\n",
    "\n",
    "# #### TEST to see if the time conversion works\n",
    "# # YYMMDDHHMMSS = MJDS_to_YYMMDDHHMMSS(start_date_MJDSEC_GPS)\n",
    "# # print(pd.to_datetime( YYMMDDHHMMSS, format='%y%m%d-%H%M%S'))\n",
    "# # print(pd.to_datetime( str(int(start_date_YYMMDDHHMMSS_UTC)), format='1%y%m%d%H%M%S'))\n",
    "\n",
    "# def RVG_Files_add_datetime_column(rvg_file):\n",
    "    \n",
    "#     # data\n",
    "#     mjdsecs = rvg_file['data']['MJDSEC_secs_timeGPS']\n",
    "\n",
    "#     #convert the MJDS to a usable string.\n",
    "#     yymmdd_str = [MJDS_to_YYMMDDHHMMSS(x) for x in mjdsecs]\n",
    "\n",
    "#     # convert string of dates to datetime for plotting\n",
    "#     dates      = [pd.to_datetime( x, format='%y%m%d-%H%M%S') for x in yymmdd_str]\n",
    "\n",
    "#     return(dates)\n",
    "\n",
    "\n",
    "# dates1 = RVG_Files_add_datetime_column(rvg_data1)\n",
    "# dates2 = RVG_Files_add_datetime_column(rvg_data2)\n",
    "\n",
    "# #add the datetime string the the Dataframe within the dictionary\n",
    "# rvg_data1['data'].insert(0, 'Date', dates1)\n",
    "# rvg_data2['data'].insert(0, 'Date', dates2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-disclaimer",
   "metadata": {},
   "source": [
    "**We can see below that each file is 29.40027 hours long** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floral-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starttime = rvg_data1['data']['Date'].iloc[0] \n",
    "# endtime = rvg_data1['data']['Date'].iloc[-1]\n",
    "# print('Start: ',starttime)\n",
    "# print('end  : ',endtime)\n",
    "\n",
    "# time_diff_days = pd.Timedelta(endtime - starttime).days * 24\n",
    "# time_diff_hours = pd.Timedelta(endtime - starttime).seconds / 3600.0\n",
    "# time_diff_totalhours = time_diff_days + time_diff_hours\n",
    "\n",
    "# print('Each file contains' ,time_diff_totalhours,'hours worth of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-qualification",
   "metadata": {},
   "source": [
    "**Find the overlap between two files:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variable-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# Range = namedtuple('Range', ['start', 'end'])\n",
    "\n",
    "# def time_overlap(file1, file2, verbose=False):\n",
    "#     data1 = file1['data']['Date']\n",
    "#     data2 = file2['data']['Date']\n",
    "#     r1 = Range(start=data1.iloc[0], end=data1.iloc[-1])\n",
    "#     r2 = Range(start=data2.iloc[0], end=data2.iloc[-1])\n",
    "#     latest_start = max(r1.start, r2.start)\n",
    "#     earliest_end = min(r1.end, r2.end)\n",
    "#     delta = (earliest_end - latest_start).total_seconds()/3600\n",
    "#     overlap = (delta)\n",
    "    \n",
    "#     if verbose:\n",
    "#         print('Latest_start',latest_start)\n",
    "#         print('Earliest_end',earliest_end)\n",
    "#         print('Overlap of ',overlap, 'hours')\n",
    "\n",
    "#     return latest_start, earliest_end, overlap\n",
    "\n",
    "# (start_overlap, end_overlap, tot_overlap) = time_overlap(rvg_data1, rvg_data2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smart-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.express as px\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=1, \n",
    "#                    )\n",
    "# string = 'X_statevector_m'\n",
    "# fig.add_trace(go.Scattergl(x=rvg_data1['data']['Date'].iloc[-20000:],\n",
    "#                            y=rvg_data1['data'][string].iloc[-20000:]*1e2,\n",
    "#                            name= \"set 1\",\n",
    "#                            mode='markers',\n",
    "#                            marker=dict(\n",
    "#                            size=6,),\n",
    "#                            ),\n",
    "#                             row=1, col=1,\n",
    "#                            )\n",
    "# fig.add_trace(go.Scattergl(x=rvg_data2['data']['Date'].iloc[:20000],\n",
    "#                            y=rvg_data2['data'][string].iloc[:20000]*1e2,\n",
    "#                            name= \"set 2\",\n",
    "#                            mode='markers',\n",
    "#                            marker=dict(\n",
    "#                            size=3,),\n",
    "#                            ),\n",
    "#                             row=1, col=1,\n",
    "#                            )\n",
    "\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"rvg files 1 and 2 overlap \",\n",
    "#     yaxis_title=\"state vector C [cm]\",\n",
    "#     xaxis_title=\"Date\",\n",
    "#     )\n",
    "\n",
    "# fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "# fig.update_xaxes(tickangle=30)\n",
    "# fig.update_xaxes( exponentformat= 'power')\n",
    "# fig.update_yaxes( exponentformat= 'power')\n",
    "\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-width",
   "metadata": {},
   "source": [
    "**We want to cut off the same amount of time from each file such that they line up perfectly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intended-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Cut same amount off each end of file.  So, ',tot_overlap/2, 'hours from each end of file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polish-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rvg_data1['data']['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nearby-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('Last time', rvg_data1['data']['Date'].iloc[-1] )\n",
    "\n",
    "# rvg_data1['data']['Date'].iloc[-1] - pd.Timedelta(tot_overlap/2, unit='hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "compressed-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def RVGfiles_timeoverlap_GetChoppingTime(file1, file2):\n",
    "    \n",
    "#     (_, _, tot_overlap) = time_overlap(file1, file2)\n",
    "    \n",
    "#     file1_start = file1['data']['Date'].iloc[0] \n",
    "#     file1_end = file1['data']['Date'].iloc[-1] \n",
    "\n",
    "#     file2_start =  file2['data']['Date'].iloc[0]\n",
    "#     file2_end =  file2['data']['Date'].iloc[-1]\n",
    "\n",
    "#     file1_new_start = file1_start + pd.Timedelta(tot_overlap/2, unit='hours')\n",
    "#     file1_new_end = file1_end - pd.Timedelta(tot_overlap/2, unit='hours')\n",
    "\n",
    "#     file2_new_start = file2_start + pd.Timedelta(tot_overlap/2, unit='hours')\n",
    "#     file2_new_end = file2_end - pd.Timedelta(tot_overlap/2, unit='hours')\n",
    "\n",
    "# #     print('file1_last:  ',file1_last)\n",
    "# #     print('file2_start: ',file2_start)\n",
    "    \n",
    "# #     print('file1_new_last:  ',file1_new_last)\n",
    "# #     print('file2_new_start: ',file2_new_start)\n",
    "\n",
    "#     return(file1_new_start, file1_new_end, file2_new_start, file2_new_end)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "activated-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1_new_start, file1_new_end, file2_new_start, file2_new_end = RVGfiles_timeoverlap_GetChoppingTime(rvg_data1, rvg_data2)\n",
    "# print('file1_new_start', file1_new_start)\n",
    "# print('file1_new_end', file1_new_end)\n",
    "# print('file2_new_start', file2_new_start)\n",
    "# print('file2_new_end', file2_new_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "focal-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def RVGfiles_chop_the_ends(file1, file2):\n",
    "#     '''\n",
    "#     Chop the ends off the file.\n",
    "#     '''\n",
    "    \n",
    "#     file1_new_start, file1_new_end, file2_new_start, file2_new_end = RVGfiles_timeoverlap_GetChoppingTime(rvg_data1, rvg_data2)\n",
    "\n",
    "    \n",
    "#     df1 = file1['data']\n",
    "#     df2 = file2['data']\n",
    "\n",
    "#     ##### Chop the FRONT off of the FIRST file\n",
    "#     # select all the values greater than our new start and grab the last one \n",
    "#     val1_front = df1.Date[df1.Date < file1_new_start].iloc[-1]\n",
    "#     indx1_front = df1.Date[df1.Date==val1_front].index.unique()[0]\n",
    "\n",
    "#     ##### Chop the END off of the FIRST file\n",
    "#     # select all the values less than our new start and grab the first one \n",
    "#     val1_end = df1.Date[df1.Date > file1_new_end].iloc[1]\n",
    "#     indx1_end = df1.Date[df1.Date==val1_end].index.unique()[0]\n",
    "\n",
    "\n",
    "    \n",
    "#     ##### Chop the FRONT off of the SECOND file\n",
    "#     # select all the values greater than our new start and grab the last one \n",
    "#     val2_front = df2.Date[df2.Date < file2_new_start].iloc[-1]\n",
    "#     indx2_front = df2.Date[df2.Date==val2_front].index.unique()[0]\n",
    "\n",
    "#     ##### Chop the END off of the SECOND file\n",
    "#     # select all the values less than our new start and grab the first one \n",
    "#     val2_end = df2.Date[df2.Date > file2_new_end].iloc[1]\n",
    "#     indx2_end = df2.Date[df2.Date==val2_end].index.unique()[0]\n",
    "\n",
    "# #     print('indx1_front',indx1_front)\n",
    "# #     print('indx1_end',indx1_end)\n",
    "# #     print('indx2_front',indx2_front)\n",
    "# #     print('indx2_end',indx2_end)\n",
    "\n",
    "#     df1_new = df1[:indx1_end][indx1_front+1:] # add one index so there is no overlap in time\n",
    "#     df2_new = df2[:indx2_end][indx2_front+1:] # add one index so there is no overlap in time\n",
    "    \n",
    "#     return(df1_new, df2_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "placed-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df1_new, df2_new) = RVGfiles_chop_the_ends(rvg_data1, rvg_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-notion",
   "metadata": {},
   "source": [
    "**Plot again to see how we did**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moved-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.express as px\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=1, \n",
    "#                    )\n",
    "# string = 'X_statevector_m'\n",
    "# fig.add_trace(go.Scattergl(x=df1_new['Date'],\n",
    "#                            y=df1_new[string]*1e2,\n",
    "#                            name= \"adjusted set 1\",\n",
    "#                            mode='markers',\n",
    "#                            marker=dict(\n",
    "#                            size=6,),\n",
    "#                            ),\n",
    "#                             row=1, col=1,\n",
    "#                            )\n",
    "# fig.add_trace(go.Scattergl(x=df2_new['Date'],\n",
    "#                            y=df2_new[string]*1e2,\n",
    "#                            name= \"adjusted set 2\",\n",
    "#                            mode='markers',\n",
    "#                            marker=dict(\n",
    "#                            size=3,),\n",
    "#                            ),\n",
    "#                             row=1, col=1,\n",
    "#                            )\n",
    "\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"Adjusted Overlap on files 1 and 2 \",\n",
    "#     yaxis_title=\"state vector X [cm]\",\n",
    "#     xaxis_title=\"Date\",\n",
    "#     )\n",
    "\n",
    "# fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "# fig.update_xaxes(tickangle=30)\n",
    "# fig.update_xaxes( exponentformat= 'power')\n",
    "# fig.update_yaxes( exponentformat= 'power')\n",
    "\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-conjunction",
   "metadata": {},
   "source": [
    "\n",
    "# Construct G2B files\n",
    "\n",
    "Determine files that will go into each arc (files between maneuvers)\n",
    "\n",
    "```\n",
    "SATPAR   139     1807001          9.53000000       1514.000\n",
    "EPOCH               181013210000.0000000181013210000.00000001810160300 00.000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-property",
   "metadata": {},
   "source": [
    "**The next steps in constucting the g2b_PCE geodyn input is to construct an ascii text file and run it through the `pce_converter.f`**\n",
    "\n",
    "Steps to construct the `g2b` binary file:\n",
    "1. Load several (sample) days of `RVG` files\n",
    "2. Process and combine them into an ascii file titled `TRAJ.txt`\n",
    "3. Feed `TRAJ.txt` into the `PCE_converter.f`. The fortran code expects the following:\n",
    "    1. A file titled `TRAJ.txt`\n",
    "        - Each record (line) of the TRAJ.txt file has one integer and four floating point words.\n",
    "        - The integer and the first floating point word form the time tag of the record. \n",
    "            - The integer of TRAJ.txt is just the first word of the RVG data record converted to an integer.\n",
    "            - The first floating point of TRAJ.txt is just the sum of words 2 and 3 of the RVG data record.\n",
    "        - The last three words are the X, Y and Z coordinates of the satellite in the J2000 coordinate system.   \n",
    "            - The X, Y and Z values are just words 5, 6 and 7 of the RVG data record.\n",
    "\n",
    "\n",
    "All auxilliary files will continue to be hosted in the path, `/data/data_geodyn/inputs/icesat2/pre_processing/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "about-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Notes:\n",
    "# SATID = 1807001\n",
    "# epoch_start = '181013210000'\n",
    "##              YYMMDDHHMMSS.mls0000cd \n",
    "# epoch_end   = '181016030000'\n",
    "##              YYMMDDHHMM SS.mls\n",
    "\n",
    "# 54 hours (2 days 6 hours)\n",
    "# Oct 13- 2100  DOY=287\n",
    "# Oct 14-       DOY=288\n",
    "# Oct 15-       DOY=289\n",
    "# Oct 16- 0300  DOY=290\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "absolute-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc1_files = ['orbit.1807001.2018.287' ,\n",
    "              'orbit.1807001.2018.288' ,\n",
    "              'orbit.1807001.2018.289A',\n",
    "              'orbit.1807001.2018.290A',\n",
    "              'orbit.1807001.2018.291',\n",
    "#               'orbit.1807001.2018.292',\n",
    "#               'orbit.1807001.2018.293',\n",
    "#               'orbit.1807001.2018.294',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-button",
   "metadata": {},
   "source": [
    "**We will do this in a class using `pygeodyn` so that much of the work to call the code is behind-the-scenes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-receptor",
   "metadata": {},
   "source": [
    "**Construct `TRAJ.txt` as an ascii file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chubby-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0,'/data/geodyn_proj/pygeodyn/utils_pygeodyn_develop/')\n",
    "# from pre_processing import pygeodyn_PreProcessing\n",
    "\n",
    "\n",
    "# path_to_preprocessing = '/data/data_geodyn/inputs/icesat2/pre_processing'\n",
    "# path_to_binaryrvg     = '/data/data_geodyn/inputs/icesat2/pre_processing/traj_files_rvg'\n",
    "# Obj = pygeodyn_PreProcessing(path_to_binaryrvg, path_to_preprocessing,  arc1_files)\n",
    "\n",
    "# # Obj.get_timechopped_rvgdata()\n",
    "\n",
    "# Obj.make_ascii_traj_txt_file_for_pcefortran()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-lease",
   "metadata": {},
   "source": [
    "**Call the fortran code with F2PY**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-smith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adult-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# path_to_data = '/data/data_geodyn/inputs/icesat2/pre_processing/'\n",
    "\n",
    "# path_to_pygeodyn = '/data/geodyn_proj/pygeodyn/utils_pygeodyn_develop/util_preprocessing/'\n",
    "\n",
    "# ### CHANGE DIRECTORY to where the fortran code is hosted\n",
    "# os.chdir(path_to_pygeodyn)\n",
    "\n",
    "# #### Compile the pce code:\n",
    "# command_1 = './compile_pce_f.sh'\n",
    "# subprocess.run(command_1, shell = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "closing-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# command_2 = './ExecutePCE.exe > out_pce 2> err_execute'\n",
    "# subprocess.run(command_2, shell = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "complete-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "pce_fortran.f compiled\n",
      "pce_fortran.f executed\n",
      "\n",
      "The G2B binary file has been saved to:/data/data_geodyn/inputs/icesat2/pre_processing/g2b_pce\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'/data/geodyn_proj/pygeodyn/utils_pygeodyn_develop/util_preprocessing/')\n",
    "from pre_processing import pygeodyn_PreProcessing\n",
    "\n",
    "path_to_preprocessing = '/data/data_geodyn/inputs/icesat2/pre_processing'\n",
    "path_to_binaryrvg     = '/data/data_geodyn/inputs/icesat2/pre_processing/traj_files_rvg'\n",
    "Obj = pygeodyn_PreProcessing(path_to_binaryrvg, path_to_preprocessing,  arc1_files)\n",
    "\n",
    "\n",
    "Obj.call_fortran_pce_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "final-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# !f2py -c -m pce_converter pce_converter.f\n",
    "# !f2py -h --overwrite-signature fortran_pce.pyf  pce_converter.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-armstrong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-institution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-ebony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-miami",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-principal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-conducting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-convenience",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-gentleman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
